{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA主题模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主题模型是一个我们在自然语言处理中比较火的话题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要内容：\n",
    "- 主题模型理论\n",
    "    - 直观版（直觉）\n",
    "    - 标准版（伪代码）\n",
    "    - 公式版（公式及所有概念）\n",
    "   \n",
    "- 实战\n",
    "    - 项目【一眼看穿\"希拉里邮件门\"】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是主题模型？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章的文本主题分类：\n",
    "<img src='./images/theme01.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理论解释：\n",
    "理解整个过程，涉及到比较复杂的数学推导。\n",
    "一般来说，从公式1一直推导到公式100，大部分同学会在公式10的时候就卡住。\n",
    "\n",
    "所以，针对主题模型，本节内容用3个不同的形式讲解，从简单到复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.直观版\n",
    "假设某企业想要招聘一个工程师，他们收到了一大把的简历，他们想直接通过简历来看谁是大牛，谁是菜逼？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/resume.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这三个要素，构成了这家企业的人力总监判断的基础：\n",
    "\n",
    "<img src='./images/3elements.png' width='20%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这位⼈人⼒力力总监要做的事是：\n",
    "\n",
    "拿出⼀一份份简历，  \n",
    "记录下每份简历包含的特征，  \n",
    "然⽽而，他并不不知道，这⼀一切代表着什什么。  \n",
    "于是他开始猜，  \n",
    "拿起⼀份简历A，  \n",
    "他看到⾥里里⾯面说A参加过七⽉月课程，  \n",
    "他就猜这位童鞋的⽔水平应该很⾼高，⼋八成应该是个好⼯工程师。   \n",
    "但是他⼜又看到A的学历只是⼩小学毕业，⼼心⾥里里⼜又有了了两成的担忧。   \n",
    "他⼜又看到B，...   \n",
    "⼜又看到C, ...  \n",
    "等等...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA的过程，符合人类的认知过程，都是不断地尝试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/skirt.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "久经沙场之后，老司机人力总监掌握了如下信息：\n",
    "\n",
    "对于是不是优秀程序员的分类，它通过人头统计大概有了数。\n",
    "\n",
    "这让他在以后看到新简历的时候，一眼就知道他是不是个优秀程序员。\n",
    "\n",
    "**对于每个特征C，他也能说出大概百分之多少的人拥有特征C可以说明他们是优秀的程序员。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结成公式就是：\n",
    "\n",
    "<img src='./images/formula01.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例子与理论的关系：\n",
    "\n",
    "<img src='./images/theme02.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是LDA?\n",
    "LDA(Latent Dirichlet Allocation)：   \n",
    "\n",
    "是一种**无监督的贝叶斯模型**。\n",
    "\n",
    "是一种**主题模型**，它可以将文档集中每篇文档的主题按照概率分布的形式给出。\n",
    "\n",
    "同时，它是一种**无监督学习算法**，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及制定主题的数量k即可。\n",
    "\n",
    "此外，LDA的另一个优点则是，**对于每一个主题均可找出一些词语来描述它。**\n",
    "\n",
    "是一种**典型的词袋模型**，即它认为一篇文档是由一组词构成的一个集合，词与词之间没有顺序以及先后的关系。  \n",
    "\n",
    "一篇文档可以**包含多个主题**，文档中每一个词都由其中的一个主题生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是贝叶斯模型？\n",
    "理论：\n",
    "$$P(\\theta, y) = P(\\theta)P(y|\\theta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\theta | y) =\\frac{P(\\theta, y)}{P(y)} =\\frac{P(y|\\theta)P(\\theta)}{P(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型：\n",
    "- 用概率作为【可信度】\n",
    "- 每次看到新数据，就更新【可信度】\n",
    "- 需要一个模型来解释数据的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**先验、后验与似然：**\n",
    "\n",
    "$$P(好工程师|简历) = P(好工程师)P(简历|好工程师)$$\n",
    "\n",
    "其中：\n",
    "- P(好工程师)是**先验**概率，应聘者概率模型；\n",
    "- P(简历|好工程师)是**似然**概率，简历数据生成模型。\n",
    "- P(好工程师|简历)是**后验**概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bayes.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过一个例子来看：   \n",
    "\n",
    "电视节目中，有A,B,C三个门，其中一扇门后有1000万的奖金。选手选择了A门，主持人打开了C门，发现C门里面什么都没有，询问选手，是否还坚持选择A门？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析：  \n",
    "\n",
    "在主持人打开C门前，A,B,C三扇门后有1000万奖金的概率是均匀的，都是$\\frac{1}{3}$。\n",
    "即:\n",
    "$$P(in\\ A)=P(in\\ B)=P(in \\ C) = \\frac{1}{3}$$\n",
    "$$P(in \\ B \\ or\\ C) = \\frac{2}{3}$$\n",
    "$$P(not\\ in\\ A) = \\frac{2}{3}$$\n",
    "\n",
    "从普通频率学的角度来计算，\n",
    "如果不换，那么奖金在A门后的概率是：\n",
    "$$\\frac{1}{3}*100\\%+\\frac{2}{3}*0\\%=\\frac{1}{3}$$\n",
    "\n",
    "如果换，能拿到奖金的概率为：\n",
    "$$\\frac{1}{3}*0\\% + \\frac{2}{3}*100\\%= \\frac{2}{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个问题的解法在于，现在主持人已经开了C门，C门后什么都没有，这个信息出现后，还能够说明当初选A门，选错的概率是$\\frac{2}{3}$吗？  \n",
    "\n",
    "假设A门后有奖金，那么打开后，$$P(in\\ A| open \\ C) = \\frac{P(open\\ C| in\\ A)P(in \\ A)}{P(open \\ C)}$$\n",
    "$$=\\frac{P(open\\ C| in\\ A)P(in \\ A)}{P(in \\ A)P(open\\ C|in\\ A)+P(in\\ B)P(open\\ C|in \\ B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处，假设A门后有奖金，则$P(open\\ C|in \\ A)$的概率为1，假设$P(open \\ C|in \\ B)$的概率为k，则上式变为：\n",
    "$$=\\frac{1*\\frac{1}{3}}{\\frac{1}{3}*1 + \\frac{1}{3}*k}$$\n",
    "$$=\\frac{1}{k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，奖金在A门或B门中的概率，受k值的影响，只要主持人有一定的偏好，k值就会影响结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，如果我们不改变选择，坚持选择A门，那么奖金在B门后的概率是，$$P(in\\ B| open \\ C) = \\frac{P(open\\ B| in\\ A)P(in \\ B)}{P(open \\ C)}$$\n",
    "$$=\\frac{P(open\\ C| in\\ B)P(in \\ B)}{P(in \\ B)P(open\\ C|in\\ B)+P(in\\ A)P(open\\ C|in \\ A)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\frac{k*\\frac{1}{3}}{\\frac{1}{3}*k+\\frac{1}{3}*1 }$$\n",
    "$$=\\frac{k}{k+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个经典的案例，解释了概率论中频度学派和贝叶斯学派之间的差异。\n",
    "\n",
    "- 在频度学派中，认为的概率是一定的，此例中都是$\\frac{1}{3}$的可能性。\n",
    "- 在贝叶斯学派中，先验概率上都是$\\frac{1}{3}$，但是主持人打开某山门且里面没有奖金的行为，提供了一个信息。在主持人打开门的瞬间，我们就知道了这个信息，将$P(in\\ B|open \\ C)$的概率设置为k，我们就能代入到贝叶斯公式里进行下一步的计算。有了k之后，我们算出来的结果，可能就和之前先验的概率相等了。\n",
    "    - 主持人打开门，是一个事件，一个conditon。做这件事给了我们一个判断的信息，这个信息就是我们后面用来调整我们的先验概率，用来得到我们的后验概率的信息点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 标准版\n",
    "以伪代码的形式讲解LDA的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用LDA找寻的就是之前例子里总监大人统计出来的经验：\n",
    "\n",
    "- 一份简历的每个特征都是因为本人有一定概率是好/坏程序员，并从好/坏这个分类中以一定概率选择某些特征而组成的。\n",
    "\n",
    "- 一篇文章的每个词都是以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语而组成的。\n",
    "\n",
    "$$P(单词|文档)=P(单词|主题)*P(主题|文档)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(单词|文档)$是已知的，认为可观测到的，可统计的。\n",
    "- $P(单词|主题)$根据主题，这个主题下，冒出该单词的概率\n",
    "- $P(主题|文档)$根据文档，判断文档属于某个主题的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做LDA时，要得到两个概率分布，这两个概率的乘积，可以印证$P(单词|文档)$。\n",
    "\n",
    "我们可以调整这两个分布，直到这两个分布的乘积结果可以和$P(单词|文档)$的概率分布拟合起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/2distributions.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA生成过程：**\n",
    "\n",
    "对于语料库中的每篇文档，LDA定义了如下生成过程（generative process）:\n",
    "    1. 对于每一篇文档，从主题分布中抽取一个主题；\n",
    "    2. 从上述被抽到的主题所对应的单词分布中抽取一个单词；\n",
    "    3. 重复上述过程直至遍历文档中的每一个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“文档-主题分布”，“主题-单词分布”，这两个分布，我们并不知道这两个分布的情况。\n",
    "\n",
    "我们此处假设两个分布都是正态分布：\n",
    "    1. 在“文档-主题分布”中随机抽取一个主题，假设抽到的是“体育”主题；\n",
    "    2. 在“体育-单词分布”中随机抽取一个单词，假设抽到的是“跑步”单词；\n",
    "    3. 通过抽到的两个分布中的概率乘积，即：P(“体育”|“文档”)P(“跑步”|“体育”)，来判断原文档中“跑步”到底出现了多少次，P(“跑步”|“文档”)。两个分布得到结果的乘积是否拟合P(“跑步”|“文档”)的概率分布。如果拟合，则说明，主题取对了，如果不拟合，再选取下一个主题。以此往复。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "稍微具体点讲：\n",
    "- w代表单词\n",
    "- d代表文档\n",
    "- t代表主题\n",
    "- 大写代表总集合，小写代表个体\n",
    "\n",
    "D中每个文档d看作一个单词序列<$w_1, w_2,...,w_n$>，$w_i$表示第i个单词。\n",
    "\n",
    "D中涉及的所有不同单词组成一个词汇表大集合V（vocabulary），LDA以**文档集合D作为输入**，希望训练出的**两个结果向量**（假设形成k个topic，V中一个m个词）：\n",
    "\n",
    "**这两个结果向量，就是上面讲到的两个分布：“文档-主题分布”，“主题-单词分布”**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对每个D中的文档d，对应到不同的Topic的概率$\\theta_d<P_{t_1},...,P_{t_k}>$，其中$P_{t_i}$表示d对应T中的第i个topic的概率。计算方法是直观的，$P_{t_i}=\\frac{n_{t_{i}}}{n}$，其中$n_{t_{i}}$表示d中对应第$i$个topic的词的数目，n是d中所有词的总数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对每个T中的主题（topic）t，生成不同的单词的概率$\\phi_{t} <P_{w_1},...P_{w_m}>$，其中，$P_{w_{i}}$表示t生成V中第i个单词的概率。 计算方法同样很直观，$P_{w_i} = \\frac{N_{w_{i}}}{N}$，其中$N_{w_i}$表示对应到topic t的V中第i个单词的数目，N表示所有对应到topic t的单词总数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，LDA的核心公式如下：\n",
    "$$P(w|d) = P(w|t)*P(t|d)$$\n",
    "\n",
    "$$P(单词|文档)=P(单词|主题)*P(主题|文档)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直观地看这个公式，就是以Topic作为中间层，可以通过当前的$\\theta_d$和$\\phi_t$给出了文档d中出现单词w的概率。  \n",
    "\n",
    "其中，$P(t|d)$利用$\\theta_d$计算得到，$P(w|t)$利用$\\phi_t$计算得到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，利用当前的$\\theta_d$和$\\phi_t$，我们可以为一个文档的一个单词计算它对应任意一个Topic时的$P(w|d)$，然后根据这些结果来更新这个词应该对应的topic。\n",
    "\n",
    "然后，如果这个更新改变了这个单词所对应的Topic，就会反过来影响$\\theta_d$和$\\phi_t$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA学习过程：\n",
    "LDA算法开始时，先随机地给$\\theta_d$和$\\phi_t$赋值（对所有的d和t)。\n",
    "\n",
    "然后：   \n",
    "1. 针对一个特定的文档$d_s$中的第$i$个单词$w_i$，如果令该单词对应的topic为$t_j$，可以把上述公式改写为：\n",
    "$$P_j(w_i|d_s) = P(w_i|t_j)P(t_j|d_s)$$\n",
    "    \n",
    "    \n",
    "2. 现在我们可以枚举T中的topic，得到所有的$P_j(w_i|d_s)$。然后可以根据这些概率值结果为$d_s$中的第$i$个单词$w_i$选择一个topic。 最简单的想法是取令$P_j(w_i|d_s)$最大的$t_j$（注意，这个式子中只有j是变量）。\n",
    "    \n",
    "    \n",
    "3. 然后，如果$d_s$中的第$i$个单词$w_i$在这里选择了一个与原先不同的topic（也就是说，这个时候$i$在遍历$d_s$中所有的单词，而$t_j$理当不变），就会对$\\theta_d$和$\\phi_t$有影响了。它们的影响又会反过来影响对上面提到的$P(w|d)$的计算。对D中所有的d中的所有w进行一次$P(w|d)$的计算并重新选择topic看作一次迭代。这样进行n次循环迭代之后，就会收敛到LDA所需要的结果了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 公式版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://blog.csdn.net/v_july_v/article/details/41209515'>LDA介绍博客</a>\n",
    "\n",
    "理解LDA，可以分为下面5个步骤：\n",
    "- 一个函数：<a href='#gamma1'>gamma函数</a>\n",
    "- 四个分布：<a href='#binomial1'>二项分布</a>、<a>多项分布</a>、<a>beta分布</a>、<a>Dirichlet分布</a>\n",
    "- 一个概念和一个理念：<a href='#conjugate1'>共轭先验</a>和<a>贝叶斯框架</a>\n",
    "- 两个模型：<a >pLSA</a>、<a>LDA</a>\n",
    "- 一个采样：<a>Gibbs采样</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='conjugate1'>共轭分布于共轭先验</a></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(Y|X) \\propto P(X|Y)P(Y)$$\n",
    "后验概率(posterior probability) $\\propto$ 似然函数(likelyhood function)* 先验概率(prior probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共轭分布就是指，**先验概率和后验概率具有一样的函数的形式，这样的分布就称之为“共轭分布”。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展解释： \n",
    "\n",
    "LDA是基于贝叶斯模型的，涉及到贝叶斯模型离不开“先验分布”，“数据（似然）”和“后验分布”三块。\n",
    "\n",
    "在贝叶斯学派这里：\n",
    "$$先验分布+数据（似然）= 后验分布$$\n",
    "\n",
    "这点其实很好理解，因为这符合我们人的思维方式，比如你对好人和坏人的认知，先验分布为100个好人和100个坏人，即你认为好人和坏人各占一半，现在你被2个好人（数据）帮助了和一个坏人骗了，于是你得到了新的后验分布为：102个好人和101个坏人。现在你的后验分布里面认为好人比坏人多了。这个后验分布接着又变成了你的先验分布，当你被1个好人（数据）帮助了和3个坏人（数据）骗了后，你又更新了你的后验分布：103个好人和104个坏人。依次继续更新下去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假如用数学和概率的方式该如何不表达呢？\n",
    "\n",
    "对于我们的数据（似然），这个好办，假设用一个二项分布就可以搞定，即对于二项分布：\n",
    "$$Binom(k|n,p) = \\dbinom{n}{k}p^k(1-p)^{n-k}$$\n",
    "\n",
    "其中p我们可以理解为好人的概率，k为好人的个数，n为好人和坏人的总数。\n",
    "\n",
    "虽然数据（似然）很好理解，但是对于先验分布，我们就需要费一番脑筋了，为什么呢？  \n",
    "\n",
    "因为我们**希望这个先验分布和数据（似然）对应的二项分布集合后，得到的后验分布在后面还可以作为先验分布！**\n",
    "\n",
    "就像上面例子里的“102个好人和101个坏人”，它是前面一次贝叶斯推荐的后验分布，又是后一次贝叶斯推荐的先验分布。  \n",
    "\n",
    "也就是说，**我们希望先验分布和后验分布的形式应该是一样的，这样的分布我们一般叫共轭分布**。\n",
    "\n",
    "而我们的例子例，我们希望找到和二项分布共轭的分布。\n",
    "\n",
    "和二项分布共轭的分布其实就是Beta分布。\n",
    "\n",
    "Beta分布的表达式为：\n",
    "$$Beta(p|\\alpha, \\beta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\n",
    "\\Gamma(\\beta)}p^{\\alpha-1}(1-p)^{\\beta-1}$$\n",
    "其中，$\\Gamma$是Gamma函数，满足$\\Gamma=(x-1)!$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='gamma1'>gamma函数</a></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma函数，**阶乘函数在实数上的推广。**\n",
    "\n",
    "我们知道，对于整数而言：\n",
    "$$\\Gamma(n) = (n-1)!$$\n",
    "\n",
    "对于实数：\n",
    "$$\\Gamma = \\int_0^{\\infty} t^{x-1}e^{-t}dt$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='binomial1'>二项分布Binomial Distribution</a></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二项分布是从伯努利分布推进的。   \n",
    "\n",
    "伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负{+,-}。\n",
    "\n",
    "而二项分布，即重复n次的伯努利试验，记为。\n",
    "\n",
    "简而言之：\n",
    "- 只做一次实验，是伯努利分布；\n",
    "- 重复了n次实验，是二项分布。\n",
    "\n",
    "二项分布的概率密度函数为：\n",
    "$$P(K=k)=\\db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
