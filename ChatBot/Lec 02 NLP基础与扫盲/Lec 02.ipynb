{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a>Lec 02 NLP基础与扫盲</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节以NLTK为基础配合讲解自然语言处理的原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录：\n",
    "- <a href='#nltk'>NLTK</a>\n",
    "- <a href='#textprocessing'>文本处理流程</a>\n",
    "    - <a href='#tokenize'>分词</a>\n",
    "    - <a href='#standard'>归一化</a>\n",
    "    - <a href='#stopwords'>停止词</a>\n",
    "- <a href='#3examples'>NLP经典三案例</a>\n",
    "    - <a href='#sentiment'>情感分析</a>\n",
    "    - <a href='#similarity'>文本相似度</a>\n",
    "    - <a href='#classification'>文本分类</a>\n",
    "- <a href='#deeplearning'>深度学习加持</a>\n",
    "    - <a herf='#autoencoder'>Autoencoder</a>\n",
    "    - <a herf='#word2vec'>Word2Vec</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='nltk'>1.NLTK</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官网：http://www.nltk.org/\n",
    "\n",
    "Python上著名的自然语言处理库。自带语料库、词性分类库、自带分类、分词，等等功能强大的社区支持，还有N多的简单版wrapper。\n",
    "\n",
    "textblob就是NLTK的简版版wrapper。\n",
    "参考网址：https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# 安装语料库\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK功能一览表\n",
    "|NLTK Modules|Functionality||\n",
    "|:-|:-:|:-:|\n",
    "|nltk.corpus|Corpus|语料|\n",
    "|nltk.tokenize, nltk.stem| Tokenizers, stemmers|分词、词干提取|\n",
    "|nltk.collocations|t-test, chi-squared, mutual-info|配置|\n",
    "|nltk.tag|n-gram, backoff, Brill, HMM, TnT||\n",
    "|nltk.classify, nltk.cluster| Decision tree, Naive Bayes, K-means|分类器|\n",
    "|nltk.chunk| Regex, n-gram, named entity||\n",
    "|nltk.parsing| Parsing|treebank 主谓宾，文法树，主要用于NER|\n",
    "|nltk.sem, nltk.interence| Semantic interpretation|情感分析|\n",
    "|nltk.metrics| Evaluation metrics|测量|\n",
    "|nltk.probability| Probability & Estimation|概率|\n",
    "|nltk.app, nltk.chat| Applications|应用|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK自带语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='textprocessing'>2.文本处理流程</a><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征工程：将文本分词，（词性标注、词干提取（词性还原）），中止词去除，得到word list，最终数字化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk01.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK文本处理流程：一条typical的文本预处理流水线</h3>\n",
    "\n",
    "<img src='./images/nltk10.png' width='35%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='tokenize'>2.1 分词</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize\n",
    "\n",
    "将长句子拆成有“意义”的小部件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = 'hello, world!'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中英文NLP的区别\n",
    "英文天然带有分隔符，用空格“ ”作为分隔符。\n",
    "中文等非拉丁语言，分词就会有两种：\n",
    "- rule: 启发式（查字典）\n",
    "- generative: 机器学习/统计方法：HMM、CRF\n",
    "\n",
    "<img src='./images/nltk02.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w2/qnnfb62x2g760j3nkh9q4ptr0000gn/T/jieba.cache\n",
      "Loading model cost 0.720 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/来到/北京/清华/清华大学/华大/大学\n"
     ]
    }
   ],
   "source": [
    "# 中文分词\n",
    "import jieba\n",
    "seg_list = jieba.cut('我来到北京清华大学', cut_all=True) # 全模式\n",
    "print('Full Mode:', '/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/来到/北京/清华大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('我来到北京清华大学', cut_all=False) # 精确模式\n",
    "print('Default Mode:', '/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 他,来到,了,网易,杭研,大厦\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('他来到了网易杭研大厦') # 默认是精确模式\n",
    "print('Default Mode:', ','.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【新词识别】：他, 来到, 了了, ⽹网易易, 杭研, ⼤大厦    \n",
    "(此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明,硕士,毕业,于,中国,科学,学院,科学院,中国科学院,计算,计算所,，,后,在,日本,京都,大学,日本京都大学,深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search('小明硕士毕业于中国科学院计算所，后在日本京都大学深造') # 搜索引擎模式\n",
    "print(','.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词之后的效果：list of words\n",
    "<img src='./images/nltk04.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有时候tokenize没那么简单\n",
    "比如：  \n",
    "社交网络上，这些乱七八糟的不和语法不合逻辑的语言很多：   \n",
    "拯救@某人，表情符号，URL，#话题符号\n",
    "\n",
    "<img src='./images/nltk05.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
     ]
    }
   ],
   "source": [
    "# 社交网络语言的tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对社交网络语言的tokenize，在预处理过程中，用re正则表达式预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则表达式\n",
    "对照表：\n",
    "http://www.regexlab.com/zh/regref.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "emoticons_str = r'''\n",
    "(?:\n",
    "    [:=;] #眼睛\n",
    "    [oO\\-]? # ⿐子\n",
    "    [D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    ")\n",
    "'''\n",
    "regex_str = [emoticons_str, \n",
    "            r'<[^>]+>', # HTML tags\n",
    "            r'(?:@[\\w_]+)', # @某⼈\n",
    "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    "            # URLs\n",
    "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "            r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词\n",
    "            r'(?:[\\w_]+)', # 其他\n",
    "            r'(?:\\S)' # 其他\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s, lowercase=False):\n",
    "    def tokenize(s):\n",
    "        return tokens_re.findall(s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokenize = [token if emotion_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='standard'>2.2 归一化</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 纷繁复杂的词形\n",
    "\n",
    "<img src='./images/nltk06.png' width='50%'/>\n",
    "<img src='./images/nltk07.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词形归一化\n",
    "- Stemming 词干提取：一般来说，就是把不影响词性的inflection的小尾巴砍掉\n",
    "    - walking 砍掉ing => walk\n",
    "    - walked 砍掉ed => walk\n",
    "    \n",
    "- Lemmatization 词形归一：把各种类型的词的变形，都归为一个形式（“查表”的方式）\n",
    "    - went 归一 => go\n",
    "    - are 归一 => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Stemming：nltk.stem.porter.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'provis'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('provision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('went')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('wenting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Stemming：nltk.stem.SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "snowball_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Stemming：nltk.stem.lancaster.LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maxim'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Lemma: nltk.stem.WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'church'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('churches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardwolf'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('aardwolves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abacus'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('abaci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hardrock'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('hardrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma的小问题\n",
    "<img src='./images/nltk08.png' width='33%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK更好地实现Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 没有PosTag词性标注，默认是 NN(名词)\n",
    "wordnet_lemmatizer.lemmatize('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加上Pos Tag（词性标注）\n",
    "wordnet_lemmatizer.lemmatize('is', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词形标注Part-Of-Speech\n",
    "<img src='./images/nltk09.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'does', 'the', 'fox', 'say']\n",
      "[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "# NLTK的词性标注 POS Tag\n",
    "import nltk\n",
    "text = nltk.word_tokenize('what does the fox say') # 先分词\n",
    "print(text)\n",
    "print(nltk.pos_tag(text)) # 后POS Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='stopwords'>2.3 停止词</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一千个he，有一千种指代。    \n",
    "一千个the，有一千种指事。\n",
    "\n",
    "对于注重理解文本【意思】的应用场景来说，歧义太多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全体stopwords列表：http://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK去除stopwords\n",
    "首先，记得在console里面加载一下词库，或者，nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'chinese', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# 先tokenize分词，得到word_list\n",
    "word_list = nltk.word_tokenize('I am chinese.')\n",
    "# 然后filter一把\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='3examples'>3.NLP经典三案例</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是自然语言处理？\n",
    "\n",
    "<img src='./images/nltk11.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本预处理让我们得到了什么？\n",
    "<img src='./images/nltk12.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK在NLP上的经典应用：情感分析、文本相似度、文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='sentiment'>3.1 情感分析</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工业界还是很受欢迎，常用于行研报告，舆情分析。\n",
    "<img src='./images/nltk13.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的sentiment dictionary:\n",
    "- like 1\n",
    "- good 2\n",
    "- bad -2\n",
    "- terrible -3\n",
    "\n",
    "类似于关键词打分机制\n",
    "\n",
    "比如：AFINN-111\n",
    "http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工业界确实是有公司用**中文的情感词库**，通过情感词库的关键词打分，最后得出结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK完成简单的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dictionary = {}\n",
    "for line in open('./AFINN/AFINN-111.txt'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abandon': -2,\n",
       " 'abandoned': -2,\n",
       " 'abandons': -2,\n",
       " 'abducted': -2,\n",
       " 'abduction': -2,\n",
       " 'abductions': -2,\n",
       " 'abhor': -3,\n",
       " 'abhorred': -3,\n",
       " 'abhorrent': -3,\n",
       " 'abhors': -3,\n",
       " 'abilities': 2,\n",
       " 'ability': 2,\n",
       " 'aboard': 1,\n",
       " 'absentee': -1,\n",
       " 'absentees': -1,\n",
       " 'absolve': 2,\n",
       " 'absolved': 2,\n",
       " 'absolves': 2,\n",
       " 'absolving': 2,\n",
       " 'absorbed': 1,\n",
       " 'abuse': -3,\n",
       " 'abused': -3,\n",
       " 'abuses': -3,\n",
       " 'abusive': -3,\n",
       " 'accept': 1,\n",
       " 'accepted': 1,\n",
       " 'accepting': 1,\n",
       " 'accepts': 1,\n",
       " 'accident': -2,\n",
       " 'accidental': -2,\n",
       " 'accidentally': -2,\n",
       " 'accidents': -2,\n",
       " 'accomplish': 2,\n",
       " 'accomplished': 2,\n",
       " 'accomplishes': 2,\n",
       " 'accusation': -2,\n",
       " 'accusations': -2,\n",
       " 'accuse': -2,\n",
       " 'accused': -2,\n",
       " 'accuses': -2,\n",
       " 'accusing': -2,\n",
       " 'ache': -2,\n",
       " 'achievable': 1,\n",
       " 'aching': -2,\n",
       " 'acquit': 2,\n",
       " 'acquits': 2,\n",
       " 'acquitted': 2,\n",
       " 'acquitting': 2,\n",
       " 'acrimonious': -3,\n",
       " 'active': 1,\n",
       " 'adequate': 1,\n",
       " 'admire': 3,\n",
       " 'admired': 3,\n",
       " 'admires': 3,\n",
       " 'admiring': 3,\n",
       " 'admit': -1,\n",
       " 'admits': -1,\n",
       " 'admitted': -1,\n",
       " 'admonish': -2,\n",
       " 'admonished': -2,\n",
       " 'adopt': 1,\n",
       " 'adopts': 1,\n",
       " 'adorable': 3,\n",
       " 'adore': 3,\n",
       " 'adored': 3,\n",
       " 'adores': 3,\n",
       " 'advanced': 1,\n",
       " 'advantage': 2,\n",
       " 'advantages': 2,\n",
       " 'adventure': 2,\n",
       " 'adventures': 2,\n",
       " 'adventurous': 2,\n",
       " 'affected': -1,\n",
       " 'affection': 3,\n",
       " 'affectionate': 3,\n",
       " 'afflicted': -1,\n",
       " 'affronted': -1,\n",
       " 'afraid': -2,\n",
       " 'aggravate': -2,\n",
       " 'aggravated': -2,\n",
       " 'aggravates': -2,\n",
       " 'aggravating': -2,\n",
       " 'aggression': -2,\n",
       " 'aggressions': -2,\n",
       " 'aggressive': -2,\n",
       " 'aghast': -2,\n",
       " 'agog': 2,\n",
       " 'agonise': -3,\n",
       " 'agonised': -3,\n",
       " 'agonises': -3,\n",
       " 'agonising': -3,\n",
       " 'agonize': -3,\n",
       " 'agonized': -3,\n",
       " 'agonizes': -3,\n",
       " 'agonizing': -3,\n",
       " 'agree': 1,\n",
       " 'agreeable': 2,\n",
       " 'agreed': 1,\n",
       " 'agreement': 1,\n",
       " 'agrees': 1,\n",
       " 'alarm': -2,\n",
       " 'alarmed': -2,\n",
       " 'alarmist': -2,\n",
       " 'alarmists': -2,\n",
       " 'alas': -1,\n",
       " 'alert': -1,\n",
       " 'alienation': -2,\n",
       " 'alive': 1,\n",
       " 'allergic': -2,\n",
       " 'allow': 1,\n",
       " 'alone': -2,\n",
       " 'amaze': 2,\n",
       " 'amazed': 2,\n",
       " 'amazes': 2,\n",
       " 'amazing': 4,\n",
       " 'ambitious': 2,\n",
       " 'ambivalent': -1,\n",
       " 'amuse': 3,\n",
       " 'amused': 3,\n",
       " 'amusement': 3,\n",
       " 'amusements': 3,\n",
       " 'anger': -3,\n",
       " 'angers': -3,\n",
       " 'angry': -3,\n",
       " 'anguish': -3,\n",
       " 'anguished': -3,\n",
       " 'animosity': -2,\n",
       " 'annoy': -2,\n",
       " 'annoyance': -2,\n",
       " 'annoyed': -2,\n",
       " 'annoying': -2,\n",
       " 'annoys': -2,\n",
       " 'antagonistic': -2,\n",
       " 'anti': -1,\n",
       " 'anticipation': 1,\n",
       " 'anxiety': -2,\n",
       " 'anxious': -2,\n",
       " 'apathetic': -3,\n",
       " 'apathy': -3,\n",
       " 'apeshit': -3,\n",
       " 'apocalyptic': -2,\n",
       " 'apologise': -1,\n",
       " 'apologised': -1,\n",
       " 'apologises': -1,\n",
       " 'apologising': -1,\n",
       " 'apologize': -1,\n",
       " 'apologized': -1,\n",
       " 'apologizes': -1,\n",
       " 'apologizing': -1,\n",
       " 'apology': -1,\n",
       " 'appalled': -2,\n",
       " 'appalling': -2,\n",
       " 'appease': 2,\n",
       " 'appeased': 2,\n",
       " 'appeases': 2,\n",
       " 'appeasing': 2,\n",
       " 'applaud': 2,\n",
       " 'applauded': 2,\n",
       " 'applauding': 2,\n",
       " 'applauds': 2,\n",
       " 'applause': 2,\n",
       " 'appreciate': 2,\n",
       " 'appreciated': 2,\n",
       " 'appreciates': 2,\n",
       " 'appreciating': 2,\n",
       " 'appreciation': 2,\n",
       " 'apprehensive': -2,\n",
       " 'approval': 2,\n",
       " 'approved': 2,\n",
       " 'approves': 2,\n",
       " 'ardent': 1,\n",
       " 'arrest': -2,\n",
       " 'arrested': -3,\n",
       " 'arrests': -2,\n",
       " 'arrogant': -2,\n",
       " 'ashame': -2,\n",
       " 'ashamed': -2,\n",
       " 'ass': -4,\n",
       " 'assassination': -3,\n",
       " 'assassinations': -3,\n",
       " 'asset': 2,\n",
       " 'assets': 2,\n",
       " 'assfucking': -4,\n",
       " 'asshole': -4,\n",
       " 'astonished': 2,\n",
       " 'astound': 3,\n",
       " 'astounded': 3,\n",
       " 'astounding': 3,\n",
       " 'astoundingly': 3,\n",
       " 'astounds': 3,\n",
       " 'attack': -1,\n",
       " 'attacked': -1,\n",
       " 'attacking': -1,\n",
       " 'attacks': -1,\n",
       " 'attract': 1,\n",
       " 'attracted': 1,\n",
       " 'attracting': 2,\n",
       " 'attraction': 2,\n",
       " 'attractions': 2,\n",
       " 'attracts': 1,\n",
       " 'audacious': 3,\n",
       " 'authority': 1,\n",
       " 'avert': -1,\n",
       " 'averted': -1,\n",
       " 'averts': -1,\n",
       " 'avid': 2,\n",
       " 'avoid': -1,\n",
       " 'avoided': -1,\n",
       " 'avoids': -1,\n",
       " 'await': -1,\n",
       " 'awaited': -1,\n",
       " 'awaits': -1,\n",
       " 'award': 3,\n",
       " 'awarded': 3,\n",
       " 'awards': 3,\n",
       " 'awesome': 4,\n",
       " 'awful': -3,\n",
       " 'awkward': -2,\n",
       " 'axe': -1,\n",
       " 'axed': -1,\n",
       " 'backed': 1,\n",
       " 'backing': 2,\n",
       " 'backs': 1,\n",
       " 'bad': -3,\n",
       " 'badass': -3,\n",
       " 'badly': -3,\n",
       " 'bailout': -2,\n",
       " 'bamboozle': -2,\n",
       " 'bamboozled': -2,\n",
       " 'bamboozles': -2,\n",
       " 'ban': -2,\n",
       " 'banish': -1,\n",
       " 'bankrupt': -3,\n",
       " 'bankster': -3,\n",
       " 'banned': -2,\n",
       " 'bargain': 2,\n",
       " 'barrier': -2,\n",
       " 'bastard': -5,\n",
       " 'bastards': -5,\n",
       " 'battle': -1,\n",
       " 'battles': -1,\n",
       " 'beaten': -2,\n",
       " 'beatific': 3,\n",
       " 'beating': -1,\n",
       " 'beauties': 3,\n",
       " 'beautiful': 3,\n",
       " 'beautifully': 3,\n",
       " 'beautify': 3,\n",
       " 'belittle': -2,\n",
       " 'belittled': -2,\n",
       " 'beloved': 3,\n",
       " 'benefit': 2,\n",
       " 'benefits': 2,\n",
       " 'benefitted': 2,\n",
       " 'benefitting': 2,\n",
       " 'bereave': -2,\n",
       " 'bereaved': -2,\n",
       " 'bereaves': -2,\n",
       " 'bereaving': -2,\n",
       " 'best': 3,\n",
       " 'betray': -3,\n",
       " 'betrayal': -3,\n",
       " 'betrayed': -3,\n",
       " 'betraying': -3,\n",
       " 'betrays': -3,\n",
       " 'better': 2,\n",
       " 'bias': -1,\n",
       " 'biased': -2,\n",
       " 'big': 1,\n",
       " 'bitch': -5,\n",
       " 'bitches': -5,\n",
       " 'bitter': -2,\n",
       " 'bitterly': -2,\n",
       " 'bizarre': -2,\n",
       " 'blah': -2,\n",
       " 'blame': -2,\n",
       " 'blamed': -2,\n",
       " 'blames': -2,\n",
       " 'blaming': -2,\n",
       " 'bless': 2,\n",
       " 'blesses': 2,\n",
       " 'blessing': 3,\n",
       " 'blind': -1,\n",
       " 'bliss': 3,\n",
       " 'blissful': 3,\n",
       " 'blithe': 2,\n",
       " 'block': -1,\n",
       " 'blockbuster': 3,\n",
       " 'blocked': -1,\n",
       " 'blocking': -1,\n",
       " 'blocks': -1,\n",
       " 'bloody': -3,\n",
       " 'blurry': -2,\n",
       " 'boastful': -2,\n",
       " 'bold': 2,\n",
       " 'boldly': 2,\n",
       " 'bomb': -1,\n",
       " 'boost': 1,\n",
       " 'boosted': 1,\n",
       " 'boosting': 1,\n",
       " 'boosts': 1,\n",
       " 'bore': -2,\n",
       " 'bored': -2,\n",
       " 'boring': -3,\n",
       " 'bother': -2,\n",
       " 'bothered': -2,\n",
       " 'bothers': -2,\n",
       " 'bothersome': -2,\n",
       " 'boycott': -2,\n",
       " 'boycotted': -2,\n",
       " 'boycotting': -2,\n",
       " 'boycotts': -2,\n",
       " 'brainwashing': -3,\n",
       " 'brave': 2,\n",
       " 'breakthrough': 3,\n",
       " 'breathtaking': 5,\n",
       " 'bribe': -3,\n",
       " 'bright': 1,\n",
       " 'brightest': 2,\n",
       " 'brightness': 1,\n",
       " 'brilliant': 4,\n",
       " 'brisk': 2,\n",
       " 'broke': -1,\n",
       " 'broken': -1,\n",
       " 'brooding': -2,\n",
       " 'bullied': -2,\n",
       " 'bullshit': -4,\n",
       " 'bully': -2,\n",
       " 'bullying': -2,\n",
       " 'bummer': -2,\n",
       " 'buoyant': 2,\n",
       " 'burden': -2,\n",
       " 'burdened': -2,\n",
       " 'burdening': -2,\n",
       " 'burdens': -2,\n",
       " 'calm': 2,\n",
       " 'calmed': 2,\n",
       " 'calming': 2,\n",
       " 'calms': 2,\n",
       " \"can't stand\": -3,\n",
       " 'cancel': -1,\n",
       " 'cancelled': -1,\n",
       " 'cancelling': -1,\n",
       " 'cancels': -1,\n",
       " 'cancer': -1,\n",
       " 'capable': 1,\n",
       " 'captivated': 3,\n",
       " 'care': 2,\n",
       " 'carefree': 1,\n",
       " 'careful': 2,\n",
       " 'carefully': 2,\n",
       " 'careless': -2,\n",
       " 'cares': 2,\n",
       " 'cashing in': -2,\n",
       " 'casualty': -2,\n",
       " 'catastrophe': -3,\n",
       " 'catastrophic': -4,\n",
       " 'cautious': -1,\n",
       " 'celebrate': 3,\n",
       " 'celebrated': 3,\n",
       " 'celebrates': 3,\n",
       " 'celebrating': 3,\n",
       " 'censor': -2,\n",
       " 'censored': -2,\n",
       " 'censors': -2,\n",
       " 'certain': 1,\n",
       " 'chagrin': -2,\n",
       " 'chagrined': -2,\n",
       " 'challenge': -1,\n",
       " 'chance': 2,\n",
       " 'chances': 2,\n",
       " 'chaos': -2,\n",
       " 'chaotic': -2,\n",
       " 'charged': -3,\n",
       " 'charges': -2,\n",
       " 'charm': 3,\n",
       " 'charming': 3,\n",
       " 'charmless': -3,\n",
       " 'chastise': -3,\n",
       " 'chastised': -3,\n",
       " 'chastises': -3,\n",
       " 'chastising': -3,\n",
       " 'cheat': -3,\n",
       " 'cheated': -3,\n",
       " 'cheater': -3,\n",
       " 'cheaters': -3,\n",
       " 'cheats': -3,\n",
       " 'cheer': 2,\n",
       " 'cheered': 2,\n",
       " 'cheerful': 2,\n",
       " 'cheering': 2,\n",
       " 'cheerless': -2,\n",
       " 'cheers': 2,\n",
       " 'cheery': 3,\n",
       " 'cherish': 2,\n",
       " 'cherished': 2,\n",
       " 'cherishes': 2,\n",
       " 'cherishing': 2,\n",
       " 'chic': 2,\n",
       " 'childish': -2,\n",
       " 'chilling': -1,\n",
       " 'choke': -2,\n",
       " 'choked': -2,\n",
       " 'chokes': -2,\n",
       " 'choking': -2,\n",
       " 'clarifies': 2,\n",
       " 'clarity': 2,\n",
       " 'clash': -2,\n",
       " 'classy': 3,\n",
       " 'clean': 2,\n",
       " 'cleaner': 2,\n",
       " 'clear': 1,\n",
       " 'cleared': 1,\n",
       " 'clearly': 1,\n",
       " 'clears': 1,\n",
       " 'clever': 2,\n",
       " 'clouded': -1,\n",
       " 'clueless': -2,\n",
       " 'cock': -5,\n",
       " 'cocksucker': -5,\n",
       " 'cocksuckers': -5,\n",
       " 'cocky': -2,\n",
       " 'coerced': -2,\n",
       " 'collapse': -2,\n",
       " 'collapsed': -2,\n",
       " 'collapses': -2,\n",
       " 'collapsing': -2,\n",
       " 'collide': -1,\n",
       " 'collides': -1,\n",
       " 'colliding': -1,\n",
       " 'collision': -2,\n",
       " 'collisions': -2,\n",
       " 'colluding': -3,\n",
       " 'combat': -1,\n",
       " 'combats': -1,\n",
       " 'comedy': 1,\n",
       " 'comfort': 2,\n",
       " 'comfortable': 2,\n",
       " 'comforting': 2,\n",
       " 'comforts': 2,\n",
       " 'commend': 2,\n",
       " 'commended': 2,\n",
       " 'commit': 1,\n",
       " 'commitment': 2,\n",
       " 'commits': 1,\n",
       " 'committed': 1,\n",
       " 'committing': 1,\n",
       " 'compassionate': 2,\n",
       " 'compelled': 1,\n",
       " 'competent': 2,\n",
       " 'competitive': 2,\n",
       " 'complacent': -2,\n",
       " 'complain': -2,\n",
       " 'complained': -2,\n",
       " 'complains': -2,\n",
       " 'comprehensive': 2,\n",
       " 'conciliate': 2,\n",
       " 'conciliated': 2,\n",
       " 'conciliates': 2,\n",
       " 'conciliating': 2,\n",
       " 'condemn': -2,\n",
       " 'condemnation': -2,\n",
       " 'condemned': -2,\n",
       " 'condemns': -2,\n",
       " 'confidence': 2,\n",
       " 'confident': 2,\n",
       " 'conflict': -2,\n",
       " 'conflicting': -2,\n",
       " 'conflictive': -2,\n",
       " 'conflicts': -2,\n",
       " 'confuse': -2,\n",
       " 'confused': -2,\n",
       " 'confusing': -2,\n",
       " 'congrats': 2,\n",
       " 'congratulate': 2,\n",
       " 'congratulation': 2,\n",
       " 'congratulations': 2,\n",
       " 'consent': 2,\n",
       " 'consents': 2,\n",
       " 'consolable': 2,\n",
       " 'conspiracy': -3,\n",
       " 'constrained': -2,\n",
       " 'contagion': -2,\n",
       " 'contagions': -2,\n",
       " 'contagious': -1,\n",
       " 'contempt': -2,\n",
       " 'contemptuous': -2,\n",
       " 'contemptuously': -2,\n",
       " 'contend': -1,\n",
       " 'contender': -1,\n",
       " 'contending': -1,\n",
       " 'contentious': -2,\n",
       " 'contestable': -2,\n",
       " 'controversial': -2,\n",
       " 'controversially': -2,\n",
       " 'convince': 1,\n",
       " 'convinced': 1,\n",
       " 'convinces': 1,\n",
       " 'convivial': 2,\n",
       " 'cool': 1,\n",
       " 'cool stuff': 3,\n",
       " 'cornered': -2,\n",
       " 'corpse': -1,\n",
       " 'costly': -2,\n",
       " 'courage': 2,\n",
       " 'courageous': 2,\n",
       " 'courteous': 2,\n",
       " 'courtesy': 2,\n",
       " 'cover-up': -3,\n",
       " 'coward': -2,\n",
       " 'cowardly': -2,\n",
       " 'coziness': 2,\n",
       " 'cramp': -1,\n",
       " 'crap': -3,\n",
       " 'crash': -2,\n",
       " 'crazier': -2,\n",
       " 'craziest': -2,\n",
       " 'crazy': -2,\n",
       " 'creative': 2,\n",
       " 'crestfallen': -2,\n",
       " 'cried': -2,\n",
       " 'cries': -2,\n",
       " 'crime': -3,\n",
       " 'criminal': -3,\n",
       " 'criminals': -3,\n",
       " 'crisis': -3,\n",
       " 'critic': -2,\n",
       " 'criticism': -2,\n",
       " 'criticize': -2,\n",
       " 'criticized': -2,\n",
       " 'criticizes': -2,\n",
       " 'criticizing': -2,\n",
       " 'critics': -2,\n",
       " 'cruel': -3,\n",
       " 'cruelty': -3,\n",
       " 'crush': -1,\n",
       " 'crushed': -2,\n",
       " 'crushes': -1,\n",
       " 'crushing': -1,\n",
       " 'cry': -1,\n",
       " 'crying': -2,\n",
       " 'cunt': -5,\n",
       " 'curious': 1,\n",
       " 'curse': -1,\n",
       " 'cut': -1,\n",
       " 'cute': 2,\n",
       " 'cuts': -1,\n",
       " 'cutting': -1,\n",
       " 'cynic': -2,\n",
       " 'cynical': -2,\n",
       " 'cynicism': -2,\n",
       " 'damage': -3,\n",
       " 'damages': -3,\n",
       " 'damn': -4,\n",
       " 'damned': -4,\n",
       " 'damnit': -4,\n",
       " 'danger': -2,\n",
       " 'daredevil': 2,\n",
       " 'daring': 2,\n",
       " 'darkest': -2,\n",
       " 'darkness': -1,\n",
       " 'dauntless': 2,\n",
       " 'dead': -3,\n",
       " 'deadlock': -2,\n",
       " 'deafening': -1,\n",
       " 'dear': 2,\n",
       " 'dearly': 3,\n",
       " 'death': -2,\n",
       " 'debonair': 2,\n",
       " 'debt': -2,\n",
       " 'deceit': -3,\n",
       " 'deceitful': -3,\n",
       " 'deceive': -3,\n",
       " 'deceived': -3,\n",
       " 'deceives': -3,\n",
       " 'deceiving': -3,\n",
       " 'deception': -3,\n",
       " 'decisive': 1,\n",
       " 'dedicated': 2,\n",
       " 'defeated': -2,\n",
       " 'defect': -3,\n",
       " 'defects': -3,\n",
       " 'defender': 2,\n",
       " 'defenders': 2,\n",
       " 'defenseless': -2,\n",
       " 'defer': -1,\n",
       " 'deferring': -1,\n",
       " 'defiant': -1,\n",
       " 'deficit': -2,\n",
       " 'degrade': -2,\n",
       " 'degraded': -2,\n",
       " 'degrades': -2,\n",
       " 'dehumanize': -2,\n",
       " 'dehumanized': -2,\n",
       " 'dehumanizes': -2,\n",
       " 'dehumanizing': -2,\n",
       " 'deject': -2,\n",
       " 'dejected': -2,\n",
       " 'dejecting': -2,\n",
       " 'dejects': -2,\n",
       " 'delay': -1,\n",
       " 'delayed': -1,\n",
       " 'delight': 3,\n",
       " 'delighted': 3,\n",
       " 'delighting': 3,\n",
       " 'delights': 3,\n",
       " 'demand': -1,\n",
       " 'demanded': -1,\n",
       " 'demanding': -1,\n",
       " 'demands': -1,\n",
       " 'demonstration': -1,\n",
       " 'demoralized': -2,\n",
       " 'denied': -2,\n",
       " 'denier': -2,\n",
       " 'deniers': -2,\n",
       " 'denies': -2,\n",
       " 'denounce': -2,\n",
       " 'denounces': -2,\n",
       " 'deny': -2,\n",
       " 'denying': -2,\n",
       " 'depressed': -2,\n",
       " 'depressing': -2,\n",
       " 'derail': -2,\n",
       " 'derailed': -2,\n",
       " 'derails': -2,\n",
       " 'deride': -2,\n",
       " 'derided': -2,\n",
       " 'derides': -2,\n",
       " 'deriding': -2,\n",
       " 'derision': -2,\n",
       " 'desirable': 2,\n",
       " 'desire': 1,\n",
       " 'desired': 2,\n",
       " 'desirous': 2,\n",
       " 'despair': -3,\n",
       " 'despairing': -3,\n",
       " 'despairs': -3,\n",
       " 'desperate': -3,\n",
       " 'desperately': -3,\n",
       " 'despondent': -3,\n",
       " 'destroy': -3,\n",
       " 'destroyed': -3,\n",
       " 'destroying': -3,\n",
       " 'destroys': -3,\n",
       " 'destruction': -3,\n",
       " 'destructive': -3,\n",
       " 'detached': -1,\n",
       " 'detain': -2,\n",
       " 'detained': -2,\n",
       " 'detention': -2,\n",
       " 'determined': 2,\n",
       " 'devastate': -2,\n",
       " 'devastated': -2,\n",
       " 'devastating': -2,\n",
       " 'devoted': 3,\n",
       " 'diamond': 1,\n",
       " 'dick': -4,\n",
       " 'dickhead': -4,\n",
       " 'die': -3,\n",
       " 'died': -3,\n",
       " 'difficult': -1,\n",
       " 'diffident': -2,\n",
       " 'dilemma': -1,\n",
       " 'dipshit': -3,\n",
       " 'dire': -3,\n",
       " 'direful': -3,\n",
       " 'dirt': -2,\n",
       " 'dirtier': -2,\n",
       " 'dirtiest': -2,\n",
       " 'dirty': -2,\n",
       " 'disabling': -1,\n",
       " 'disadvantage': -2,\n",
       " 'disadvantaged': -2,\n",
       " 'disappear': -1,\n",
       " 'disappeared': -1,\n",
       " 'disappears': -1,\n",
       " 'disappoint': -2,\n",
       " 'disappointed': -2,\n",
       " 'disappointing': -2,\n",
       " 'disappointment': -2,\n",
       " 'disappointments': -2,\n",
       " 'disappoints': -2,\n",
       " 'disaster': -2,\n",
       " 'disasters': -2,\n",
       " 'disastrous': -3,\n",
       " 'disbelieve': -2,\n",
       " 'discard': -1,\n",
       " 'discarded': -1,\n",
       " 'discarding': -1,\n",
       " 'discards': -1,\n",
       " 'disconsolate': -2,\n",
       " 'disconsolation': -2,\n",
       " 'discontented': -2,\n",
       " 'discord': -2,\n",
       " 'discounted': -1,\n",
       " 'discouraged': -2,\n",
       " 'discredited': -2,\n",
       " 'disdain': -2,\n",
       " 'disgrace': -2,\n",
       " 'disgraced': -2,\n",
       " 'disguise': -1,\n",
       " 'disguised': -1,\n",
       " 'disguises': -1,\n",
       " 'disguising': -1,\n",
       " 'disgust': -3,\n",
       " 'disgusted': -3,\n",
       " 'disgusting': -3,\n",
       " 'disheartened': -2,\n",
       " 'dishonest': -2,\n",
       " 'disillusioned': -2,\n",
       " 'disinclined': -2,\n",
       " 'disjointed': -2,\n",
       " 'dislike': -2,\n",
       " 'dismal': -2,\n",
       " 'dismayed': -2,\n",
       " 'disorder': -2,\n",
       " 'disorganized': -2,\n",
       " 'disoriented': -2,\n",
       " 'disparage': -2,\n",
       " 'disparaged': -2,\n",
       " 'disparages': -2,\n",
       " 'disparaging': -2,\n",
       " 'displeased': -2,\n",
       " 'dispute': -2,\n",
       " 'disputed': -2,\n",
       " 'disputes': -2,\n",
       " 'disputing': -2,\n",
       " 'disqualified': -2,\n",
       " 'disquiet': -2,\n",
       " 'disregard': -2,\n",
       " 'disregarded': -2,\n",
       " 'disregarding': -2,\n",
       " 'disregards': -2,\n",
       " 'disrespect': -2,\n",
       " 'disrespected': -2,\n",
       " 'disruption': -2,\n",
       " 'disruptions': -2,\n",
       " 'disruptive': -2,\n",
       " 'dissatisfied': -2,\n",
       " 'distort': -2,\n",
       " 'distorted': -2,\n",
       " 'distorting': -2,\n",
       " 'distorts': -2,\n",
       " 'distract': -2,\n",
       " 'distracted': -2,\n",
       " 'distraction': -2,\n",
       " 'distracts': -2,\n",
       " 'distress': -2,\n",
       " 'distressed': -2,\n",
       " 'distresses': -2,\n",
       " 'distressing': -2,\n",
       " 'distrust': -3,\n",
       " 'distrustful': -3,\n",
       " 'disturb': -2,\n",
       " 'disturbed': -2,\n",
       " 'disturbing': -2,\n",
       " 'disturbs': -2,\n",
       " 'dithering': -2,\n",
       " 'dizzy': -1,\n",
       " 'dodging': -2,\n",
       " 'dodgy': -2,\n",
       " 'does not work': -3,\n",
       " 'dolorous': -2,\n",
       " 'dont like': -2,\n",
       " 'doom': -2,\n",
       " 'doomed': -2,\n",
       " 'doubt': -1,\n",
       " 'doubted': -1,\n",
       " 'doubtful': -1,\n",
       " 'doubting': -1,\n",
       " 'doubts': -1,\n",
       " 'douche': -3,\n",
       " 'douchebag': -3,\n",
       " 'downcast': -2,\n",
       " 'downhearted': -2,\n",
       " 'downside': -2,\n",
       " 'drag': -1,\n",
       " 'dragged': -1,\n",
       " 'drags': -1,\n",
       " 'drained': -2,\n",
       " 'dread': -2,\n",
       " 'dreaded': -2,\n",
       " 'dreadful': -3,\n",
       " 'dreading': -2,\n",
       " 'dream': 1,\n",
       " 'dreams': 1,\n",
       " 'dreary': -2,\n",
       " 'droopy': -2,\n",
       " 'drop': -1,\n",
       " 'drown': -2,\n",
       " 'drowned': -2,\n",
       " 'drowns': -2,\n",
       " 'drunk': -2,\n",
       " 'dubious': -2,\n",
       " 'dud': -2,\n",
       " 'dull': -2,\n",
       " 'dumb': -3,\n",
       " 'dumbass': -3,\n",
       " 'dump': -1,\n",
       " 'dumped': -2,\n",
       " 'dumps': -1,\n",
       " 'dupe': -2,\n",
       " 'duped': -2,\n",
       " 'dysfunction': -2,\n",
       " 'eager': 2,\n",
       " 'earnest': 2,\n",
       " 'ease': 2,\n",
       " 'easy': 1,\n",
       " 'ecstatic': 4,\n",
       " 'eerie': -2,\n",
       " 'eery': -2,\n",
       " 'effective': 2,\n",
       " 'effectively': 2,\n",
       " 'elated': 3,\n",
       " 'elation': 3,\n",
       " 'elegant': 2,\n",
       " 'elegantly': 2,\n",
       " 'embarrass': -2,\n",
       " 'embarrassed': -2,\n",
       " 'embarrasses': -2,\n",
       " 'embarrassing': -2,\n",
       " 'embarrassment': -2,\n",
       " 'embittered': -2,\n",
       " 'embrace': 1,\n",
       " 'emergency': -2,\n",
       " 'empathetic': 2,\n",
       " 'emptiness': -1,\n",
       " 'empty': -1,\n",
       " 'enchanted': 2,\n",
       " 'encourage': 2,\n",
       " 'encouraged': 2,\n",
       " 'encouragement': 2,\n",
       " 'encourages': 2,\n",
       " 'endorse': 2,\n",
       " 'endorsed': 2,\n",
       " 'endorsement': 2,\n",
       " 'endorses': 2,\n",
       " 'enemies': -2,\n",
       " 'enemy': -2,\n",
       " 'energetic': 2,\n",
       " 'engage': 1,\n",
       " 'engages': 1,\n",
       " 'engrossed': 1,\n",
       " 'enjoy': 2,\n",
       " 'enjoying': 2,\n",
       " 'enjoys': 2,\n",
       " 'enlighten': 2,\n",
       " 'enlightened': 2,\n",
       " 'enlightening': 2,\n",
       " 'enlightens': 2,\n",
       " 'ennui': -2,\n",
       " 'enrage': -2,\n",
       " 'enraged': -2,\n",
       " 'enrages': -2,\n",
       " 'enraging': -2,\n",
       " 'enrapture': 3,\n",
       " 'enslave': -2,\n",
       " 'enslaved': -2,\n",
       " 'enslaves': -2,\n",
       " 'ensure': 1,\n",
       " 'ensuring': 1,\n",
       " 'enterprising': 1,\n",
       " 'entertaining': 2,\n",
       " 'enthral': 3,\n",
       " 'enthusiastic': 3,\n",
       " 'entitled': 1,\n",
       " 'entrusted': 2,\n",
       " 'envies': -1,\n",
       " 'envious': -2,\n",
       " 'envy': -1,\n",
       " 'envying': -1,\n",
       " 'erroneous': -2,\n",
       " 'error': -2,\n",
       " 'errors': -2,\n",
       " 'escape': -1,\n",
       " 'escapes': -1,\n",
       " 'escaping': -1,\n",
       " 'esteemed': 2,\n",
       " 'ethical': 2,\n",
       " 'euphoria': 3,\n",
       " 'euphoric': 4,\n",
       " 'eviction': -1,\n",
       " 'evil': -3,\n",
       " 'exaggerate': -2,\n",
       " 'exaggerated': -2,\n",
       " 'exaggerates': -2,\n",
       " 'exaggerating': -2,\n",
       " 'exasperated': 2,\n",
       " 'excellence': 3,\n",
       " 'excellent': 3,\n",
       " 'excite': 3,\n",
       " 'excited': 3,\n",
       " 'excitement': 3,\n",
       " 'exciting': 3,\n",
       " 'exclude': -1,\n",
       " 'excluded': -2,\n",
       " 'exclusion': -1,\n",
       " 'exclusive': 2,\n",
       " 'excuse': -1,\n",
       " 'exempt': -1,\n",
       " 'exhausted': -2,\n",
       " 'exhilarated': 3,\n",
       " 'exhilarates': 3,\n",
       " 'exhilarating': 3,\n",
       " 'exonerate': 2,\n",
       " 'exonerated': 2,\n",
       " 'exonerates': 2,\n",
       " 'exonerating': 2,\n",
       " 'expand': 1,\n",
       " 'expands': 1,\n",
       " 'expel': -2,\n",
       " 'expelled': -2,\n",
       " 'expelling': -2,\n",
       " 'expels': -2,\n",
       " 'exploit': -2,\n",
       " 'exploited': -2,\n",
       " 'exploiting': -2,\n",
       " 'exploits': -2,\n",
       " 'exploration': 1,\n",
       " 'explorations': 1,\n",
       " 'expose': -1,\n",
       " 'exposed': -1,\n",
       " 'exposes': -1,\n",
       " 'exposing': -1,\n",
       " 'extend': 1,\n",
       " 'extends': 1,\n",
       " 'exuberant': 4,\n",
       " 'exultant': 3,\n",
       " 'exultantly': 3,\n",
       " 'fabulous': 4,\n",
       " 'fad': -2,\n",
       " 'fag': -3,\n",
       " 'faggot': -3,\n",
       " 'faggots': -3,\n",
       " 'fail': -2,\n",
       " 'failed': -2,\n",
       " 'failing': -2,\n",
       " 'fails': -2,\n",
       " 'failure': -2,\n",
       " 'failures': -2,\n",
       " 'fainthearted': -2,\n",
       " 'fair': 2,\n",
       " 'faith': 1,\n",
       " 'faithful': 3,\n",
       " 'fake': -3,\n",
       " 'fakes': -3,\n",
       " 'faking': -3,\n",
       " 'fallen': -2,\n",
       " 'falling': -1,\n",
       " 'falsified': -3,\n",
       " 'falsify': -3,\n",
       " 'fame': 1,\n",
       " 'fan': 3,\n",
       " 'fantastic': 4,\n",
       " 'farce': -1,\n",
       " 'fascinate': 3,\n",
       " 'fascinated': 3,\n",
       " 'fascinates': 3,\n",
       " 'fascinating': 3,\n",
       " 'fascist': -2,\n",
       " 'fascists': -2,\n",
       " 'fatalities': -3,\n",
       " 'fatality': -3,\n",
       " 'fatigue': -2,\n",
       " 'fatigued': -2,\n",
       " 'fatigues': -2,\n",
       " 'fatiguing': -2,\n",
       " 'favor': 2,\n",
       " 'favored': 2,\n",
       " 'favorite': 2,\n",
       " 'favorited': 2,\n",
       " 'favorites': 2,\n",
       " 'favors': 2,\n",
       " 'fear': -2,\n",
       " 'fearful': -2,\n",
       " 'fearing': -2,\n",
       " 'fearless': 2,\n",
       " 'fearsome': -2,\n",
       " 'fed up': -3,\n",
       " 'feeble': -2,\n",
       " 'feeling': 1,\n",
       " 'felonies': -3,\n",
       " 'felony': -3,\n",
       " 'fervent': 2,\n",
       " 'fervid': 2,\n",
       " 'festive': 2,\n",
       " 'fiasco': -3,\n",
       " 'fidgety': -2,\n",
       " 'fight': -1,\n",
       " 'fine': 2,\n",
       " 'fire': -2,\n",
       " 'fired': -2,\n",
       " 'firing': -2,\n",
       " 'fit': 1,\n",
       " 'fitness': 1,\n",
       " 'flagship': 2,\n",
       " 'flees': -1,\n",
       " 'flop': -2,\n",
       " 'flops': -2,\n",
       " 'flu': -2,\n",
       " ...}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize('I love you! Beatiful Girl!')\n",
    "# 把这个打分表记录在一个Dict上以后\n",
    "# 跑一遍整个句子，把对应的值相加\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in words)\n",
    "# 有值就是Dict中的值，没有就是0\n",
    "\n",
    "# 于是就得到了一个sentiment score\n",
    "total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，这个方法太Naive。\n",
    "- 新词怎么半？\n",
    "- 特殊词汇怎么办？\n",
    "- 更深层次的玩意儿怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 升级：配上ML的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# 随手造训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "\n",
    "def preprocess(s):\n",
    "    # Func：句子处理\n",
    "    # 这里简单的用了split()，把句子中每个单词分开\n",
    "    # 显然，还有更多的processiing method可以用\n",
    "    return {word:True for word in s.lower().split()}\n",
    "    # return⻓长这样:\n",
    "    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}\n",
    "    # 其中, 前一个叫fname, 对应每个出现的文本单词;\n",
    "    # 后一个叫fval, 指的是每个⽂本单词对应的值。\n",
    "    # 这里我们用最简单的True,来表示,这个词『出现在当前的句句⼦子中』的意义。\n",
    "    # 当然啦, 我们以后可以升级这个方程, 让它带有更加⽜逼的fval, ⽐如 word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "# 把训练集给做成标准形式\n",
    "training_data = [[preprocess(s1), 'pos'],\n",
    "                 [preprocess(s2), 'pos'],\n",
    "                 [preprocess(s3), 'neg'],\n",
    "                 [preprocess(s4), 'neg'],\n",
    "                ]\n",
    "\n",
    "# 喂给model\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "\n",
    "print(model.classify(preprocess('this is a good book')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='similarity'>3.2 文本相似度</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk14.png' width='60%'/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用元素频率表示文本特征\n",
    "<img src='./images/nltk15.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 余弦定理\n",
    "$$similarity = \\cos \\theta = \\frac{A \\cdot B}{||A|| ||B||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency频率统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# 做个词库\n",
    "corpus = 'this is my sentence this is my life this is the day'\n",
    "\n",
    "# 文本预处理tokenize\n",
    "# 可以根据需要做任何的preprocessing: stopwords, lemma, stemming , etc\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 借用NLTK的FreqDist统计一下文字出现的评率\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# fdist就类似于一个Dict,带上某个单词，可以看到它在整个文章中出现的次数。\n",
    "print(fdist['is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'this': 3, 'is': 3, 'my': 2, 'sentence': 1, 'life': 1, 'the': 1, 'day': 1})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[('this', 3), ('is', 3), ('my', 2), ('sentence', 1), ('life', 1), ('the', 1), ('day', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 把最常用的50个单词拿出来\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "size = len(standard_freq_vector)\n",
    "print(size)\n",
    "print(standard_freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 0, 'is': 1, 'my': 2, 'sentence': 3, 'life': 4, 'the': 5, 'day': 6}\n"
     ]
    }
   ],
   "source": [
    "# Func: 按照出现频率大小，记录下每一个单词的位置\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter +=1\n",
    "    return res\n",
    "\n",
    "# 把标准的单词位置记录下来，得到一个位置对照表\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "print(standard_position_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 这时，如果我们有一个新句子\n",
    "sentence = 'this is cool'\n",
    "\n",
    "# 先新建一个跟我们的标准vector同样大小的向量\n",
    "freq_vector = [0] * size\n",
    "\n",
    "# 简单的preprocessing\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# 对这个新句子里面的每一个单词\n",
    "for word in tokens:\n",
    "    try:\n",
    "        # 如果在我们的词库中出现过，那么就在“标准位置“上+1\n",
    "        freq_vector[standard_position_dict[word]] +=1\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "print(freq_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='classification'>3.3 文本分类</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk16.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- TF: Term Frequency，衡量一个term在文档中出现得有多频繁。\n",
    "\n",
    "$$TF(t) = \\frac{t出现在文档中的次数}{文档中term的总数}$$\n",
    "\n",
    "- IDF: Inverse Document Frequency，衡量一个term有多重要。\n",
    "有写词出现的很多，但是明显不是很有卵用。比如：'is','the','and'之类的。\n",
    "为了平衡，我们把罕见的词的重要性（weight）搞高，把常见词的重要性搞低。\n",
    "\n",
    "$$IDF(t) = \\log_e \\frac{文档总数}{含有t的文档总数}$$\n",
    "\n",
    "$$TF-IDF = TF*IDF$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='deeplearning'>4.深度学习加持</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='autoencoder'>4.1 Autoencoder</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='word2vec'>4.2 Word2Vec</a></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
