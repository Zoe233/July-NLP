{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a>Lec 02 NLP基础与扫盲</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节以NLTK为基础配合讲解自然语言处理的原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录：\n",
    "- <a href='#nltk'>NLTK</a>\n",
    "- <a href='#textprocessing'>文本处理流程</a>\n",
    "    - <a href='#tokenize'>分词</a>\n",
    "    - <a href='#standard'>归一化</a>\n",
    "    - <a href='#stopwords'>停止词</a>\n",
    "- <a href='#3examples'>NLP经典三案例</a>\n",
    "    - <a href='#sentiment'>情感分析</a>\n",
    "    - <a href='#similarity'>文本相似度</a>\n",
    "    - <a href='#classification'>文本分类</a>\n",
    "- <a href='#deeplearning'>深度学习加持</a>\n",
    "    - <a herf='#autoencoder'>Autoencoder</a>\n",
    "    - <a herf='#word2vec'>Word2Vec</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='nltk'>1.NLTK</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官网：http://www.nltk.org/\n",
    "\n",
    "Python上著名的自然语言处理库。自带语料库、词性分类库、自带分类、分词，等等功能强大的社区支持，还有N多的简单版wrapper。\n",
    "\n",
    "textblob就是NLTK的简版版wrapper。\n",
    "参考网址：https://textblob.readthedocs.io/en/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# 安装语料库\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK功能一览表\n",
    "|NLTK Modules|Functionality||\n",
    "|:-|:-:|:-:|\n",
    "|nltk.corpus|Corpus|语料|\n",
    "|nltk.tokenize, nltk.stem| Tokenizers, stemmers|分词、词干提取|\n",
    "|nltk.collocations|t-test, chi-squared, mutual-info|配置|\n",
    "|nltk.tag|n-gram, backoff, Brill, HMM, TnT||\n",
    "|nltk.classify, nltk.cluster| Decision tree, Naive Bayes, K-means|分类器|\n",
    "|nltk.chunk| Regex, n-gram, named entity||\n",
    "|nltk.parsing| Parsing|treebank 主谓宾，文法树，主要用于NER|\n",
    "|nltk.sem, nltk.interence| Semantic interpretation|情感分析|\n",
    "|nltk.metrics| Evaluation metrics|测量|\n",
    "|nltk.probability| Probability & Estimation|概率|\n",
    "|nltk.app, nltk.chat| Applications|应用|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK自带语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='textprocessing'>2.文本处理流程</a><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征工程：将文本分词，（词性标注、词干提取（词性还原）），中止词去除，得到word list，最终数字化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk01.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK文本处理流程：一条typical的文本预处理流水线</h3>\n",
    "\n",
    "<img src='./images/nltk10.png' width='35%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='tokenize'>2.1 分词</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize\n",
    "\n",
    "将长句子拆成有“意义”的小部件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = 'hello, world!'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中英文NLP的区别\n",
    "英文天然带有分隔符，用空格“ ”作为分隔符。\n",
    "中文等非拉丁语言，分词就会有两种：\n",
    "- rule: 启发式（查字典）\n",
    "- generative: 机器学习/统计方法：HMM、CRF\n",
    "\n",
    "<img src='./images/nltk02.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w2/qnnfb62x2g760j3nkh9q4ptr0000gn/T/jieba.cache\n",
      "Loading model cost 0.720 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/来到/北京/清华/清华大学/华大/大学\n"
     ]
    }
   ],
   "source": [
    "# 中文分词\n",
    "import jieba\n",
    "seg_list = jieba.cut('我来到北京清华大学', cut_all=True) # 全模式\n",
    "print('Full Mode:', '/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/来到/北京/清华大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('我来到北京清华大学', cut_all=False) # 精确模式\n",
    "print('Default Mode:', '/'.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 他,来到,了,网易,杭研,大厦\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('他来到了网易杭研大厦') # 默认是精确模式\n",
    "print('Default Mode:', ','.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【新词识别】：他, 来到, 了了, ⽹网易易, 杭研, ⼤大厦    \n",
    "(此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了了)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明,硕士,毕业,于,中国,科学,学院,科学院,中国科学院,计算,计算所,，,后,在,日本,京都,大学,日本京都大学,深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search('小明硕士毕业于中国科学院计算所，后在日本京都大学深造') # 搜索引擎模式\n",
    "print(','.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词之后的效果：list of words\n",
    "<img src='./images/nltk04.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有时候tokenize没那么简单\n",
    "比如：  \n",
    "社交网络上，这些乱七八糟的不和语法不合逻辑的语言很多：   \n",
    "拯救@某人，表情符号，URL，#话题符号\n",
    "\n",
    "<img src='./images/nltk05.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@', 'angelababy', ':', 'love', 'you', 'baby', '!', ':', 'D', 'http', ':', '//ah.love', '#', '168cm']\n"
     ]
    }
   ],
   "source": [
    "# 社交网络语言的tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对社交网络语言的tokenize，在预处理过程中，用re正则表达式预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正则表达式\n",
    "对照表：\n",
    "http://www.regexlab.com/zh/regref.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "emoticons_str = r'''\n",
    "(?:\n",
    "    [:=;] #眼睛\n",
    "    [oO\\-]? # ⿐子\n",
    "    [D\\)\\]\\(\\]/\\\\OpP] # 嘴\n",
    ")\n",
    "'''\n",
    "regex_str = [emoticons_str, \n",
    "            r'<[^>]+>', # HTML tags\n",
    "            r'(?:@[\\w_]+)', # @某⼈\n",
    "            r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # 话题标签\n",
    "            r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    "            # URLs\n",
    "            r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # 数字\n",
    "            r\"(?:[a-z][a-z'\\-_]+[a-z])\", # 含有 - 和 ‘ 的单词\n",
    "            r'(?:[\\w_]+)', # 其他\n",
    "            r'(?:\\S)' # 其他\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s, lowercase=False):\n",
    "    def tokenize(s):\n",
    "        return tokens_re.findall(s)\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokenize = [token if emotion_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@angelababy', ':', 'love', 'you', 'baby', '!', ':D', 'http://ah.love', '#168cm']\n"
     ]
    }
   ],
   "source": [
    "tweet = 'RT @angelababy: love you baby! :D http://ah.love #168cm'\n",
    "print(preprocess(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='standard'>2.2 归一化</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 纷繁复杂的词形\n",
    "\n",
    "<img src='./images/nltk06.png' width='50%'/>\n",
    "<img src='./images/nltk07.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词形归一化\n",
    "- Stemming 词干提取：一般来说，就是把不影响词性的inflection的小尾巴砍掉\n",
    "    - walking 砍掉ing => walk\n",
    "    - walked 砍掉ed => walk\n",
    "    \n",
    "- Lemmatization 词形归一：把各种类型的词的变形，都归为一个形式（“查表”的方式）\n",
    "    - went 归一 => go\n",
    "    - are 归一 => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Stemming：nltk.stem.porter.PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipli'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'provis'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('provision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('went')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer.stem('wenting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Stemming：nltk.stem.SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maximum'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "snowball_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Stemming：nltk.stem.lancaster.LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maxim'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "lancaster_stemmer.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presum'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmer.stem('presumably')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现Lemma: nltk.stem.WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'church'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('churches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardwolf'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('aardwolves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abacus'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('abaci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hardrock'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('hardrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma的小问题\n",
    "<img src='./images/nltk08.png' width='33%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK更好地实现Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 没有PosTag词性标注，默认是 NN(名词)\n",
    "wordnet_lemmatizer.lemmatize('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加上Pos Tag（词性标注）\n",
    "wordnet_lemmatizer.lemmatize('is', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词形标注Part-Of-Speech\n",
    "<img src='./images/nltk09.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'does', 'the', 'fox', 'say']\n",
      "[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "# NLTK的词性标注 POS Tag\n",
    "import nltk\n",
    "text = nltk.word_tokenize('what does the fox say') # 先分词\n",
    "print(text)\n",
    "print(nltk.pos_tag(text)) # 后POS Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='stopwords'>2.3 停止词</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一千个he，有一千种指代。    \n",
    "一千个the，有一千种指事。\n",
    "\n",
    "对于注重理解文本【意思】的应用场景来说，歧义太多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全体stopwords列表：http://www.ranks.nl/stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK去除stopwords\n",
    "首先，记得在console里面加载一下词库，或者，nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'chinese', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# 先tokenize分词，得到word_list\n",
    "word_list = nltk.word_tokenize('I am chinese.')\n",
    "# 然后filter一把\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='3examples'>3.NLP经典三案例</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "什么是自然语言处理？\n",
    "\n",
    "<img src='./images/nltk11.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本预处理让我们得到了什么？\n",
    "<img src='./images/nltk12.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK在NLP上的经典应用：情感分析、文本相似度、文本分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='sentiment'>3.1 情感分析</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工业界还是很受欢迎，常用于行研报告，舆情分析。\n",
    "<img src='./images/nltk13.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的sentiment dictionary:\n",
    "- like 1\n",
    "- good 2\n",
    "- bad -2\n",
    "- terrible -3\n",
    "\n",
    "类似于关键词打分机制\n",
    "\n",
    "比如：AFINN-111\n",
    "http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工业界确实是有公司用**中文的情感词库**，通过情感词库的关键词打分，最后得出结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK完成简单的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dictionary = {}\n",
    "for line in open('./AFINN/AFINN-111.txt'):\n",
    "    word, score = line.split('\\t')\n",
    "    sentiment_dictionary[word] = int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "print(len(sentiment_dictionary))\n",
    "print(sentiment_dictionary['crash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize('I love you! Beatiful Girl!')\n",
    "# 把这个打分表记录在一个Dict上以后\n",
    "# 跑一遍整个句子，把对应的值相加\n",
    "total_score = sum(sentiment_dictionary.get(word, 0) for word in words)\n",
    "# 有值就是Dict中的值，没有就是0\n",
    "\n",
    "# 于是就得到了一个sentiment score\n",
    "total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，这个方法太Naive。   \n",
    "好处是：稳定。   \n",
    "坏处是：不够高级。   \n",
    "- 新词怎么半？\n",
    "- 特殊词汇怎么办？\n",
    "- 更深层次的玩意儿怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 升级：配上ML的情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# 随手造训练集\n",
    "s1 = 'this is a good book'\n",
    "s2 = 'this is a awesome book'\n",
    "s3 = 'this is a bad book'\n",
    "s4 = 'this is a terrible book'\n",
    "\n",
    "def preprocess(s):\n",
    "    # Func：句子处理\n",
    "    # 这里简单的用了split()，把句子中每个单词分开\n",
    "    # 显然，还有更多的processiing method可以用\n",
    "    return {word:True for word in s.lower().split()}\n",
    "    # return⻓长这样:\n",
    "    # {'this': True, 'is':True, 'a':True, 'good':True, 'book':True}\n",
    "    # 其中, 前一个叫fname, 对应每个出现的文本单词;\n",
    "    # 后一个叫fval, 指的是每个⽂本单词对应的值。\n",
    "    # 这里我们用最简单的True,来表示,这个词『出现在当前的句句⼦子中』的意义。\n",
    "    # 当然啦, 我们以后可以升级这个方程, 让它带有更加⽜逼的fval, ⽐如 word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': True, 'is': True, 'a': True, 'good': True, 'book': True}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess('this is a good book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n"
     ]
    }
   ],
   "source": [
    "# 把训练集给做成标准形式\n",
    "training_data = [[preprocess(s1), 'pos'],\n",
    "                 [preprocess(s2), 'pos'],\n",
    "                 [preprocess(s3), 'neg'],\n",
    "                 [preprocess(s4), 'neg'],\n",
    "                ]\n",
    "\n",
    "# 喂给model\n",
    "model = NaiveBayesClassifier.train(training_data)\n",
    "\n",
    "print(model.classify(preprocess('this is a good boy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'this': True, 'is': True, 'a': True, 'good': True, 'book': True}, 'pos'],\n",
       " [{'this': True, 'is': True, 'a': True, 'awesome': True, 'book': True}, 'pos'],\n",
       " [{'this': True, 'is': True, 'a': True, 'bad': True, 'book': True}, 'neg'],\n",
       " [{'this': True, 'is': True, 'a': True, 'terrible': True, 'book': True},\n",
       "  'neg']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='similarity'>3.2 文本相似度</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk14.png' width='60%'/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用元素频率表示文本特征\n",
    "<img src='./images/nltk15.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为向量就是：\n",
    "{1,0,3,0,1,1},    \n",
    "{1,0,2,0,1,1},   \n",
    "{0,1,0,1,0,0},   此处用词频表示句子的特征向量。\n",
    "\n",
    "通过**余弦定理**计算两个向量在二维空间内的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 余弦定理\n",
    "$$similarity = \\cos \\theta = \\frac{A \\cdot B}{||A|| ||B||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于二维空间，根据向量点积公式，显然可以得知：\n",
    "$$\\cos \\theta = \\frac{A \\cdot B}{||A|| ||B||}$$\n",
    "\n",
    "假设向量a,b的坐标分别为$(x_1,y_1)$,$(x_2,y_2)$，则：\n",
    "$$\\cos \\theta = \\frac{x_1x_2 + y_1 y_2}{\\sqrt{x_1^2+y_1^2} \\sqrt{x_2^2+y_2^2}}$$\n",
    "\n",
    "设向量A=$(A_1, A_2, ..., A_n)$, B=$(B_1, B_2,..., B_n)$。推广到多维：\n",
    "$$\\cos \\theta=\\frac{\\sum A_i * B_i}{\\sqrt{\\sum A_i^2}\\sqrt{\\sum B_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余弦值的范围在[-1,1]之间，\n",
    "- 值越趋近于1，代表两个向量的方向越接近；\n",
    "- 越趋近于-1，他们的方向越相反；\n",
    "- 接近于0，表示两个向量近乎于正交。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最常见的应用就是计算文本相似度。将两个文本根据他们词，建立两个向量，计算这两个向量的余弦值，就可以知道两个文本在统计学方法中他们的相似度情况。实践证明，这是一个非常有效的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequency频率统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'my', 'sentence', 'this', 'is', 'my', 'life', 'this', 'is', 'the', 'day']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "# 做个词库\n",
    "corpus = 'this is my sentence this is my life this is the day'\n",
    "\n",
    "# 文本预处理tokenize\n",
    "# 可以根据需要做任何的preprocessing: stopwords, lemma, stemming , etc\n",
    "tokens = nltk.word_tokenize(corpus)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# 借用NLTK的FreqDist统计一下文字出现的评率\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# fdist就类似于一个Dict,带上某个单词，可以看到它在整个文章中出现的次数。\n",
    "print(fdist['is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'this': 3, 'is': 3, 'my': 2, 'sentence': 1, 'life': 1, 'the': 1, 'day': 1})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[('this', 3), ('is', 3), ('my', 2), ('sentence', 1), ('life', 1), ('the', 1), ('day', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 把最常用的50个单词拿出来\n",
    "standard_freq_vector = fdist.most_common(50)\n",
    "size = len(standard_freq_vector)\n",
    "print(size)\n",
    "print(standard_freq_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 0, 'is': 1, 'my': 2, 'sentence': 3, 'life': 4, 'the': 5, 'day': 6}\n"
     ]
    }
   ],
   "source": [
    "# Func: 按照出现频率大小，记录下每一个单词的位置\n",
    "def position_lookup(v):\n",
    "    res = {}\n",
    "    counter = 0\n",
    "    for word in v:\n",
    "        res[word[0]] = counter\n",
    "        counter +=1\n",
    "    return res\n",
    "\n",
    "# 把标准的单词位置记录下来，得到一个位置对照表\n",
    "standard_position_dict = position_lookup(standard_freq_vector)\n",
    "print(standard_position_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 这时，如果我们有一个新句子\n",
    "sentence = 'this is cool'\n",
    "\n",
    "# 先新建一个跟我们的标准vector同样大小的向量\n",
    "freq_vector = [0] * size\n",
    "\n",
    "# 简单的preprocessing\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# 对这个新句子里面的每一个单词\n",
    "for word in tokens:\n",
    "    try:\n",
    "        # 如果在我们的词库中出现过，那么就在“标准位置“上+1\n",
    "        freq_vector[standard_position_dict[word]] +=1\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "print(freq_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='classification'>3.3 文本分类</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk16.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在文本分类中，用TF-IDF来表示文本的特征向量。\n",
    "\n",
    "相对于词频统计，在业界，TF-IDF是用得非常多的。\n",
    "词频是可以用的，但是对于单词的选取是有一定的问题的，计算出来的相似度没有一定的代表性。更加推荐TF-IDF。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "- TF: Term Frequency，衡量一个term在文档中出现得有多频繁。\n",
    "\n",
    "$$TF(t) = \\frac{t出现在文档中的次数}{文档中term的总数}$$\n",
    "\n",
    "- IDF: Inverse Document Frequency，衡量一个term有多重要。\n",
    "有写词出现的很多，但是明显不是很有卵用。比如：'is','the','and'之类的。\n",
    "为了平衡，我们把罕见的词的重要性（weight）搞高，把常见词的重要性搞低。\n",
    "\n",
    "$$IDF(t) = \\log_e \\frac{文档总数}{含有t的文档总数}$$\n",
    "\n",
    "$$TF-IDF = TF*IDF$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子：   \n",
    "一个文档有100个词，其中单词baby出现了3次。   \n",
    "那么，TF(baby) = (3/100) =0.03。\n",
    "\n",
    "好，现在我们如果有10M的文档，baby出现在其中的1000个文档中。   \n",
    "那么，IDF(baby) = $\\log_{10} (10000000/1000) $= 4\n",
    "\n",
    "所以，$TF-IDF(baby) = TF(baby) * IDF(baby) = 0.03 * 4 =0.12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(10000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学之美中是以2为底的log函数，此处文档中是以e为底的log函数，但示例中是以10为底的log函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK实现TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "# ⾸先, 把所有的⽂档放到TextCollection类中。\n",
    "# 这个类会⾃动帮你断句, 做统计, 做计算\n",
    "corpus = TextCollection(['this is sentence one',\n",
    "                                'this is sentence two',\n",
    "                                'this is sentence three'])\n",
    "# 直接就能算出tfidf\n",
    "# (term: ⼀句话中的某个term, text: 这句话)\n",
    "print(corpus.tf_idf('this', 'this is sentence four'))\n",
    "# 0.444342\n",
    "\n",
    "# 同理, 怎么得到⼀个标准⼤小的vector来表示所有的句子?\n",
    "# 对于每个新句子\n",
    "new_sentence = 'this is sentence five'\n",
    "# 遍历⼀遍所有的vocabulary中的词:\n",
    "for word in corpus.tokens:\n",
    "    print(corpus.tf_idf(word, new_sentence))\n",
    "# 我们会得到⼀个巨长(=所有vocab⻓度)的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "\n",
    "# 就所有文档放到TextCollection类中，该类会自动断句，做统计，做计算\n",
    "corpus = ['this is sentence one','this is sentence two', 'this is sentence three']\n",
    "tokens = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
    "textC= TextCollection(tokens)\n",
    "\n",
    "# 直接算出tf_idf\n",
    "print(textC.tf_idf('this', 'this is sentence one'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textC.idf('go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "from nltk.text import TextCollection\n",
    "from nltk.book import text1,text2,text3\n",
    "gutenberg = TextCollection(nltk.corpus.gutenberg)\n",
    "mytexts = TextCollection([text1, text2, text3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(mytexts.tf('the', 'in the world'))\n",
    "print(mytexts.tf_idf('the', 'in the world'))\n",
    "print(mytexts.idf('the'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：此处nltk.text.TextCollection跑出来结果很奇怪，很难获得如最上面示例的结果。可能是包的安装不完全，或者是用法改变等原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**接下来？可以用机器学习的模型**：\n",
    "\n",
    "<img src='./images/nltk17.png' width='30%'/>\n",
    "\n",
    "文本分类中，Label可以是：小说，新闻，等类别标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='deeplearning'>4.深度学习加持</a></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep learning** is a branch of machine learning based on a set of algorithms that attempt to model **high-level abbstractions** in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk18.png' width='40%'/>\n",
    "\n",
    "Keras:基于Theano和TensorFlow的深度学习库   \n",
    "Keras是一个高层神经网络API，Keras由纯Python编写而成并基Tensorflow、Theano以及CNTK后端。Keras 为支持快速实验而生，能够把你的idea迅速转换为结果，如果你有如下需求，请选择Keras：\n",
    "- 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）\n",
    "- 支持CNN和RNN，或二者的结合\n",
    "- 无缝CPU和GPU切换\n",
    "\n",
    "gensim:业界最好的做Word2Vec的库。   \n",
    "Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。   \n",
    "- 它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法， \n",
    "- 支持流式训练，\n",
    "- 并提供了诸如相似度计算，信息检索等一些常用任务的API接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='autoencoder'>4.1 Auto-Encoder</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk19.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自动编码器是一种数据的**压缩算法**，其中数据的**压缩**和**解压缩**函数是数据相关的、有损的、从样本中自动学习的。\n",
    "- Data-specific 只能指定模型学习的数据，如学习的是动物图片，就不能勇看来数字图片。\n",
    "- Lossy 有损耗的\n",
    "- Learn from examples\n",
    "\n",
    "在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）自动编码器是数据相关的（data-specific 或 data-dependent），这意味着自动编码器只能压缩那些与训练数据类似的数据。比如，使用人脸训练出来的自动编码器在压缩别的图片，比如树木时性能很差，因为它学习到的特征是与人脸相关的。\n",
    "\n",
    "2）自动编码器是有损的，意思是解压缩的输出与原来的输入相比是退化的，MP3，JPEG等压缩算法也是如此。这与无损压缩算法不同。\n",
    "\n",
    "3）自动编码器是从数据样本中自动学习的，这意味着很容易对指定类的输入训练出一种特定的编码器，而不需要完成任何新工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建一个自动编码器需要完成下面三样工作：\n",
    "- 搭建编码器，\n",
    "- 搭建解码器，\n",
    "- 设定一个损失函数，用以衡量由于压缩而损失掉的信息。\n",
    "\n",
    "编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。\n",
    "编码器和解码器的参数可以通过**最小化损失函数而优化**，例如SGD。\n",
    "\n",
    "自编码器是一个**自监督的算法，并不是一个无监督算法**。    \n",
    "自监督学习是监督学习的一个实例，其标签产生自输入数据。要获得一个自监督的模型，你需要**一个靠谱的目标跟一个损失函数**，仅仅把目标设定为重构输入可能不是正确的选项。     \n",
    "基本上，要求模型在像素级上精确重构输入不是机器学习的兴趣所在，**学习到高级的抽象特征才是。**   \n",
    "事实上，当主要任务是**分类、定位**之类的任务时，那些对这类任务而言的最好的特征基本上都是重构输入时的最差的那种特征。\n",
    "\n",
    "目前自编码器的应用主要有两个方面：\n",
    "- 第一是数据去噪，\n",
    "- 第二是为进行可视化而降维。    \n",
    "配合适当的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。\n",
    "\n",
    "对于2D的数据可视化，t-SNE（读作tee-snee）或许是目前最好的算法，但通常还是需要原数据的维度相对低一些。   \n",
    "所以，**可视化高维数据的一个好办法是**:\n",
    "- 首先使用自编码器将维度降低到较低的水平（如32维），\n",
    "- 然后再使用t-SNE将其投影在2D平面上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASCIIAutoencoder():\n",
    "    '''基于字符的Autoencoder.'''\n",
    "    \n",
    "    def __init__(self, sen_len=512, encoding_dim=32, epoch=50, val_ratio=0.3):\n",
    "        '''\n",
    "        :param sen_len: 把sentences pad成相同的长度\n",
    "        :param encoding_dim: 压缩后的维度dim\n",
    "        :param epoch: 要跑多少epoch\n",
    "        :param kmeanmodel: 简单的KNN Clustering 模型\n",
    "        把512长度的长度压缩成32位的编码。\n",
    "        '''\n",
    "        self.sen_len = sen_len\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.autoencoder = None\n",
    "        self.encoder = None\n",
    "        self.kmeanmodel = KMeans(n_clusters=2)\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def fit(self, x):\n",
    "        '''\n",
    "        模型构建。\n",
    "        :param x: input text\n",
    "        '''\n",
    "        # 把所有的trainset都搞成同一个size，并把每一个字符都换成ascii码\n",
    "        x_train = self.preprocess(x, length=self.sen_len)\n",
    "        # 然后给input预留好位置\n",
    "        input_text = Input(shape=(self.sen_len, ))\n",
    "        # 'encoded' 每经过一层，都被刷新成小一点的“压缩后表达式”\n",
    "        encoded = Dense(1024, activation='tanh')(input_text)\n",
    "        encoded = Dense(512, activation='tanh')(encoded)\n",
    "        encoded = Dense(128, activation='tanh')(encoded)\n",
    "        encoded = Dense(self.encoding_dim, activation='tanh')(encoded)\n",
    "        \n",
    "        # 'decoded' 就是把刚刚压缩完的东西，给反过来还原成input_text\n",
    "        decoded = Dense(128, activation='tanh')(encoded)\n",
    "        decoded = Dense(512, activation='tanh')(decoded)\n",
    "        decoded = Dense(1024, activation='tanh')(decoded)\n",
    "        decoded = Dense(self.sen_len, activation='sigmoid')(decoded)\n",
    "        \n",
    "        # 整个从大到小再到大的model，叫autoencoder\n",
    "        self.autoencoder = Model(input=input_text, output=decoded)\n",
    "        \n",
    "        # 那么，只从大到小（也就是一半的model）就叫 encoder\n",
    "        self.encoder = Model(input=input_text, output=encoded)\n",
    "        \n",
    "        # 同理，我们接下来搞一个decoder出来，也就是从小到大的model\n",
    "        # 首先，encoded的input size给预留好\n",
    "        encoded_input = Input(shape=(1024,))\n",
    "        # autoencoder的最后一层，就应该是decoder的第一层\n",
    "        decoder_layer = self.autoencoder.layers[-1]\n",
    "        # 然后，我们从头到尾连起来，就是一个decoder了！\n",
    "        decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "        \n",
    "        # compile\n",
    "        self.autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # 跑起来\n",
    "        self.autoencoder.fit(x_train, x_train, nb_epoch=self.epoch, batch_size=1000, shuffle=True)\n",
    "        \n",
    "        # 这一部分是自己拿自己train一下KNN，一件简单的基于距离的分类器\n",
    "        x_train = self.encoder.predict(x_train)\n",
    "        self.kmeanmodel.fit(x_train)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        做预测。\n",
    "        :param x: input text\n",
    "        :param: predictions\n",
    "        '''\n",
    "        # 同理，第一步：把来的都搞成ASCII化，并且长度相同\n",
    "        x_test = self.preprocess(x, length=self.sen_len)\n",
    "        # 然后用encoder把test集给压缩\n",
    "        x_test = self.encoder.predict(x_test)\n",
    "        # KNN给分类出来\n",
    "        preds = self.kmeanmodel.predict(x_test)\n",
    "        \n",
    "        return preds\n",
    "    \n",
    "    def preprocess(self, s_list, length=256):\n",
    "        ''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_2:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='word2vec'>4.2 Word2Vec</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk20.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lexical Taxonomy 词汇分类：WordNet (Miller, 1990)\n",
    "- Symbolic Representation 符号表示：One-Hot (Turian et al., 2010)\n",
    "- Distributional Similarity Based Representation 相似度表示：\n",
    "- Full document: TF-IDF (Joachims, 1996)\n",
    "- Window: co-occurrence matrix + SVD (Bullinaria & Levy, 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "语料：   \n",
    "Are you kidding?   \n",
    "No, I am serious?   \n",
    "I am kidding.   \n",
    "You are serious.  \n",
    "Are you serious?   \n",
    "Am I kidding?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window=1时的矩阵化表示：\n",
    "<img src='./images/nltk21.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk22.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk23.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk24.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nltk25.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip-gram**:\n",
    "    works well with small amount of the training data, represents well even rate words or phrases.\n",
    "\n",
    "**CBOW**:\n",
    "    several times faster to train than the skip-gram, slightly better accuracy for the frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学习之自编码器AutoEncoder - CSDN\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 去除HTML，用到BeautifulSoup\n",
    "raw_text = requests.get('https://blog.csdn.net/marsjhao/article/details/73480859')\n",
    "beautiful_text = BeautifulSoup(raw_text.text, 'html.parser').get_text()\n",
    "\n",
    "print(beautiful_text[20:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 把字母的去掉，用正则表达式解决\n",
    "import re\n",
    "chinese_only = re.sub(r'[a-zA-Z]','', beautiful_text)\n",
    "# 去除\\n\\t等换行符\n",
    "text = re.sub(r'\\\\n?','',chinese_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n深度学习之自编码器 - 博客\\n\\n\\n\\n          = \"\";\\n         _ = \"://../\";\\n         _ = \"://.///\";\\n          = \"\";\\n          = ;\\n          = ;\\n          = \"://..//?=://..////73480859\"\\n          = \"://../\";\\n        //页面皮肤样式\\n          = \"-\";\\n    \\n\\n        //         \\n         _ = _ || [];\\n        (() {\\n              = .(\"\");\\n            . = \"://../.?65251933243997715\";\\n              = .(\"\")[0];\\n            ..(, );\\n        })();\\n        //          \\n    \\n\\n\\n\\n\\n\\n\\n\\n        ., ._, ._{\\n            : \\n        }\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n原 深度学习之自编码器\\n\\n\\n\\n2017年06月20日 10:13:32\\n阅读数：31905\\n\\n标签：\\n深度学习\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t自编码器\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t卷积自编码器\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n个人分类：\\n机器学习/深度学习\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t版权声明：本文为博主原创文章，未经博主允许不得转载。\\t\\t\\t\\t\\t://..////73480859\\t\\t\\t\\t\\n\\n\\n一、什么是自编码器（）\\n\\n自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的。在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。\\n1）自动编码器是数据相关的（- 或 -），这意味着自动编码器只能压缩那些与训练数据类似的数据。比如，使用人脸训练出来的自动编码器在压缩别的图片，比如树木时性能很差，因为它学习到的特征是与人脸相关的。\\n2）自动编码器是有损的，意思是解压缩的输出与原来的输入相比是退化的，3，等压缩算法也是如此。这与无损压缩算法不同。\\n3）自动编码器是从数据样本中自动学习的，这意味着很容易对指定类的输入训练出一种特定的编码器，而不需要完成任何新工作。\\n搭建一个自动编码器需要完成下面三样工作：搭建编码器，搭建解码器，设定一个损失函数，用以衡量由于压缩而损失掉的信息。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化，例如。\\n自编码器是一个自监督的算法，并不是一个无监督算法。自监督学习是监督学习的一个实例，其标签产生自输入数据。要获得一个自监督的模型，你需要一个靠谱的目标跟一个损失函数，仅仅把目标设定为重构输入可能不是正确的选项。基本上，要求模型在像素级上精确重构输入不是机器学习的兴趣所在，学习到高级的抽象特征才是。事实上，当主要任务是分类、定位之类的任务时，那些对这类任务而言的最好的特征基本上都是重构输入时的最差的那种特征。\\n目前自编码器的应用主要有两个方面，第一是数据去噪，第二是为进行可视化而降维。配合适当的维度和稀疏约束，自编码器可以学习到比等技术更有意思的数据投影。\\n对于2的数据可视化，-（读作-）或许是目前最好的算法，但通常还是需要原数据的维度相对低一些。所以，可视化高维数据的一个好办法是首先使用自编码器将维度降低到较低的水平（如32维），然后再使用-将其投影在2平面上。\\n二、几种自编码器\\n自编码器（）是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器（））内部有一个隐藏层 ，可以产生编码（）表示输入。该网络可以看作由两部分组成：一个由函数  = () 表示的编码器和一个生成重构的解码器  = ()。如果一个自编码器只是简单地学会将处处设置为 (()) = ，那么这个自编码器就没什么特别的用处。相反，我们不应该将自编码器设计成输入到输出完全相等。这通常需要向自编码器强加一些约束，使它只能近似地复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。\\n1. 欠完备自编码器\\n从自编码器获得有用特征的一种方法是限制 的维度比  小，这种编码维度小于输入维度的自编码器称为欠完备（）自编码器。学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。\\n学习过程可以简单地描述为最小化一个损失函数(,(()))，其中  是一个损失函数，惩罚(()) 与  的差异，如均方误差。当解码器是线性的且  是均方误差，欠完备的自编码器会学习出与  相同的生成子空间。这种情况下，自编码器在训练来执行复制任务的同时学到了训据的主元子空间。如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。\\n2. 正则自编码器\\n正则自编码器使用的损失函数可以鼓励模型学习其他特性（除了将输入复制到输出），而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。这些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息。\\n2.1 稀疏自编码器\\n稀疏自编码器简单地在训练时结合编码层的稀疏惩罚 Ω() 和重构误差：(,(())) + Ω()，其中 () 是解码器的输出，通常  是编码器的输出，即  = ()。稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。稀疏正则化的自编码器必须反映训练数据集的独特统计特征，而不是简单地充当恒等函数。以这种方式训练，执行附带稀疏惩罚的复制任务可以得到能学习有用特征的模型。\\n2.2 去噪自编码器\\n去噪自编码器（, ）最小化(,((˜ )))，其中 ˜  是被某种噪声损坏的  的副本。因此去噪自编码器必须撤消这些损坏，而不是简单地复制输入。\\n2.3 收缩自编码器\\n另一正则化自编码器的策略是使用一个类似稀疏自编码器中的惩罚项 Ω，\\n\\n这迫使模型学习一个在  变化小时目标也没有太大变化的函数。因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布信息的特征。这样正则化的自编码器被称为收缩自编码器（ , ）。这种方法与去噪自编码器、流形学习和概率模型存在一定理论联系。\\n3. 表示能力、层的大小和深度\\n万能近似定理保证至少有一层隐藏层且隐藏单元足够多的前馈神经网络能以任意精度近似任意函数（在很大范围里），这是非平凡深度（至少有一层隐藏层）的一个主要优点。这意味着具有单隐藏层的自编码器在数据域内能表示任意近似数据的恒等函数。但是，从输入到编码的映射是浅层的。这意味这我们不能任意添加约束，比如约束编码稀疏。深度自编码器（编码器至少包含一层额外隐藏层）在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。\\n深度可以指数地降低表示某些函数的计算成本。深度也能指数地减少学习一些函数所需的训练数据量。实验中，深度自编码器能比相应的浅层或线性自编码器产生更好的压缩效率。\\n训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。\\n4. 去噪自编码器\\n去噪自编码器（, ）是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器。\\n 的训练准则（条件高斯( | )）能让自编码器学到能估计数据分布得分的向量场 ((()) − ) ，这是  的一个重要特性。\\n\\n5. 收缩自编码器\\n收缩自编码器 (  .,2011,) 在编码  = () 的基础上添加了显式的正则项，鼓励  的导数尽可能小：\\n\\n惩罚项 Ω() 为平方 范数（元素平方之和），作用于与编码器的函数相关偏导数的  矩阵。\\n收缩（）源于  弯曲空间的方式。具体来说，由于  训练为抵抗输入扰动，鼓励将输入点邻域映射到输出点处更小的邻域。我们能认为这是将输入的邻域收缩到更小的输出邻域。\\n三、使用建立简单的自编码器\\n1. 单隐含层自编码器\\n建立一个全连接的编码器和解码器。也可以单独使用编码器和解码器，在此使用的函数式模型即可以灵活地构建自编码器。\\n50个后，看起来我们的自编码器优化的不错了，损失_: 0.1037。\\n .  , \\n .  \\n .  \\n   \\n .  \\n\\n(_, _), (_, _) = ._()\\n\\n_ = _.(\\'32\\') / 255.\\n_ = _.(\\'32\\') / 255.\\n_ = _.(((_), .(_.[1:])))\\n_ = _.(((_), .(_.[1:])))\\n(_.)\\n(_.)\\n\\n_ = 32\\n_ = (=(784,))\\n\\n = (_, =\\'\\')(_)\\n = (784, =\\'\\')()\\n\\n = (=_, =)\\n = (=_, =)\\n\\n_ = (=(_,))\\n_ = .[-1]\\n\\n = (=_, =_(_))\\n\\n.(=\\'\\', =\\'_\\')\\n\\n.(_, _, =50, _=256, \\n                =, _=(_, _))\\n\\n_ = .(_)\\n_ = .(_)\\n\\n = 10  #      \\n.(=(20, 4))\\n   ():\\n     = .(2, ,  + 1)\\n    .(_[].(28, 28))\\n    .()\\n    ._()._()\\n    ._()._()\\n\\n     = .(2, ,  + 1 + )\\n    .(_[].(28, 28))\\n    .()\\n    ._()._()\\n    ._()._()\\n.()2. 稀疏自编码器、深层自编码器\\n\\n为码字加上稀疏性约束。如果我们对隐层单元施加稀疏性约束的话，会得到更为紧凑的表达，只有一小部分神经元会被激活。在中，我们可以通过添加一个_达到对某层激活值进行约束的目的。\\n = (_, =\\'\\',_=._1(10-5))(_)\\n把多个自编码器叠起来即加深自编码器的深度，50个后，损失_:0.0926，比1个隐含层的自编码器要好一些。\\n_ = (=(784,))\\n = (128, =\\'\\')(_)\\n = (64, =\\'\\')()\\n_ = (32, =\\'\\')()\\n\\n = (64, =\\'\\')(_)\\n = (128, =\\'\\')()\\n = (784, =\\'\\')()\\n\\n = (=_, =)\\n = (=_, =_)\\n\\n.(=\\'\\', =\\'_\\')\\n\\n.(_, _, =50, _=256, \\n                =, _=(_, _))\\n\\n_ = .(_)\\n_ = .(_)3. 卷积自编码器：用卷积层构建自编码器\\n当输入是图像时，使用卷积神经网络是更好的。卷积自编码器的编码器部分由卷积层和层构成，负责空域下采样。而解码器由卷积层和上采样层构成。50个后，损失_: 0.1018。\\n_ = (=(28, 28, 1))\\n\\n = 2(16, (3, 3), =\\'\\', =\\'\\')(_)\\n = 2((2, 2), =\\'\\')()\\n = 2(8, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2), =\\'\\')()\\n = 2(8, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2), =\\'\\')()\\n\\n = 2(8, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2))()\\n = 2(8, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2))()\\n = 2(16, (3, 3), =\\'\\')()\\n = 2((2, 2))()\\n = 2(1, (3, 3), =\\'\\', =\\'\\')()\\n\\n = (=_, =)\\n.(=\\'\\', =\\'_\\')\\n\\n# 打开一个终端并启动，终端中输入  --=/\\n.(_, _, =50, _=256,\\n                =, _=(_, _),\\n                =[(_=\\'\\')])\\n\\n_ = .(_)4. 使用自动编码器进行图像去噪\\n我们把训练样本用噪声污染，然后使解码器解码出干净的照片，以获得去噪自动编码器。首先我们把原图片加入高斯噪声，然后把像素值到0~1。\\n .  , 2, 2, 2\\n .  \\n .  \\n   \\n .  \\n .  \\n\\n(_, _), (_, _) = ._()\\n_ = _.(\\'32\\') / 255.\\n_ = _.(\\'32\\') / 255.\\n_ = .(_, ((_), 28, 28, 1))\\n_ = .(_, ((_), 28, 28, 1))\\n_ = 0.5\\n__ = _ + _ * ..(=0.0, =1.0, =_.) \\n__ = _ + _ * ..(=0.0, =1.0, =_.) \\n__ = .(__, 0., 1.)\\n__ = .(__, 0., 1.)\\n(_.)\\n(_.)\\n\\n_ = (=(28, 28, 1))\\n\\n = 2(32, (3, 3), =\\'\\', =\\'\\')(_)\\n = 2((2, 2), =\\'\\')()\\n = 2(32, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2), =\\'\\')()\\n\\n = 2(32, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2))()\\n = 2(32, (3, 3), =\\'\\', =\\'\\')()\\n = 2((2, 2))()\\n = 2(1, (3, 3), =\\'\\', =\\'\\')()\\n\\n = (=_, =)\\n.(=\\'\\', =\\'_\\')\\n\\n# 打开一个终端并启动，终端中输入  --=/\\n.(__, _, =10, _=256,\\n                =, _=(__, _),\\n                =[(_=\\'\\', _=)])\\n\\n_ = .(__)\\n\\n = 10\\n.(=(30, 6))\\n   ():\\n     = .(3, ,  + 1)\\n    .(_[].(28, 28))\\n    .()\\n    ._()._()\\n    ._()._()\\n    \\n     = .(3, ,  + 1 + )\\n    .(__[].(28, 28))\\n    .()\\n    ._()._()\\n    ._()._()\\n\\n     = .(3, ,  + 1 + 2*)\\n    .(_[].(28, 28))\\n    .()\\n    ._()._()\\n    ._()._()\\n.() \\n\\n\\n阅读更多\\n登录后自动展开\\n\\n\\n\\n\\n    $(\".\").();\\n\\n\\n\\n\\n.(\"525334\"). = \"://..///525334.?=\" + .();\\n\\n\\n\\t((){\\n\\t\\t  = $(\"#-\");\\n\\t\\t(.>0){\\n\\t\\t\\t  = $().();\\n\\t\\t\\t  = $(\"._\");\\n\\t\\t\\t  = .();\\n\\t\\t\\t( > *2){\\n\\t\\t\\t\\t.({\\n\\t\\t\\t\\t\\t\\'\\':*2+\\'\\',\\n\\t\\t\\t\\t\\t\\'\\':\\'\\'\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\t.((){\\n\\t\\t\\t\\t\\t.(\"\");\\n\\t\\t\\t\\t\\t$().().();\\n\\t\\t\\t\\t})\\n\\t\\t\\t}{\\n\\t\\t\\t\\t.().();\\n\\t\\t\\t}\\n\\t\\t}\\n\\t})()\\n\\n\\n\\n                  = $(\".--\").() - 40;\\n                _({\\n                    : ,\\n                     : 130,\\n                     : \"0231\",\\n                    : \"0\",\\n                     : \"\",\\n                     : {\\n                     : \"\",\\n                     : 0,\\n                    : 231,\\n                    : 130,\\n                     : \"\",\\n                     : ,\\n                     : ,\\n                    : 20,\\n                    : \"#333\",\\n                     : \" \",\\n                    : \"\",\\n                     : 0,\\n                     : 0,\\n                     : 10,\\n                     : 16,\\n                     : ,\\n                    : 14,\\n                    : \"#666\",\\n                     : \" \",\\n                     : 0,\\n                     : 0,\\n                     : 0,\\n                     : 0,\\n                    : \"#\",\\n                    : \"#016\"\\n                    }\\n                })\\n             \\n\\n\\n\\t\\t想对作者说点什么？\\n\\t\\t\\n我来说一句\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t自动编码器学习总结\\t\\t\\t\\t\\n\\n\\n\\n\\n_1\\n\\n\\n\\n\\n07-23\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t2476\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t自编码器学习总结，整理的原理，在分类任务上的使用，稀疏，以及对改进得到文档或句子向量表达的一篇（: -...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t 模型之：自编码器 \\n\\n\\n\\n\\n010555688\\n\\n\\n\\n\\n04-24\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t1.8万\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t9.1、自动编码器\\r\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  最简单的一种方法是利用人工神经网络的特点，人工神经网络（）本身就是具有层次结构的系统，如果给定一个神经网络，...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n.4__=\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t自动编码器原理及实现(一)\\t\\t\\t\\t\\n\\n\\n\\n\\n1990\\n\\n\\n\\n\\n02-07\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t5745\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t一、什么是自动编码器（也叫自编码器）\\r\\n网络上有关自动编码器的资料参差不齐，许多基本概念及原理没有讲清楚。现在，我来一个比较系统的讲述\\r\\n一下自动编码器的一些概念及原理，在最后，附上实现代...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t对抗自编码器指南之一：自编码器 \\n\\n\\n\\n\\n3333\\n\\n\\n\\n\\n12-09\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t2015\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t自编码器是一种特殊的神经网络（` `），它的输出目标（``）就是输入（所以它基本上就是试图将输出重构为输入），由于它不需要任何人工标注，所以可以采用无监督的方式进...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t自动编码器（）\\t\\t\\t\\t\\n\\n\\n\\n\\n010089444\\n\\n\\n\\n\\n09-20\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t2.3万\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t是一种无监督的学习算法。在深度学习中，用于在训练阶段开始前，确定权重矩阵的初始值。神经网络中的权重矩阵可看作是对输入的数据进行特征转换，即先将数据编...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t人工智能 - 自编码器（）\\t\\t\\t\\t\\n\\n\\n\\n\\n012515223\\n\\n\\n\\n\\n08-28\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t1275\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t自编码器，使用稀疏的高阶特征重新组合，来重构自己，输入与输出一致。的框架，参考源码，同时，复制_的模型文件。工程配置下载的依赖库：...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t  = $(\".-\").() - 48;\\n\\t\\t\\t\\t_({\\n\\t\\t\\t\\t\\t: ,\\n\\t\\t\\t\\t\\t: 60,\\n\\t\\t\\t\\t\\t: \\'\\',\\n\\t\\t\\t\\t\\t: \\'___0\\',\\n\\t\\t\\t\\t\\t: \\'\\',\\n\\t\\t\\t\\t\\t: {\\n\\t\\t\\t\\t\\t\\t: \\'\\',\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: 90,\\n\\t\\t\\t\\t\\t\\t: 60,\\n\\t\\t\\t\\t\\t\\t: \\'\\',\\n\\t\\t\\t\\t\\t\\t: ,\\n\\t\\t\\t\\t\\t\\t: ,\\n\\t\\t\\t\\t\\t\\t: 20,\\n\\t\\t\\t\\t\\t\\t: \\'#333\\',\\n\\t\\t\\t\\t\\t\\t: \\' \\',\\n\\t\\t\\t\\t\\t\\t: \\'\\',\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: 10,\\n\\t\\t\\t\\t\\t\\t: 8,\\n\\t\\t\\t\\t\\t\\t: ,\\n\\t\\t\\t\\t\\t\\t: 14,\\n\\t\\t\\t\\t\\t\\t: 14,\\n\\t\\t\\t\\t\\t\\t: \\'#666\\',\\n\\t\\t\\t\\t\\t\\t: \\' \\',\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: 0,\\n\\t\\t\\t\\t\\t\\t: \\'#\\',\\n\\t\\t\\t\\t\\t\\t: \\'#016\\'\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t})\\n\\t\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t  自编码器（五）\\t\\t\\t\\t\\n\\n\\n\\n\\n_19707521\\n\\n\\n\\n\\n12-07\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t2899\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t一、什么是自编码器（）\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n“自编码”是一种数据压缩算法，其中压缩和解压缩功能是1）数据特定的，2）有损的，3）从例子中自动学习而不是由人工设计。此外，在几...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t基于无监督学习的自编码器实现\\t\\t\\t\\t\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n12-11\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t2549\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t无监督学习介绍及部分函数实现\\n内容简介目前许多有监督学习算法，如，或是，决策树等，都在工业界分类或决策任务上取得了不错的成果。但是这些有监督学习需要大量带标签的数据，对数据...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n自编码器与堆叠自编码器简述\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n05-30\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t1万\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t作者：科研君\\r\\n链接：://..//41490383//103006793\\r\\n来源：知乎\\r\\n著作权归作者所有。商业转载请联系作者获得授权，非...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t上实现自编码器 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n03-31\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t9187\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t一、自编码器简介\\r\\n无监督特征学习（  ）是一种仿人脑的对特征逐层抽象提取的过程，学习过程中有两点：一是无监督学习，即对训练数据不需要进行标签化标...\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n文章热词\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习图片分类方法\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习 目标标注\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习算法类别\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t多智能体深度学习\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习分布式模型\\t\\t\\t\\t\\t\\n\\n\\n\\n相关热词\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习深度学习\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习和\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习（\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t深度学习\\n\\n\\n\\n\\n\\n\\n\\n\\n没有更多推荐了，返回首页\\n\\n\\n\\n\\n\\n个人资料\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n关注\\n\\n\\n\\n\\n\\n原创\\n59\\n\\n\\n粉丝\\n138\\n\\n\\n喜欢\\n53\\n\\n\\n评论\\n45\\n\\n\\n\\n\\n等级：\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n访问：\\n\\n                32万+            \\n\\n\\n积分：\\n\\n                3137            \\n\\n\\n排名：\\n1万+\\n\\n\\n\\n勋章：\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n持之以恒\\n\\n\\n                        授予每个自然月内发布4篇或4篇以上原创或翻译博文的用户。不积跬步无以至千里，不积小流无以成江海，程序人生的精彩需要坚持不懈地积累！\\n                    \\n\\n\\n\\n\\n \\n\\n \\n\\n\\n\\n访问统计\\n\\n\\n\\n\\n\\n最新文章\\n\\n\\n\\n：一个推荐系统算法库\\n\\n\\n卷积神经网络补充\\n\\n\\n论文笔记：  :   \\n\\n\\n关联容器总结\\n\\n\\n序列容器总结\\n\\n\\n\\n\\n\\n归档\\n\\n\\n\\n\\n\\n                    2018年9月                    3篇\\n\\n\\n\\n\\n\\n                    2018年8月                    9篇\\n\\n\\n\\n\\n\\n                    2017年12月                    20篇\\n\\n\\n\\n\\n\\n                    2017年11月                    30篇\\n\\n\\n\\n\\n\\n                    2017年10月                    2篇\\n\\n\\n\\n\\n\\n                    2017年9月                    2篇\\n\\n\\n\\n\\n\\n                    2017年6月                    10篇\\n\\n\\n\\n\\n\\n                    2017年5月                    7篇\\n\\n\\n\\n\\n\\n                    2017年4月                    7篇\\n\\n\\n\\n\\n\\n                    2017年3月                    19篇\\n\\n\\n\\n\\n\\n                    2017年2月                    1篇\\n\\n\\n\\n\\n\\n展开\\n\\n\\n\\n个人分类\\n\\n\\n\\n\\n                    - 中文文档                    50篇\\n\\n\\n\\n\\n                    机器学习/深度学习                    22篇\\n\\n\\n\\n\\n                    ++                    10篇\\n\\n\\n\\n\\n                                        11篇\\n\\n\\n\\n\\n                                        6篇\\n\\n\\n\\n\\n                    目标跟踪                    1篇\\n\\n\\n\\n\\n                                        5篇\\n\\n\\n\\n\\n                                        4篇\\n\\n\\n\\n\\n                                        2篇\\n\\n\\n\\n\\n\\n展开\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t$(\".-\").((){\\n\\t\\t$().(\\'.-\\').(\\'-\\');\\n\\t\\t$().();\\n\\t})\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t(() {\\n\\t\\t\\t\\t  = \"_\" + .().(36).(2);\\n\\t\\t\\t\\t.(\\'< =\"_0001\"></>\\');\\n\\t\\t\\t\\t(.=. || []).({\\n\\t\\t\\t\\t\\t: \\'5851901\\',\\n\\t\\t\\t\\t\\t: \\'_0001\\',\\n\\t\\t\\t\\t\\t: \\'520,40\\',\\n\\t\\t\\t\\t\\t: \\'-\\'\\n\\t\\t\\t\\t});\\n\\t\\t\\t})();\\n\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t#_0001{\\n\\t\\t\\t\\t\\t:  !;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\n\\n\\n\\n登录\\n\\n\\n注册\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n点赞\\n取消点赞\\n\\n13\\n\\n\\n\\n\\n\\n\\n\\n评论\\n\\n\\t\\t\\t\\t\\t\\t8\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n目录\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n收藏\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t手机看\\n\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n上一篇\\n\\n\\n\\n\\n\\n\\n\\n下一篇\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t#_360_ > *{\\n\\t\\t\\t\\t\\t -: -8;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\n\\n _.({  : \\'0\\', : 60, : 60, : \\'\\', : \\'_360_\\' }); \\n\\n\\n\\n\\n\\n.___ = { \"\": { \"\": {}, \"\": \"\", \"\": \"1\", \"\": , \"\": \"\", \"\": \"0\", \"\": \"16\" }, \"\": {} };  () 0[((\\'\\')[0] || ).((\\'\\')). = \\'://.////.?=89860594\\'];\\n\\n      = 10;\\n     =  > 1 ? ( + (>6 ? 2 : 1)) : ;\\n      = \"深度学习之自编码器\";\\n      = 7;\\n      = \"73480859\";\\n      = 8;\\n      = ;\\n      = \"://..////73480859\";\\n      = \"://../\";\\n    //1禁止评论，2正常\\n      = 2;\\n    //百度搜索\\n      = \"%6%7%1%5%%6%5%%6%4%9%0+%8%87%%7%%96%7%0%81\";\\n      = ;\\n    // 代码段样式\\n      = \\'--\\';\\n      = [\"6156566460\",\"81716\",\"78015668\",\"\"];//高亮数组\\n\\n\\n\\n\\n\\n    ({\\n        : 8,\\n        : ,\\n        : () {\\n            (,\"深度学习之自编码器\");\\n        }\\n    })\\n\\n\\n\\n\\n\\n\\n\\n\\n        关闭\\n    \\n\\n\\n\\t\\t\\t(() {\\n\\t\\t\\t\\t(.=. || []).({\\n\\t\\t\\t\\t\\t: \\'5868728\\',\\n\\t\\t\\t\\t\\t: \\'\\',\\n\\t\\t\\t\\t\\t: \\'250,500\\',\\n\\t\\t\\t\\t\\t: \\'-\\'\\n\\t\\t\\t\\t});\\n\\t\\t\\t})();\\n\\t\\t\\t\\n\\n\\n\\n\\n\\n        关闭\\n    \\n\\n\\n\\n\\n        关闭\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t$((){\\n\\t\\t  = /&(||||||#{1,5});/,\\n\\t\\t     = {\\n\\t\\t\\t\\t\\t: \\'<\\',\\n\\t\\t\\t\\t\\t: \\'>\\',\\n\\t\\t\\t\\t\\t: \\'&\\',\\n\\t\\t\\t\\t\\t: \\'\"\\',\\n\\t\\t\\t\\t\\t: \\'000\\',\\n\\t\\t\\t\\t\\t: \\'00\\'\\n\\t\\t\\t\\t}\\n\\t\\t  = ( ,  ) {\\n\\t\\t\\t\\t [  ];\\n\\t\\t};\\n\\t\\t = (  ) {\\n\\t\\t\\t\\t .( ,  );\\n\\t\\t}\\n\\t  .();\\n\\t  .();\\n\\t  .();\\n\\t\\t($(\\' .-\\').>0){\\n\\t\\t\\t$(\\' .-\\').((,){\\n\\t\\t\\t\\t  = .((.))\\n\\t\\t\\t\\t. = .;\\n\\t\\t\\t\\t. = \\'-\\'+.;\\n\\t\\t\\t});\\n\\t\\t}\\n\\t})\\n\\t\\n\\n\\n    ..({\\n            \"-\": {\\n                    : { : , : \"94%\" },\\n                    : \\n            },\\n            2: {\\n                : \"\"\\n            },\\n            2: {\\n                : \\'\\'\\n            }\\n    });\\n\\n'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/zoe/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<html><head><title>The Dormouse's story</title></head>\n",
       "<body>\n",
       "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
       "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;\n",
       "and they lived at the bottom of a well.</p>\n",
       "<p class=\"story\">...</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\\n\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n...\\n\""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
