{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络DNN模型与前向传播算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 从感知机到神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机的模型，它是一个有若干输入和一个输出的模型，如下图:\n",
    "<img src='./images/percentron1.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出和输入之间学习到一个**线性关系**，得到中间输出结果：\n",
    "\n",
    "$$z = \\sum_{i=1}^{m} w_i x_i +b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着是一个神经元激活函数:\n",
    "$$sign(z) = \\begin{cases}\n",
    "-1 , z<0\\\\ 1, z \\ge -1\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从而得到我们想要的输出结果1或者-1。\n",
    "\n",
    "这个模型只能用于**二元分类**，且无法学习比较复杂的**非线性模型**，因此在工业界无法使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而神经网络则在感知机的模型上做了扩展，总结下主要有三点：   \n",
    "    1. 加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。\n",
    "\n",
    "<img src='./images/mlp1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. 输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域，比如：降维和聚类等。多个神经元输出的输出层对应的一个实例。如下图，输出层现在有4个神经元了。\n",
    "    \n",
    "<img src='./images/mlp2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. 对激活函数做扩展，感知机的激活函数是sign(z)，虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：\n",
    "    \n",
    "   $$f(z) = \\frac{1}{1+e^{-z}}$$\n",
    "   \n",
    "    还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DNN的基本结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节我们了解了神经网络基于感知机的扩展，主要在三方面：\n",
    "- 添加隐藏层\n",
    "- 单一输出变成多输出\n",
    "- 激活函数：由简单的Sign(z)符号函数，添加多种变化的激活函数，如：Sigmoid, tanh, ReLU, softmax等\n",
    "\n",
    "\n",
    "而DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准。   \n",
    "\n",
    "多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）, 名字实在是多。\n",
    "\n",
    "本文后面我们讲到的神经网络都默认为DNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类：\n",
    "- 输入层\n",
    "- 隐藏层\n",
    "- 输出层\n",
    "\n",
    "如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。\n",
    "\n",
    "<img src='./images/dnn1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。\n",
    "\n",
    "虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系$z=∑w_ix_i+b$加上一个激活函数σ(z)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**由于DNN层数多，则我们的线性关系系数w和偏倚b的数量也就是很多了。具体的参数在DNN是如何定义的呢？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们来看看线性关系系数$w$的定义。\n",
    "\n",
    "以下图一个三层的DNN为例，**第二层的第4个神经元**到**第三层的第2个神经元**的线性系数定义为$w^3_{24}$。\n",
    "- 上标3代表线性系数w所在的层数；\n",
    "- 而下标对应的是输出的第三层索引2和输入的第二层索引4。\n",
    "\n",
    "你也许会问，**为什么不是$w^3_{42}$, 而是$w^3_{24}$呢？**\n",
    "\n",
    "这主要是为了便于模型用于矩阵表示运算，如果是$w^3_{42}$，而每次进行矩阵运算是$w^Tx+b$，需要进行转置。  \n",
    "将输出的索引放在前面的话，则线性运算不用转置，即直接为$wx+b$。\n",
    "\n",
    "总结下，第$l−1$层的第$k$个神经元到第$l$层的第$j$个神经元的线性系数定义为$w^l_{jk}$。  \n",
    "注意，输入层是没有w参数的。\n",
    "\n",
    "<img src='./images/dnn2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看看偏倚b的定义。\n",
    "\n",
    "还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b^2_3$。\n",
    "其中：\n",
    "- 上标2代表所在的层数\n",
    "- 下标3代表偏倚所在的神经元的索引。\n",
    "\n",
    "同样的道理，第三个的第一个神经元的偏倚应该表示为$b^3_1$。  \n",
    "同样的，输入层是没有偏倚参数b的。\n",
    "\n",
    "<img src='./images/dnn3.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DNN前向传播算法数学原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一节，我们已经介绍了DNN各层线性关系系数w,偏倚b的定义。\n",
    "\n",
    "假设我们选择的激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN，利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。\n",
    "\n",
    "<img src='./images/dnnfp.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第二层的输出$a^2_1$,$a^2_2$,$a^2_3$，我们有：\n",
    "$$a^2_1 = \\sigma(z_1^2) = \\sigma(w^2_{11}x_1 + w^2_{12}x_2 + w^2_{13}x_3 + b_1^2  )$$\n",
    "$$a^2_2 = \\sigma(z_2^2) = \\sigma(w^2_{21}x_1 + w^2_{22}x_2 + w^2_{23}x_3 + b_2^2  )$$\n",
    "$$a^2_3 = \\sigma(z_3^2) = \\sigma(w^2_{31}x_1 + w^2_{32}x_2 + w^2_{33}x_3 + b_3^2  )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第三层的输出$a_1^3$，我们有：\n",
    "$$a_1^3 = \\sigma(z_1^3) = \\sigma(w_{11}^3a^2_1 + w^3_{12}a^2_2+w^{3}_{13}a^2_3 + b^3_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面的例子一般化，假设第$l-1$层共有m个神经元，则对第$l$层的第$j$个神经元的输出$a^l_j$，我们有：\n",
    "$$a^l_j = \\sigma(z^l_j) = \\sigma(\\sum_{k=1}^m w_{jk}^l a_k^{l-1} + b^l_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，如果$l=2$，则对于$a^1_k$即为输入层的$x_k$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看出，使用代数法一个个的表示输出比较复杂，而如果使用**矩阵法**则比较的简洁。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设第$l-1$层共有m个神经元，而第$l$层有n个神经元，而第$l$层的线性系数$w$组成了一个$n*m$的矩阵$W^l$，第$l$层的偏倚$b$组成了一个$n*1$的向量$b^l$，第$l-1$层的输出$a$组成了一个$m*1$的向量$a^{l-1}$，第$l$层的未激活前线性输出$z$组成了一个$n*1$的向量$z_l$，第$l$层的输出$a$组成了一个$n*1$的向量$a^l$。 \n",
    "\n",
    "则用矩阵表示，第$l$层的输出为：\n",
    "$$a^l = \\sigma(z^l) = \\sigma(W^l a^{l-1} + b^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DNN前向传播算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DNN前向传播算法小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
