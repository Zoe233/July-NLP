{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度神经网络DNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#dnnmodel'>一、DNN模型与前向传播算法</a>\n",
    "- <a href='#1'>1. 从感知机到神经网络</a>\n",
    "- <a href='#2'>2. DNN的基本结构</a>\n",
    "- <a href='#3'>3. DNN前向传播算法的数学原理</a>\n",
    "- <a href='#4'>4. DNN前向传播算法</a>\n",
    "- <a href='#5'>5. DNN前向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#bp'>二、DNN模型与反向传播算法BP</a>\n",
    "- <a href='#bp1'>1. DNN反向传播算法要解决的问题</a>\n",
    "- <a href='#bp2'>2. DNN反向传播算法的基本思路</a>\n",
    "- <a href='#bp3'>3. DNN反向传播算法过程</a>\n",
    "- <a href='#bp4'>4. DNN反向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#cost_activation'>三、DNN模型与损失函数和激活函数的选择</a>\n",
    "- <a href='#cost_activation1'>1. 均方差损失函数+Sigmoid激活函数的问题</a>\n",
    "- <a href='#cost_activation2'>2. 使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度</a>\n",
    "- <a href='#cost_activation3'>3. 使用对数似然损失函数和softmax激活函数进行DNN分类输出</a>\n",
    "- <a href='#cost_activation4'>4. 梯度爆炸、梯度消失与ReLU激活函数</a>\n",
    "- <a href='#cost_activation5'>5. DNN其他激活函数</a>\n",
    "- <a href='#cost_activation6'>6. DNN损失函数和激活函数小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='dnnmodel'>一、DNN模型与前向传播算法</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络（Deep Neural Networks， 以下简称DNN）是深度学习的基础，而要理解DNN，首先我们要理解DNN模型，下面我们就对DNN的模型与前向传播算法做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='1'>1. 从感知机到神经网络</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机的模型，它是一个有若干输入和一个输出的模型，如下图:\n",
    "<img src='./images/percentron1.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出和输入之间学习到一个**线性关系**，得到中间输出结果：\n",
    "\n",
    "$$z = \\sum_{i=1}^{m} w_i x_i +b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着是一个神经元激活函数:\n",
    "$$sign(z) = \\begin{cases}\n",
    "-1 , z<0\\\\ 1, z \\ge -1\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从而得到我们想要的输出结果1或者-1。\n",
    "\n",
    "这个模型只能用于**二元分类**，且无法学习比较复杂的**非线性模型**，因此在工业界无法使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而神经网络则在感知机的模型上做了扩展，总结下主要有三点：   \n",
    "    1. 加入了隐藏层，隐藏层可以有多层，增强模型的表达能力，如下图实例，当然增加了这么多隐藏层模型的复杂度也增加了好多。\n",
    "\n",
    "<img src='./images/mlp1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. 输出层的神经元也可以不止一个输出，可以有多个输出，这样模型可以灵活的应用于分类回归，以及其他的机器学习领域，比如：降维和聚类等。多个神经元输出的输出层对应的一个实例。如下图，输出层现在有4个神经元了。\n",
    "    \n",
    "<img src='./images/mlp2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. 对激活函数做扩展，感知机的激活函数是sign(z)，虽然简单但是处理能力有限，因此神经网络中一般使用的其他的激活函数，比如我们在逻辑回归里面使用过的Sigmoid函数，即：\n",
    "    \n",
    "   $$f(z) = \\frac{1}{1+e^{-z}}$$\n",
    "   \n",
    "    还有后来出现的tanx, softmax,和ReLU等。通过使用不同的激活函数，神经网络的表达能力进一步增强。对于各种常用的激活函数，我们在后面再专门讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='2'>2. DNN的基本结构</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节我们了解了神经网络基于感知机的扩展，主要在三方面：\n",
    "- 添加隐藏层\n",
    "- 单一输出变成多输出\n",
    "- 激活函数：由简单的Sign(z)符号函数，添加多种变化的激活函数，如：Sigmoid, tanh, ReLU, softmax等\n",
    "\n",
    "\n",
    "而DNN可以理解为有很多隐藏层的神经网络。这个很多其实也没有什么度量标准。   \n",
    "\n",
    "多层神经网络和深度神经网络DNN其实也是指的一个东西，当然，DNN有时也叫做多层感知机（Multi-Layer perceptron,MLP）, 名字实在是多。\n",
    "\n",
    "本文后面我们讲到的神经网络都默认为DNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类：\n",
    "- 输入层\n",
    "- 隐藏层\n",
    "- 输出层\n",
    "\n",
    "如下图示例，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐藏层。\n",
    "\n",
    "<img src='./images/dnn1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。\n",
    "\n",
    "虽然DNN看起来很复杂，但是从小的局部模型来说，还是和感知机一样，即一个线性关系$z=∑w_ix_i+b$加上一个激活函数σ(z)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**由于DNN层数多，则我们的线性关系系数w和偏倚b的数量也就是很多了。具体的参数在DNN是如何定义的呢？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们来看看线性关系系数$w$的定义。\n",
    "\n",
    "以下图一个三层的DNN为例，**第二层的第4个神经元**到**第三层的第2个神经元**的线性系数定义为$w^3_{24}$。\n",
    "- 上标3代表线性系数w所在的层数；\n",
    "- 而下标对应的是输出的第三层索引2和输入的第二层索引4。\n",
    "\n",
    "你也许会问，**为什么不是$w^3_{42}$, 而是$w^3_{24}$呢？**\n",
    "\n",
    "这主要是为了便于模型用于矩阵表示运算，如果是$w^3_{42}$，而每次进行矩阵运算是$w^Tx+b$，需要进行转置。  \n",
    "将输出的索引放在前面的话，则线性运算不用转置，即直接为$wx+b$。\n",
    "\n",
    "总结下，第$l−1$层的第$k$个神经元到第$l$层的第$j$个神经元的线性系数定义为$w^l_{jk}$。  \n",
    "注意，输入层是没有w参数的。\n",
    "\n",
    "<img src='./images/dnn2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再来看看偏倚b的定义。\n",
    "\n",
    "还是以这个三层的DNN为例，第二层的第三个神经元对应的偏倚定义为$b^2_3$。\n",
    "其中：\n",
    "- 上标2代表所在的层数\n",
    "- 下标3代表偏倚所在的神经元的索引。\n",
    "\n",
    "同样的道理，第三个的第一个神经元的偏倚应该表示为$b^3_1$。  \n",
    "同样的，输入层是没有偏倚参数b的。\n",
    "\n",
    "<img src='./images/dnn3.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='3'>3. DNN前向传播算法数学原理</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一节，我们已经介绍了DNN各层线性关系系数w,偏倚b的定义。\n",
    "\n",
    "假设我们选择的激活函数是σ(z)，隐藏层和输出层的输出值为a，则对于下图的三层DNN，利用和感知机一样的思路，我们可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。\n",
    "\n",
    "<img src='./images/dnnfp.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第二层的输出$a^2_1$,$a^2_2$,$a^2_3$，我们有：\n",
    "$$a^2_1 = \\sigma(z_1^2) = \\sigma(w^2_{11}x_1 + w^2_{12}x_2 + w^2_{13}x_3 + b_1^2  )$$\n",
    "$$a^2_2 = \\sigma(z_2^2) = \\sigma(w^2_{21}x_1 + w^2_{22}x_2 + w^2_{23}x_3 + b_2^2  )$$\n",
    "$$a^2_3 = \\sigma(z_3^2) = \\sigma(w^2_{31}x_1 + w^2_{32}x_2 + w^2_{33}x_3 + b_3^2  )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于第三层的输出$a_1^3$，我们有：\n",
    "$$a_1^3 = \\sigma(z_1^3) = \\sigma(w_{11}^3a^2_1 + w^3_{12}a^2_2+w^{3}_{13}a^2_3 + b^3_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面的例子一般化，假设第$l-1$层共有m个神经元，则对第$l$层的第$j$个神经元的输出$a^l_j$，我们有：\n",
    "$$a^l_j = \\sigma(z^l_j) = \\sigma(\\sum_{k=1}^m w_{jk}^l a_k^{l-1} + b^l_{j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，如果$l=2$，则对于$a^1_k$即为输入层的$x_k$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看出，使用代数法一个个的表示输出比较复杂，而如果使用**矩阵法**则比较的简洁。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设第$l-1$层共有m个神经元，而第$l$层有n个神经元，而第$l$层的线性系数$w$组成了一个$n*m$的矩阵$W^l$，第$l$层的偏倚$b$组成了一个$n*1$的向量$b^l$，第$l-1$层的输出$a$组成了一个$m*1$的向量$a^{l-1}$，第$l$层的未激活前线性输出$z$组成了一个$n*1$的向量$z_l$，第$l$层的输出$a$组成了一个$n*1$的向量$a^l$。 \n",
    "\n",
    "则用矩阵表示，第$l$层的输出为：\n",
    "$$a^l = \\sigma(z^l) = \\sigma(W^l a^{l-1} + b^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个表示方法简洁漂亮，后面我们的讨论都会基于上面的这个矩阵法表示来。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/dnnfp2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='4'>4. DNN前向传播算法</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上一节的数学推导，DNN的前向传播算法也就不难了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓的DNN的前向传播算法也就是利用我们的若干个权重系数矩阵W，偏倚向量b来和输入值向量x进行一系列**线性运算**和**激活运算**，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入: 总层数L，所有隐藏层和输出层对应的矩阵W，偏倚向量b，输入值向量x\n",
    "\n",
    "输出：输出层的输出$a^L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 初始化$a^1=x$\n",
    "2. for l=2 to L, 计算：\n",
    "$$a^l=σ(z^l)=σ(W^la^{l−1}+b^l)$$\n",
    "最后的结果即为输出$a^L$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='5'>5. DNN前向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单独看DNN前向传播算法，似乎没有什么大用处，而且这一大堆的矩阵W，偏倚向量b对应的参数怎么获得呢？\n",
    "\n",
    "**怎么得到最优的矩阵W，偏倚向量b呢？**\n",
    "\n",
    "这个我们在讲DNN的**反向传播算法**时再讲。\n",
    "\n",
    "而理解反向传播算法的前提就是理解DNN的模型与前向传播算法。这也是我们这一篇先讲的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='bp'>二、DNN模型与反向传播算法BP</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度神经网络（DNN）模型与前向传播算法中，我们对DNN的模型和前向传播算法做了总结，这里我们更进一步，对DNN的反向传播算法（Back Propagation，BP）做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp1'>1. DNN反向传播算法要解决的问题</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在了解DNN的反向传播算法前，我们**先要知道DNN反向传播算法要解决的问题，也就是说，什么时候我们需要这个反向传播算法？**　\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回到我们监督学习的一般问题，假设我们有m个训练样本：$\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$,其中$x$为输入向量，特征维度为$n_{in}$，而$y$为输出向量，特征维度为$n_{out}$。我们需要利用这m个样本训练出一个模型，当有一个新的测试样本$(x_{test},?)$来到时, 我们可以预测$y_{test}$向量的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们采用DNN的模型，即我们使输入层有$n_in$个神经元，而输出层有$n_out$个神经元。\n",
    "\n",
    "再加上一些含有若干神经元的隐藏层。\n",
    "\n",
    "此时我们**需要找到合适的所有隐藏层和输出层对应的线性系数矩阵W，偏倚向量b，让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。**\n",
    "\n",
    "**怎么找到合适的参数呢？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果大家对传统的机器学习的算法优化过程熟悉的话，这里就很容易联想到我们可以用一个**合适的损失函数**来**度量训练样本的输出损失**，接着对这个损失函数进行**优化求最小化的极值**，对应的一系列线性系数矩阵W，偏倚向量b即为我们的最终结果。\n",
    "\n",
    "在DNN中，损失函数优化极值求解的过程最常见的一般是通过**梯度下降法**来一步步迭代完成的，当然也可以是其他的迭代方法。比如：**牛顿法**与**拟牛顿法**。\n",
    "\n",
    "**对DNN的损失函数用梯度下降法进行迭代优化求极小值的过程**即为我们的**反向传播算法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp2'>2. DNN反向传播算法的基本思路</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行DNN反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。\n",
    "\n",
    "你也许会问：   \n",
    "**训练样本计算出的输出是怎么得来的？**  \n",
    "这个输出是随机选择一系列W,b,用我们上一节的前向传播算法计算出来的。即通过一系列的计算：$a^l=σ(z^l)=σ(W^la^{l−1}+b^l)$。计算到输出层第L层对应的$a^L$即为前向传播算法计算出来的输出。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回到损失函数，DNN可选择的损失函数有不少，为了专注算法，这里我们使用最常见的**均方差**来度量损失。\n",
    "\n",
    "即对于每个样本，我们期望最小化下式：\n",
    "$$J(W,b,x,y)=\\frac{1}{2}||a^L-y||^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$a^L$和y为特征维度为$n_{out}$的向量，而$||S||_2$为S的L2范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数有了，现在我们开始用梯度下降法迭代求解每一层的W,b。\n",
    "\n",
    "首先是输出层第$L$层。注意到输出层的W,b满足下式：\n",
    "$$a^L = \\sigma(z^L) = \\sigma(w^La^{L-1} +b^L)$$\n",
    "\n",
    "这样对于输出层的参数，我们的损失函数变为：\n",
    "$$J(W,b,x,y)=\\frac{1}{2}||a^L-y||^2_2 = \\frac{1}{2}||\\sigma(w^La^{L-1} + b^L) -y||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，求解W和b的梯度就简单了：\n",
    "$$\\frac{\\partial J(W,b,x,y)}{\\partial W^L} = \\frac{\\partial J(W,b,x,y)}{\\partial z^L} \\frac{\\partial z^L}{\\partial W^L} = (a^L-y) \\odot \\sigma' (z^L)(a^{L-1})^T$$\n",
    "\n",
    "$$\\frac{\\partial J(W,b,x,y)}{\\partial b^L} = \\frac{\\partial J(W,b,x,y)}{\\partial z^L} \\frac{\\partial z^L}{\\partial b^L} = (a^L-y) \\odot \\sigma' (z^L)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，上式中有一个符号$\\odot$，它代表Hadamard积，对于两个维度相同的向量$A(a_1,a_2,...,a_n)^T$和$B(b_1,b_2,...,b_n)^T$，则$A\\odot B = (a_1 b_1, a_2b_2,...,a_nb_n)^T$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算推导过程见网页：https://www.cnblogs.com/pinard/p/6422831.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归过程，有点麻烦，后面补"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp3'>3. DNN反向传播算法过程</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们总结下DNN反向传播算法的过程。\n",
    "\n",
    "由于梯度下降法有\n",
    "- 批量（Batch），\n",
    "- 小批量(mini-Batch)，\n",
    "- 随机  \n",
    "三个变种，为了简化描述，这里我们以最基本的批量梯度下降法为例来描述反向传播算法。\n",
    "\n",
    "实际上在业界使用最多的是**mini-Batch的梯度下降法**。不过区别仅仅在于迭代时训练样本的选择而已。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入: 总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长α，最大迭代次数MAX与停止迭代阈值ϵ，输入的m个训练样本$\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 初始化各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b的值为一个随机值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. for iter to 1 to MAX：  \n",
    "\n",
    "2-1) for i =1 to m：   \n",
    "    a) 将DNN输入$a^1$设置为$x_i$；  \n",
    "    b) for l=2 to L，进行前向传播算法计算$a^{i,l}=σ(z……{i,l})=σ(W^la^{i,l−1}+b^l)$   \n",
    "    c) 通过损失函数计算输出层的$δ^{i,L}$    \n",
    "    d) for l= L to 2, 进行反向传播算法计算$δ^{i,l}=(W^{l+1})^Tδ^{i,l+1}⊙σ′(z^{i,l})$   \n",
    "            \n",
    "2-2) for l = 2 to L，更新第l层的$W^l,b^l$:\n",
    "    $$W^l=W^l−α∑_{i=1}^mδ^{i,l}(a^{i,l−1})^T$$\n",
    "    $$b^l=b^l−α∑_{i=1}^mδ^{i,l}$$\n",
    "            如果所有W，b的变化值都小于停止迭代阈值ϵ，则跳出迭代循环到步骤3。\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp4'>4. DNN反向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了DNN反向传播算法，我们就可以很方便的用DNN的模型去解决第一节里面提到了各种监督学习的分类回归问题。\n",
    "\n",
    "当然DNN的参数众多，矩阵运算量也很大，直接使用会有各种各样的问题。\n",
    "\n",
    "有哪些问题以及如何尝试解决这些问题并优化DNN模型与算法，我们在下一篇讲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#cost_activation'>三、DNN模型与损失函数和激活函数的选择</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cost_activation1'>1. 均方差损失函数+Sigmoid激活函数的问题</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cost_activation2'>2. 使用交叉熵损失函数+Sigmoid激活函数改进DNN算法收敛速度</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cost_activation3'>3. 使用对数似然损失函数和softmax激活函数进行DNN分类输出</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cost_activation4'>4. 梯度爆炸、梯度消失与ReLU激活函数</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cost_activation5'>5. DNN其他激活函数</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#cost_activation6'>6. DNN损失函数和激活函数小结</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
