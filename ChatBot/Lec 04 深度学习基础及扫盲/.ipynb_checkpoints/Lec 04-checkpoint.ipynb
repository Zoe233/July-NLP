{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 04 深度学习基础及扫盲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要内容：\n",
    "- <a href='#rnn'>1. 循环神经网络RNN</a>\n",
    "    - <a href='#rnn1'>1.1 场景与多种引用</a>\n",
    "    - <a href='#dnn_rnn'>1.2 神经网络到循环神经网络RNN</a>\n",
    "    - <a href='#rnn2'>1.3 NLP文字序列最爱的RNN</a>\n",
    "        - 双向RNN\n",
    "        - 深度RNN\n",
    "    - <a href='#rnn3'>1.4 BPTT算法</a>\n",
    "    - <a href='#pic'>1.5 看图说话</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#lstm'>2. LSTM</a>\n",
    "    - <a href='#lstm1'>2.1 长时依赖问题</a>\n",
    "    - <a href='#lstm2'>2.2 “记忆细胞”与状态</a>\n",
    "    - <a href='#lstm3'>2.3 LSTM的变体</a>\n",
    "    - <a href='#gru'>2.4 GRU</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#application'>3.NLP的应用</a>\n",
    "    - <a href='#application1'>3.1 各式各样的生成模型</a>\n",
    "    - <a href='#application2'>3.2 看图说话基础版与高级版</a>\n",
    "    - <a href='#application3'>3.3 序列到序列学习（机器翻译等）</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='rnn'>1. 循环神经网络RNN</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='rnn1'>1.1 场景与多种引用</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理中，另外一个重要的应用领域，就是**文本的自动撰写**。\n",
    "\n",
    "关键词、关键短语、自动摘要提取都属于这个领域中的一种应用。\n",
    "不过这些应用，都是**由多到少的生成**。\n",
    "\n",
    "这里我们介绍其另外一种应用：**由少到多的生成**，包括句子的复写，由关键词、主题生成文章或者段落等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 由多到少的生成：\n",
    "    - 关键词\n",
    "    - 关键短语\n",
    "    - 自动摘要\n",
    "\n",
    "- 又少到多的生成：\n",
    "    - 句子的复写\n",
    "    - 由关键词、主题生成文章或者段落"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如MarkDown，Latex等这种有模式pattern的语言，都可以通过RNN（循环神经网络）的生成模型，学习到如何生成相应的符合其pattern的产物，如：Markdown格式的笔记，Latex格式的数学公式等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍循环神经网络的应用：\n",
    "- 模仿论文（连公式的格式都是正确的）\n",
    "- 模仿Linux内核代码“写程序”\n",
    "- 模仿小四的作品\n",
    "- 机器翻译（Seq2Seq）\n",
    "- 看图说话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn1.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这篇生成的论文，在样式上是符合人类的论文的书写习惯，可能内容上没有什么实际的研究意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn2.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的语言在做编码的时候，语法是不一样的，如java和c需要标记变量的类型，而python则是东岱语言，不需要预先定义变量的类型，而是根据赋值的类型而变化。\n",
    "\n",
    "学习到编码方式和风格，RNN将序列的模式学会，前后组合在一起，有具体的实际意义。然后RNN可以通过这样的序列，生成相应的序列。如：论文和代码都不一定有实际意义，但是至少都是符合相关的语法和模式的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn3.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果序列对应的是中文，那么生成的就是中文。\n",
    "\n",
    "模型的参数如何；  \n",
    "给定的数据如何（丰富度，准确性等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn4.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列到序列，Seq2Seq的应用。\n",
    "\n",
    "对于计算机而言，喂给的是向量，生成的向量，只是向量相应的映射的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn5.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个任务，在后续会讲解。 \n",
    "\n",
    "中间的一副图：问和答，是更高级的一种。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='dnn_rnn'>1.2 神经网络到循环神经网络RNN</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nn1.png' width='70%'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络中，我们有很多种的结构和名称。\n",
    "\n",
    "如：\n",
    "- MLP/DNN： 一般的神经网络结构比较简单，input layer, n * hidden layer , output layer\n",
    "- CNN卷积神经网络，主要用于处理图像，也可以应用于自然语言处理等多方面\n",
    "- RNN( LSTM, GRU)： 自然语言处理很多的处理序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 循环神经网络RNN\n",
    "\n",
    "为什么有BP神经网络，CNN，还要RNN?\n",
    "- 传统神经网络（包括CNN），输入和输出都是相互独立的。\n",
    "    - 图像上的猫和狗是分隔开的，但有些任务，后续的输出和之前的内容是相关的。\n",
    "    - 如：“我是中国人，我的母语是__”\n",
    "- RNN引入“记忆”的概念\n",
    "    - 循环2字 来源于其每个元素都执行相同的任务。\n",
    "    - 但是输出依赖于 “输入”和“记忆”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统的MLP/DNN，都是输入和输出相互独立的。\n",
    "\n",
    "但在，文本的序列问题，前后是有关联的。 这种依赖于上下文，在RNN中引入了“记忆”的概念。\n",
    "\n",
    "如上面的应用中的看图提问并回答，那么提问的就要存在“记忆”中，然后基于问题去回答。\n",
    "\n",
    "“循环”相当于一个“回路”，就是一遍一遍地去做。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn6.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_t$是时间t处的输入\n",
    "- $S_t$是时间t处的“记忆”，$S_t = f(UX_t +WS_{t-1})$，f可以是tanh等\n",
    "- $O_t$是时间t处的输出，比如：是预测下个词的话，可能是softmax输出的属于每个候选词的概率，$O_t = softmax(VS_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn_e1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**循环神经网络之 结构细节 **\n",
    "- 可以把隐状态$S_t$视作“记忆体”，捕捉了之前时间点上的信息。\n",
    "- 输出$O_t$由当前时间及之前所有的“记忆“共同计算得到。\n",
    "- 很可惜，实际应用中，$S_t$并不能捕捉和保留之前所有信息（记忆有限？）\n",
    "    - LSTM可以解决这个问题，长短时记忆网络\n",
    "- 不同于CNN，这里的RNN其实整个神经网络都共享一组参数（U,V,W），极大减小了需要训练和预估的参数量。\n",
    "- 图中的$O_t$在有些任务下是不存在的，比如：文本情感分析，其实只需要最后的output结果就行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='rnn2'>1.3 NLP文字序列最爱的RNN</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN与生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn7.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN生成模型模仿语言风格例子：\n",
    "\n",
    "<img src='./images/rnn8.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn_hand.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在所有的RNN或LSTM的模型中，一个点不代表一个neuron，而是代表相对应的维度大小的所有的值。\n",
    "\n",
    "如：\n",
    "上图中的$s_{t}$就代表了100x100维的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不同类型的RNN\n",
    "- 双向RNN\n",
    "    - 有些情况下，当前的输出不知依赖于之前的序列元素，还可能依赖之后的序列元素\n",
    "    - 比如： 从一段话中踢掉部分词，让你补全。如：完形填空。\n",
    "    - 直观理解：双向RNN叠加\n",
    "    <img src='./images/birnn1.png' width='70%'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双向RNN（BiRNN)：  \n",
    "- $\\overrightarrow h_t$一个向量捕获了从前往后的信息，$h_t$受时刻t-1的$h_{t-1}$的影响；\n",
    "- $\\overleftarrow h_t$一个向量捕获了从后往前的信息，$h_t$受时刻t+1的$h_{t+1}$的影响；\n",
    "\n",
    "\n",
    "$U[\\overrightarrow h_t;\\overleftarrow h_t]$ 将这两个维度一样的向量做一个拼接，拼成一个矩阵U，再拿到最后的结果$y_t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在经典的循环神经网络中，状态的传输是从前往后单向的。  \n",
    "然而，在有些问题中，当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。  \n",
    "这时就需要双向RNN（BiRNN）来解决这类问题。\n",
    "\n",
    "例如：预测一个语句中缺失的单词不仅需要根据前文来判断，也需要根据后面的内容，这时双向RNN就可以发挥它的作用。\n",
    "\n",
    "双向RNN是由两个RNN上下叠加在一起组成的。输出由这两个RNN的状态共同决定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，双向RNN的主题结构就是两个单向RNN的结合。在每一个时刻t，输入会同时提供给这两个方向相反的RNN，而输出则是由这两个单向RNN共同决定（可以拼接或者求和等）。\n",
    "\n",
    "同样地，将双向RNN中的RNN替换成LSTM或者GRU结构，则组成了BiLSTM和BiGRU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 深层双向RNN\n",
    "    - 和双向RNN的区别是每一步/每个时间点我们设定多层结构\n",
    "    <img src='./images/birnn2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在同一个时间点上，反复学习了好几遍之后，得到的$y_t$。\n",
    "\n",
    "上图就是在同一时间点，学习了3遍之后得到的输出，会反复的消化知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深层双向RNN中的W，V可能是共享的，也可能在不同层次之间不是共享的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='rnn3'>1.4 BPTT算法</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLP(DNN)与CNN用BP算法求偏导\n",
    "- BPTT和BP是一个思路，只不过既然有step，就和时间t有关系\n",
    "\n",
    "<img src='./images/bptt1.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bptt2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定上面的$x_0$表示“我”，$x_1$表示“爱”，$x_2$表示“北京”，$x_3$表示“天安门”，$x_4$表示“广场”。\n",
    "\n",
    "希望预测的的output是$y_0$为“爱”的对应的向量值最大，同理，$y_1$为“北京”，$y_2$为“天安门”，$y_3$为“广场”，$y_4$为“。”\n",
    "\n",
    "假设这里的输入$x_i$是一个1x4w大小的one-hot向量，那么对应的输出的$y_i$也是1x4w的softmax的概率向量。其中$\\sum y_i=1$。\n",
    "\n",
    "想要衡量实际的$y_i$的输出和$\\hat y_i$的softmax的概率向量的差异，度量模型输出的结果和实际的结果之间的差异的**损失函数**，此处用的是**交叉熵**。\n",
    "\n",
    "$$E_t(y_t,\\hat y_t) = -y_t \\log \\hat y_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列的预测，每个位置的损失都需要纳入考量。  \n",
    "所以，每个位置上的损失最后需要相加。\n",
    "$$E(y,\\hat y)=\\sum_{t} E_t(y_t,\\hat y_t)$$\n",
    "$$=-\\sum_t y_t \\log \\hat y_t$$\n",
    "最终这个才是预测值和标准答案之间的差距。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bptt3.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着时间轴，继续往前（begin)求偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='pic'>1.5 看图说话</a>\n",
    "这个是基于相关的Paper的实际的项目“看图说话”的简单介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN与图片描述输出："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnnandpic.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看图说话，我们可能希望能够生成一句话“人带草帽”。\n",
    "\n",
    "这个网络结构是一个AlexNet的CNN，由卷积层+池化层+卷积层+池化层+卷积层+池化层+卷积层+池化层+2个全连接层FC构成。\n",
    "\n",
    "全连接层的时候，把最上面输入的图表示成 4096x1的一个向量。\n",
    "\n",
    "这个向量表示的是图像的内容。\n",
    "\n",
    "上图任务中，针对图像的内容，去表述生成一句话。如果没有图像4096x1的向量的信息的介入，可以随机生成话语。  \n",
    "\n",
    "把图像的向量灌进来这个RNN结构中，V只在Start的时候灌进来，后面就不会再添加进来。\n",
    "在论文里面，这样的效果是最好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnnandpic2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图片描述数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/coco.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/coco2.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm'>2. LSTM</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面提到的RNN的解决了，对之前的信息保存的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='lstm1'>2.1 长时依赖问题</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是！存在长期依赖的问题。\n",
    "- 看电影的时候，某些情节的推断需要依赖很久以前的一些细节。\n",
    "- 很多其他的任务也一样。\n",
    "- 很可惜随着时间间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。\n",
    "- 也就是说，记忆容量有限，一本书从头到尾一字不漏的去记，肯定离得越远的东西忘得越多。\n",
    "- 怎么办：LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RNN中，梯度下降法中，因为是链式的梯度相乘，只要里面有一个梯度为0，那结果就是0，还有连乘的量大，每个梯度都小于1的话，就会出现梯度弥散的问题。\n",
    "\n",
    "为什么会有梯度弥散？  \n",
    "因为梯度下降法求解的时候，用了一堆的连乘。\n",
    "\n",
    "在LSTM中，将其变成了加法，解决了RNN中出现的梯度弥散的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM是RNN的一种，大体结构几乎一样。 \n",
    "\n",
    "区别是：\n",
    "- 它的“记忆细胞”改造过。\n",
    "- 该记的信息会一直传递，不该记的会被“门”截断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='lstm2'>2.2 “记忆细胞”与状态</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前提到的RNN结构如下：\n",
    "\n",
    "<img src='./images/rnn_p.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们把“记忆细胞”表示得酷炫有点：\n",
    "<img src='./images/lstm_ku.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM呢？\n",
    "- “记忆细胞”变得稍微复杂了一点点\n",
    "<img src='./images/lstm_memory.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图太复杂，细节看不懂？别着急，我们解释解释。\n",
    "\n",
    "<img src='./images/lstm_detail.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural Network Layer: 逐个元素去取sigmoid  \n",
    "- Pointwise Operation：逐点的元素的乘法  \n",
    "- Vector Transfer: 向量的传播  \n",
    "- Concatenate: 拼接  \n",
    "- Copy: 复制  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM关键：“记忆细胞”\n",
    "- 细胞状态类似于**传送带**。直接在整个链上面运行，只有一些少量的线性交互。 信息在上面流传保持不变会很容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lstm_cell.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSMT怎么控制“细胞状态”?\n",
    "- 通过“门”让信息选择性通过，来去除或者增加信息到细胞状态。\n",
    "- 包含一个sigmoid神经网络层 和 一个pointwise乘法操作\n",
    "- Sigmoid层输出0-1之间的概率值，描述每个部分有多少量可以通过。\n",
    "    - 0 表示“不许任何量通过”，\n",
    "    - 1 表示“允许任意量通过”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/cell_state.png' width='15%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM的几个关键“门”与操作：\n",
    "第1步：决定从“细胞状态”中丢弃什么信息 ==>“遗忘门（forget gate）”\n",
    "    \n",
    "    比如：\n",
    "        完形填空中填“他”或者“她”的问题，细胞状态可能包含当前主语的类别，当我们看到新的代词，我们希望忘记旧的代词。\n",
    "        \n",
    "<img src='./images/gate_forget.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面介绍的Sigmoid层输出0-1之间的概率值，描述每个部分有多少量可以通过。\n",
    "\n",
    "在“遗忘门”中，由当前时刻的输入$x_t$，上个时刻的输出$h_{t-1}$，通过Sigmoid得到的$f_t$，决定了多大程度地让之前的信息通过。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第2步：决定放什么新信息到“细胞状态”中 ==>“输入门（input gate）”\n",
    "- Sigmoid层决定什么值需要更新\n",
    "- Tanh层创建一个新的候选值向量$\\overline C_t$\n",
    "\n",
    "上述2步是为状态更新做准备。\n",
    "\n",
    "<img src='./images/gate_input.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第3步：更新“细胞状态”==> 更新“细胞状态(Cell State)”\n",
    "- 更新$C_{t-1}$为$C_t$\n",
    "- 把旧状态与$f_t$相乘，丢弃掉我们确定需要丢弃的信息\n",
    "- 加上$i_t * \\overline C_t$。这就是新的后选址，根据我们决定更新每个状态的程序进行变化。\n",
    "\n",
    "\n",
    "<img src='./images/gate_update.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\overline C_t$表示的当前时候学到的全部信息；  \n",
    "$C_{t-1}$表示上一时刻的记忆；  \n",
    "$f_t$是一个0-1的概率向量；    \n",
    "$i_t$是一个0-1的概率向量；   \n",
    "\n",
    "新来的全部的信息，根据$i_t$判断抽取需要的信息；  \n",
    "上一时刻的信息，根据$f_t$来判断需要抽取的记忆。\n",
    "\n",
    "两部分的信息相加，用以更新“细胞状态”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第4步：基于“细胞状态”得到输出 ==> “输出门（output gate）”\n",
    "- 首先运行一个sigmoid层来确定细胞状态的哪个部分将输出\n",
    "- 接着用tanh处理细胞状态（得到一个在-1到1之间的值），再将它和sigmoid门的输出相乘，输出我们确定输出的那部分。\n",
    "\n",
    "比如：我们可能需要单复数信息来确定输出“他”还是“他们”\n",
    "\n",
    "<img src='./images/gate_output.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般情况下，在LSTM里面的激活函数sigmoid和tanh都是这样的。 \n",
    "\n",
    "这样的结构，保证了迭代过程中的保证了一定的范围。如果有别的，可以实现稳定的迭代效果，就可以变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='lstm3'>2.3 LSTM的变体</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 变种1：\n",
    "    - 增加“peephole connection”\n",
    "    - 让 门层 也会接受细胞状态的输入\n",
    "    \n",
    "    <img src='./images/peephole.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 变种2：\n",
    "    - 通过使用 coupled 忘记和输入门\n",
    "    - 之前是分开确定需要忘记和添加的信息，这里是一同做出决定。\n",
    "    \n",
    "<img src='./images/coupled.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='gru'>2.4 GRU</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated Recurrent Unit(GRU)，2014年提出。\n",
    "\n",
    "- 将遗忘门 和 输入门 合并成了一个单一的 更新门\n",
    "- 同样还混合了细胞状态和隐藏状态，和其他一些改动\n",
    "- 比标准LSTM简单。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/gru.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM好像很好用？   \n",
    "但是参数特别多。  \n",
    "\n",
    "GRU做了一部分简化，思想是：“丢掉了多少，我就补充多少信息”。\n",
    "$$h_t = (1-z_t) * h_{t-1} + z_t * \\overline h_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从性能上讲，GRU和LSTM的效果是差不多的。\n",
    "\n",
    "GRU和LSTM也是共享权重的，每一个cell中都是共享权重的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='application'>3.NLP的应用</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='application1'>3.1 各式各样的生成模型</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN生成模型仿照维基百科\n",
    "数据：https://cs.stanford.edu/people/karpathy/char-rnn/wiki.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集是结构化的文本序列：\n",
    "<img src='./images/wiki1.png' width='70%'/>\n",
    "<img src='./images/wiki2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN生成模型：到底发生了什么？ \n",
    "\n",
    "依旧是：https://gist.github.com/karpathy/d4dee566867f8291f086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个项目是逐个字母的去学wiki百科里面的内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn_generative.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图依次是训练100轮，200轮，300轮的学习的结果。\n",
    "\n",
    "最上面是100轮的结果，似乎什么都没学到。\n",
    "第二幅图是学到单词的长度；  \n",
    "第三幅图已经能很好的学到了单词的长度和部分单词的模式；\n",
    "到第四幅图的时候，已经可以学到大部分的单词的组合方式了；  \n",
    "到最后一幅图的时候，连接符，标点符号等模式都学到了，首字母大写的规则等。\n",
    "\n",
    "因为是字母级别（字符级别，char_rnn）的，所以学到的输出内容是一个一个字母的往外输出的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn_generative2.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这幅图是概率从高到低的图形化显示。  \n",
    "如：  \n",
    "\"www\"后面会接\".\"；  \n",
    "\n",
    "这个图是用来帮助理解LSTM的输出的倾向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 变形拓展：RNN生成模型“写”食谱？\n",
    "案例：https://gist.github.com/nylki/1efbaa36635956d35bcc%ED%AF%80%ED%B0%83   \n",
    "\n",
    "代码继续用：https://gist.github.com/karpathy/d4ee566867f8291f086  \n",
    "数据：http://www.ffts.com/recipes/lg/1g32965.zip  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/recipe.png' width='40%'/>\n",
    "<img src='./images/recipe2.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模仿奥巴马演讲？\n",
    "https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0#.9sb793kbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/obama.png' width='40%'/>\n",
    "<img src='./images/obama2.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合成音乐？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果能把音乐的乐谱表示成文本形式，是不是可以借用RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/music.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abc notation转化，参见：\n",
    "http://abcnotation.com/blog/2010/01/31/how-to-understand-abc-the-basics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/abc.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='application2'>3.2 看图说话基础版与高级版</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 看图说话高级版本：注意力模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码：https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/attention.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应不同的词会投射不同的注意力。  \n",
    "如上面图片示例中，当已经完成了\"A woman is throwing a __\"的时候，这个时候注意力会集中在\"frisbee\"上。\n",
    "\n",
    "上图示例左侧是图片，右侧标注了注意力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/attention2.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='application3'>3.3 序列到序列学习（机器翻译等）</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果用一个生成模型去做一个机器人，那么会去用序列到序列的模型。\n",
    "\n",
    "在机器翻译领域，会用Google TensorFlow中的Seq2Seq这套框架。\n",
    "\n",
    "这套框架不仅可以用翻译系统，只要是可以做成序列到序列的模型，都能用这套框架拿来用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列到序列问题的电影应用——机器翻译。\n",
    "\n",
    "下面介绍一些学者的尝试和效果对比等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#ts\">神经网络翻译系统</a>\n",
    "- <a href=\"#ts1\">初版</a>\n",
    "    - <a href=\"#ts2\">初版：小小的优化-词嵌入</a>\n",
    "    - <a href=\"#ts3\">初版：效果如何</a>\n",
    "- <a href=\"#bi\">改进I：双向RNN</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"ts1\">初版</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器翻译：\n",
    "- 输入：一串文本\n",
    "- 输出：另外一串文本\n",
    "\n",
    "最初的思路是：  \n",
    "先Encoder，后Decoder的结构。  \n",
    "用Encoder这种结构，将序列中的所有的信息，压缩在中间的“memory”当中。  \n",
    "再对“memory”在Decoder中去做解码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初版：所有的源语言信息压缩在“记忆”里\n",
    "- 再从“记忆”里解码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不管是RNN还是LSTM，通过Encoder，生成“记忆”。\n",
    "\n",
    "生成模型，先训练再预测。\n",
    "\n",
    "最初的输出是one-hot的编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/ts1.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/translation2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图介绍了编码、解码过程，还有损失函数的计算。\n",
    "\n",
    "假设这个翻译词库的大小为4w，那么每个输入的序列用one-hot编码，那么输入就是1x4w的独热向量，$h_{t-1}$是上一时刻的传递过来的“memory”的信息。\n",
    "\n",
    "解码过程，就是对记忆$h_{t-1}$的信息通过计算获得。\n",
    "\n",
    "输出层为多分类的问题，激活函数为softmax，结果是一个1x4w的概率向量。\n",
    "\n",
    "损失函数就是计算每个输出$\\hat y_i$和实际值$y_i$之间的**KL距离**，即交叉熵，使其总和最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"ts2\">初版：小小的优化-词嵌入</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于最初的输入编码为one-hot编码，则输出也是同样维度大小的概率向量。\n",
    "\n",
    "这对内存和计算的消耗是非常大。 \n",
    "\n",
    "所以，就会有常见的优化：word embedding词嵌入。\n",
    "\n",
    "原本的one-hot向量，变成比如500维的稠密向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/embedding.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"ts3\">初版：效果如何</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**所以效果如何？**   \n",
    "\n",
    "- 这种方式的NMT模型，比传统的SMT模型要差\n",
    "- 整体差很多，某些条件下差距小\n",
    "- 句子越长，效果越差\n",
    "- 词表越大，UNK（生僻词，未编码）越少，翻译效果越好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/original_ts.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图是该paper中的结果显示。\n",
    "\n",
    "其中Model\"Moses\"是SMT（Statistical Machine Translation）基于统计的翻译系统中很有名的一种算法。一般情况下，会把它作为比较的对象。\n",
    "\n",
    "评估结果是基于n-gram的BLEU评估指标，数值越大，结果就越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在全部的情况下，RNNenc和grConv的表现很差，但是在“没有未知的词”的情况下，表现提升比较大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们不可能把所有的词都做编码，高频词去做编码，如果是生僻词出现，则RNNenc的效果不会很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn_enc.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子越长，效果越差。\n",
    "\n",
    "在RNN中，需要将序列的信息，全部在Encoder中压缩到“memory”中。  \n",
    "\n",
    "“memory”的维度是有限的，丢进来训练的平行语料的长度非常长的话，比如300字，那么“memory”对其的记忆是很困难的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"bi\">改进I：双向RNN</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/birnn.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双向RNN就是：\n",
    "- 一个从前往后读序列，进行Encoder，获得$\\overrightarrow h_t$；\n",
    "- 一个从后往前读序列，进行Encoder，获得$\\overleftarrow h_t$；\n",
    "- 最后，将得到的$\\overrightarrow h_t$和$\\overleftarrow h_t$通过拼接的形式，最后通过计算，得到$y_t$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在翻译系统中，一种语言的序列和另一种语言的序列，不可能是一一对应的。一定会出现词汇序列的顺序的不一致。\n",
    "\n",
    "比如：  \n",
    "英文和中文的倒装，主谓宾结构等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 改进的模型：\n",
    "    - 双向RNN用于捕获周边（两侧的信息）\n",
    "    - “注意力”模型关注当前翻译的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/improve.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder部分非常的简单。\n",
    "\n",
    "就是一个从前往后的RNN和一个从后往前的RNN，组合在一起的双向的RNN。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/improve_decoder.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当i=0的时候，看双向RNN的结构图可以看出来，我们只有$\\overleftarrow h_i$有内容，$\\overrightarrow h_i$中是没有memory的，所以其解码的时候计算为$tanh(W_s \\overleftarrow h_i)$。\n",
    "\n",
    "\n",
    "翻译的过程，也是一个词一个词出来的。\n",
    "\n",
    "翻译的时候依赖于它的输出和之前的memory还有\"注意力\"模型参数。  \n",
    "$y_i$是输出的结果；    \n",
    "$S_{i-1}$是之前的解码过程的memory；    \n",
    "$c_i$是“注意力”模型的向量。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/improve_attention.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha_{ij}$就是“注意力”的概率向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/more_encoder.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/more_attention.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow序列到序列的学习：\n",
    "- https://www.tensorflow.org/tutorials/seq2seq\n",
    "- https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/seq2seq\n",
    "- https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/s2s.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来看看Tensorflow的实现：\n",
    "\n",
    "<img src='./images/tf_s2s.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些tricks:\n",
    "- Sampled softmax and output projection\n",
    "<img src='./images/trick1.png' width='70%'/>\n",
    "- Bucketing and padding\n",
    "<img src='./images/trick2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络翻译系统的现状？\n",
    "<img src='./images/status.png' width='70%'/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
