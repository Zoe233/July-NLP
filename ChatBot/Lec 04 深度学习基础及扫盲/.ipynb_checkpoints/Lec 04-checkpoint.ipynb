{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 04 深度学习基础及扫盲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主要内容：\n",
    "- <a href='#rnn'>1. 循环神经网络RNN</a>\n",
    "    - <a href='#rnn1'>1.1 场景与多种引用</a>\n",
    "    - <a href='#dnn_rnn'>1.2 神经网络到循环神经网络</a>\n",
    "    - <a href='#rnn2'>1.2 NLP文字序列最爱的RNN</a>\n",
    "    - <a href='#rnn3'>1.3 BPTT算法</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#lstm'>2. LSTM</a>\n",
    "    - <a href='#lstm1'>2.1 长时依赖问题</a>\n",
    "    - <a href='#lstm2'>2.2 “记忆细胞”与状态</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#application'>3.NLP的应用</a>\n",
    "    - <a href='#application1'>3.1 各式各样的生成模型</a>\n",
    "    - <a href='#application2'>3.2 看图说话基础版与高级版</a>\n",
    "    - <a href='#application3'>3.3 序列到序列学习（机器翻译等）</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='rnn'>1. 循环神经网络RNN</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='rnn1'>1.1 场景与多种引用</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理中，另外一个重要的应用领域，就是**文本的自动撰写**。\n",
    "\n",
    "关键词、关键短语、自动摘要提取都属于这个领域中的一种应用。\n",
    "不过这些应用，都是**由多到少的生成**。\n",
    "\n",
    "这里我们介绍其另外一种应用：**由少到多的生成**，包括句子的复写，由关键词、主题生成文章或者段落等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 由多到少的生成：\n",
    "    - 关键词\n",
    "    - 关键短语\n",
    "    - 自动摘要\n",
    "\n",
    "- 又少到多的生成：\n",
    "    - 句子的复写\n",
    "    - 由关键词、主题生成文章或者段落"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如MarkDown，Latex等这种有模式pattern的语言，都可以通过RNN（循环神经网络）的生成模型，学习到如何生成相应的符合其pattern的产物，如：Markdown格式的笔记，Latex格式的数学公式等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面介绍循环神经网络的应用：\n",
    "- 模仿论文（连公式的格式都是正确的）\n",
    "- 模仿Linux内核代码“写程序”\n",
    "- 模仿小四的作品\n",
    "- 机器翻译（Seq2Seq）\n",
    "- 看图说话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn1.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这篇生成的论文，在样式上是符合人类的论文的书写习惯，可能内容上没有什么实际的研究意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn2.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的语言在做编码的时候，语法是不一样的，如java和c需要标记变量的类型，而python则是东岱语言，不需要预先定义变量的类型，而是根据赋值的类型而变化。\n",
    "\n",
    "学习到编码方式和风格，RNN将序列的模式学会，前后组合在一起，有具体的实际意义。然后RNN可以通过这样的序列，生成相应的序列。如：论文和代码都不一定有实际意义，但是至少都是符合相关的语法和模式的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn3.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果序列对应的是中文，那么生成的就是中文。\n",
    "\n",
    "模型的参数如何；  \n",
    "给定的数据如何（丰富度，准确性等）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn4.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列到序列，Seq2Seq的应用。\n",
    "\n",
    "对于计算机而言，喂给的是向量，生成的向量，只是向量相应的映射的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn5.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个任务，在后续会讲解。 \n",
    "\n",
    "中间的一副图：问和答，是更高级的一种。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='dnn_rnn'>1.2 神经网络到循环神经网络</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/nn1.png' width='70%'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络中，我们有很多种的结构和名称。\n",
    "\n",
    "如：\n",
    "- MLP/DNN： 一般的神经网络结构比较简单，input layer, n * hidden layer , output layer\n",
    "- CNN卷积神经网络，主要用于处理图像，也可以应用于自然语言处理等多方面\n",
    "- RNN( LSTM, GRU)： 自然语言处理很多的处理序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 循环神经网络RNN\n",
    "\n",
    "为什么有BP神经网络，CNN，还要RNN?\n",
    "- 传统神经网络（包括CNN），输入和输出都是相互独立的。\n",
    "    - 图像上的猫和狗是分隔开的，但有些任务，后续的输出和之前的内容是相关的。\n",
    "    - 如：“我是中国人，我的母语是__”\n",
    "- RNN引入“记忆”的概念\n",
    "    - 循环2字 来源于其每个元素都执行相同的任务。\n",
    "    - 但是输出依赖于 “输入”和“记忆”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统的MLP/DNN，都是输入和输出相互独立的。\n",
    "\n",
    "但在，文本的序列问题，前后是有关联的。 这种依赖于上下文，在RNN中引入了“记忆”的概念。\n",
    "\n",
    "如上面的应用中的看图提问并回答，那么提问的就要存在“记忆”中，然后基于问题去回答。\n",
    "\n",
    "“循环”相当于一个“回路”，就是一遍一遍地去做。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='rnn2'>1.2 NLP文字序列最爱的RNN</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn6.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_t$是时间t处的输入\n",
    "- $S_t$是时间t处的“记忆”，$S_t = f(UW_t +WS_{t-1})$，f可以是tanh等\n",
    "- $O_t$是时间t处的输出，比如：是预测下个词的话，可能是softmax输出的属于每个候选词的概率，$O_t = softmax(VS_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**循环神经网络之 结构细节 **\n",
    "- 可以把隐状态$S_t$视作“记忆体”，捕捉了之前时间点上的信息。\n",
    "- 输出$O_t$由当前时间及之前所有的“记忆“共同计算得到。\n",
    "- 很可惜，实际应用中，$S_t$并不能捕捉和保留之前所有信息（记忆有限？）\n",
    "- 不同于CNN，这里的RNN其实整个神经网络都共享一组参数（U,V,W），极大减小了需要训练和预估的参数量。\n",
    "- 图中的$O_t$在有些任务下是不存在的，比如：文本情感分析，其实只需要最后的output结果就行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN与生成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn7.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'\n",
    "\n",
    "# 读取数据，添加SENTENCE_START和SENTENCENT_END在开头和结尾\n",
    "print('Reading CSV file...')\n",
    "with open('data/reddit-comments-2015-08.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.next()\n",
    "    # 分句\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN生成模型模仿语言风格例子：\n",
    "\n",
    "<img src='./images/rnn8.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不同类型的RNN\n",
    "- 双向RNN\n",
    "    - 有些情况下，当前的输出不知依赖于之前的序列元素，还可能依赖之后的序列元素\n",
    "    - 比如： 从一段话中踢掉部分词，让你补全\n",
    "    - 直观理解：双向RNN叠加\n",
    "    <img src='./images/birnn1.png' width='70%'/>\n",
    "\n",
    "- 深层双向RNN\n",
    "    - 和双向RNN的区别是每一步/每个时间点我们设定多层结构\n",
    "    <img src='./images/birnn2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='rnn3'>1.3 BPTT算法</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MLP(DNN)与CNN用BP算法求偏导\n",
    "- BPTT和BP是一个思路，只不过既然有step，就和时间t有关系\n",
    "\n",
    "<img src='./images/bptt1.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bptt2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bptt3.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN与图片描述输出："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnnandpic.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnnandpic2.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图片描述数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
