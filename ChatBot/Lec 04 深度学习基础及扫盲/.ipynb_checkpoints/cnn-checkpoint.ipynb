{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#cnn'>一、卷积神经网络CNN的模型结构</a>\n",
    "- <a href='#cnn1'>1. CNN的基本结构</a>\n",
    "- <a href='#cnn2'>2. 初识卷积</a>\n",
    "- <a href='#cnn3'>3. CNN中的卷积层</a>\n",
    "- <a href='#cnn4'>4. CNN中的池化层</a>\n",
    "- <a href='#cnn5'>5. CNN模型结构小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#fp'>二、卷积神经网络CNN前向传播算法</a>\n",
    "- <a href='#fp1'>1. 回顾CNN的结构</a>\n",
    "- <a href='#fp2'>2. CNN输入层前向传播到卷积层</a>\n",
    "- <a href='#fp3'>3. 隐藏层前向传播到卷积层</a>\n",
    "- <a href='#fp4'>4. 隐藏层前向传播到池化层</a>\n",
    "- <a href='#fp5'>5. 影藏层前向传播到全连接层</a>\n",
    "- <a href='#fp6'>6. CNN前向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#bp'>三、卷积神经网络CNN反向传播算法</a>\n",
    "- <a href='#bp1'>1. 回顾DNN的反向传播算法</a>\n",
    "- <a href='#bp2'>2. CNN的反向传播算法思想</a>\n",
    "- <a href='#bp3'>3. 已知池化层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n",
    "- <a href='#bp4'>4. 已知卷积层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n",
    "- <a href='#bp5'>5. 已知卷积层的$\\delta^l$，推导该层的W,b的梯度</a>\n",
    "- <a href='#bp6'>6. CNN反向传播算法总结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='cnn'>一、卷积神经网络CNN的模型结构</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面我们讲述了DNN的模型与前向反向传播算法。\n",
    "\n",
    "而在DNN大类中，**卷积神经网络(Convolutional Neural Networks，以下简称CNN)是最为成功的DNN特例之一。**\n",
    "\n",
    "CNN广泛的应用于**图像识别**，当然现在也应用于**NLP等其他领域**，本文我们就对CNN的模型结构做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn1'>1. CNN的基本结构</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们来看看CNN的基本结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个常见的CNN例子如下图：\n",
    "\n",
    "<img src='./images/cnn_basic.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中是一个图形识别的CNN模型。\n",
    "\n",
    "可以看出最左边的船的图像就是我们的**输入层**，计算机理解为**输入若干个矩阵**，这点和DNN基本相同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着是**卷积层（Convolution Layer）**，这个是CNN特有的，我们后面专门来讲。\n",
    "卷积层的激活函数使用的是ReLU。我们在DNN中介绍过ReLU的激活函数，它其实很简单，就是ReLU(x)=max(0,x)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在卷积层后面是**池化层(Pooling layer)**，这个也是CNN特有的，我们后面也会专门来讲。\n",
    "需要注意的是，**池化层没有激活函数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积层+池化层的组合**可以**在隐藏层出现很多次**，上图中出现两次。\n",
    "\n",
    "而实际上这个次数是根据模型的需要而来的。\n",
    "\n",
    "当然我们也可以灵活使用使用**卷积层+卷积层**，或者**卷积层+卷积层+池化层**的组合，这些在构建模型的时候没有限制。\n",
    "\n",
    "但是最常见的CNN都是**若干卷积层+池化层的组合**，如上图中的CNN结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在若干卷积层+池化层后面是**全连接层（Fully Connected Layer, 简称FC）**，全连接层其实就是我们前面讲的DNN结构，只是**输出层使用了Softmax激活函数来做图像识别的分类**，这点我们在DNN中也有讲述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面CNN的模型描述可以看出，CNN相对于DNN，**比较特殊的是卷积层和池化层**，如果我们熟悉DNN，只要把卷积层和池化层的原理搞清楚了，那么搞清楚CNN就容易很多了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn2'>2. 初识卷积</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们去学习卷积层的模型原理，在学习卷积层的模型原理前，我们需要了解**什么是卷积**，以及**CNN中的卷积是什么样子的**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家学习数学时都有学过卷积的知识，微积分中卷积的表达式为：\n",
    "$$S(t) = \\int x(t-a)w(a)da$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "离散形式是：\n",
    "$$s(t) = \\sum_a x(t-a)w(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个式子如果用矩阵表示可以为：\n",
    "$$s(t)=(X*W)(t)$$\n",
    "\n",
    "其中星号\\*表示卷积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是二维的卷积，则表示式为：\n",
    "<img src='./images/convolutional1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CNN中，虽然我们也是说卷积，但是我们的卷积公式和严格意义数学中的定义稍有不同,比如对于二维的卷积，定义为：\n",
    "\n",
    "<img src='./images/convolutional2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个式子虽然从数学上讲不是严格意义上的卷积，但是大牛们都这么叫了，那么我们也跟着这么叫了。后面讲的CNN的卷积都是指的上面的最后一个式子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，我们叫W为我们的**卷积核**，而X则为我们的输入。\n",
    "\n",
    "如果X是一个二维输入的矩阵，而W也是一个二维的矩阵。  \n",
    "\n",
    "但是如果X是多维张量，那么W也是一个多维的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn3'>3. CNN中的卷积层</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了卷积的基本知识，我们现在来**看看CNN中的卷积**。\n",
    "\n",
    "假如是对图像卷积，回想我们的上一节的卷积公式，其实就是**对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子如下，图中的输入是一个二维的3x4的矩阵，而卷积核是一个2x2的矩阵。\n",
    "\n",
    "这里我们假设卷积是**一次移动一个像素**来卷积的。\n",
    "\n",
    "那么，首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的$S_{00}$的元素，值为$aw+bx+ey+fz$。\n",
    "\n",
    "接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的$S_{01}$的元素，同样的方法，我们可以得到输出矩阵S的$S_{02}$，$S_{10}$，$S_{11}$，$S_{12}$的元素。\n",
    "\n",
    "最终我们得到卷积输出的矩阵为一个2x3的矩阵S。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/convolutional3.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再举一个动态的卷积过程的例子如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有下面这个绿色的5x5输入矩阵，卷积核是一个下面这个黄色的3x3的矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3x3的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/convolutional4.png' width='20%'/>\n",
    "<img src='./images/convolutional5.png' width='10%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/convolutional7.gif' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面举的例子都是二维的输入，卷积的过程比较简单，那么**如果输入是多维的呢？** \n",
    "\n",
    "比如在前面一组**卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？**\n",
    "\n",
    "又比如**输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢**？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在斯坦福大学的cs231n的课程上，有一个<a href='http://cs231n.github.io/assets/conv-demo/index.html'>动态的例子，链接在这</a>。建议大家对照着例子中的动图看下面的讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家打开这个例子可以看到，这里面输入是3个7x7的矩阵。\n",
    "\n",
    "实际上原输入是3个5x5的矩阵。只是在原来的输入周围加上了**1的padding**，即将周围都填充一圈的0，变成了3个7x7的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子里面使用了**两个卷积核**，我们先关注于卷积核$W_0$。\n",
    "\n",
    "和上面的例子相比，由于输入是3个7x7的矩阵，或者说是7x7x3的张量，则我们对应的卷积核$W_0$也必须最后一维是3的张量，这里卷积核$W_0$的单个子矩阵维度为3x3。那么卷积核$W_0$实际上是一个3x3x3的张量。\n",
    "\n",
    "同时和上面的例子比，这里的步幅为2，也就是每次卷积后会移动2个像素的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的卷积过程和上面的2维矩阵类似，上面是矩阵的卷积，即**两个矩阵对应位置的元素相乘后相加**。这里是张量的卷积，即两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏倚b。\n",
    "\n",
    "两个张量分别是W0和W1，是3x3x3的张量。  \n",
    "一个张量得到一个Output Volume，是一个3x3x2的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7x7x3的张量和3x3x3的卷积核张量W0卷积的结果是一个3x3的矩阵。\n",
    "\n",
    "由于我们有两个卷积核W0和W1，因此最后卷积的结果是两个3x3的矩阵。\n",
    "\n",
    "或者说卷积的结果是一个3x3x2的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细回味下卷积的过程，输入是7x7x3的张量，卷积核是两个3x3x3的张量。\n",
    "\n",
    "卷积步幅为2，最后得到了输出是3x3x2的张量。\n",
    "\n",
    "如果把上面的卷积过程用数学公式表达出来就是：\n",
    "<img src='./images/convolutional6.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$n_{in}$为输入矩阵的个数，或者是张量的最后一维的维数。   \n",
    "$X_k$代表第k个输入矩阵。  \n",
    "$W_k$代表卷积核的第k个子卷积核矩阵。   \n",
    "$s(i,j)$即卷积核W对应的输出矩阵的对应位置元素的值。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的例子，相信大家对CNN的卷积层的卷积过程有了一定的了解。\n",
    "\n",
    "对于卷积后的输出，一般会通过**ReLU激活函数**，将输出的张量中的小于0的位置对应的元素值都变为0。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn4'>4. CNN中的池化层</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比卷积层的复杂，池化层则要简单的多，所谓的池化，个人理解就是**对输入张量的各个子矩阵进行压缩。**\n",
    "\n",
    "假如是2x2的池化，那么就将子矩阵的每2x2个元素变成一个元素，如果是3x3的池化，那么就将子矩阵的每3x3个元素变成一个元素，这样输入矩阵的维度就变小了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想将输入子矩阵的每nxn个元素变成一个元素，那么需要一个池化标准。\n",
    "\n",
    "常见的池化标准有2个：\n",
    "- MAX\n",
    "- 或者是Average。\n",
    "\n",
    "即**取对应区域的最大值或者平均值作为池化后的元素值**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个例子采用取最大值的池化方法。同时采用的是2x2的池化。步幅为2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对红色2x2区域进行池化，由于此2x2区域的最大值为6.   \n",
    "那么对应的池化输出位置的值为6，由于步幅为2，此时移动到绿色的位置去进行池化，输出的最大值为8.\n",
    "同样的方法，可以得到黄色区域和蓝色区域的输出值。 \n",
    "\n",
    "最终，我们的输入4x4的矩阵在池化后变成了2x2的矩阵。进行了压缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/pooling1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn5'>5. CNN模型结构小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解了CNN模型中的卷积层和池化层，就基本理解了CNN的基本原理，后面再去理解CNN模型的前向传播算法和反向传播算法就容易了。下一篇我们就来讨论CNN模型的前向传播算法。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='fp'>二、卷积神经网络CNN前向传播算法</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在卷积神经网络(CNN)模型结构中，我们对CNN的模型结构做了总结，这里我们就在CNN的模型基础上，看看CNN的前向传播算法是什么样子的。重点会和传统的DNN比较讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp1'>1. 回顾CNN的结构</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一篇里，我们已经讲到了CNN的结构，包括：\n",
    "- 输入层，\n",
    "- 若干的卷积层+ReLU激活函数，\n",
    "- 若干的池化层，\n",
    "- DNN全连接层，\n",
    "- 以及最后的用Softmax激活函数的输出层。\n",
    "\n",
    "这里我们用一个彩色的汽车样本的图像识别再从感官上回顾下CNN的结构。   \n",
    "图中的CONV即为卷积层，POOL即为池化层，而FC即为DNN全连接层，包括了我们上面最后的用Softmax激活函数的输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/cnn_struc1.jpg' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，要理顺CNN的前向传播算法，重点是:\n",
    "- 输入层的前向传播，\n",
    "- 卷积层的前向传播\n",
    "- 以及池化层的前向传播。\n",
    "\n",
    "而DNN全连接层和用Softmax激活函数的输出层的前向传播算法我们在讲DNN时已经讲到了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp2'>2. CNN输入层前向传播到卷积层</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入层的前向传播是CNN前向传播算法的第一步。\n",
    "\n",
    "一般输入层对应的都是卷积层，因此我们标题是输入层前向传播到卷积层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们这里还是以图像识别为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先考虑最简单的，样本都是二维的黑白图片。这样输入层X就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核W就也是矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果样本都是有RGB的彩色图片，这样输入X就是3个矩阵，即分别对应R，G和B的矩阵，或者说是一个张量。这时和卷积层相连的卷积核W就也是张量，对应的最后一维的维度为3。即每个卷积核都是3个子矩阵组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的方法，对于3D的彩色图片之类的样本，我们的输入X可以是4维，5维的张量，那么对应的卷积核W也是个高维的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不管维度多高，对于我们的输入，前向传播的过程可以表示为：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## <a name='fp3'>3. 隐藏层前向传播到卷积层</a>\n",
    "## <a name='fp4'>4. 隐藏层前向传播到池化层</a>\n",
    "## <a name='fp5'>5. 影藏层前向传播到全连接层</a>\n",
    "## <a name='fp6'>6. CNN前向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='bp'>一、卷积神经网络CNN反向传播算法</a>\n",
    "- <a name='bp1'>1. 回顾DNN的反向传播算法</a>\n",
    "- <a name='bp2'>2. CNN的反向传播算法思想</a>\n",
    "- <a name='bp3'>3. 已知池化层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n",
    "- <a name='bp4'>4. 已知卷积层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n",
    "- <a name='bp5'>5. 已知卷积层的$\\delta^l$，推导该层的W,b的梯度</a>\n",
    "- <a name='bp6'>6. CNN反向传播算法总结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
