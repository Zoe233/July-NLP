{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#cnn'>一、卷积神经网络CNN的模型结构</a>\n",
    "- <a href='#cnn1'>1. CNN的基本结构</a>\n",
    "- <a href='#cnn2'>2. 初识卷积</a>\n",
    "- <a href='#cnn3'>3. CNN中的卷积层</a>\n",
    "- <a href='#cnn4'>4. CNN中的池化层</a>\n",
    "- <a href='#cnn5'>5. CNN模型结构小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#fp'>二、卷积神经网络CNN前向传播算法</a>\n",
    "- <a href='#fp1'>1. 回顾CNN的结构</a>\n",
    "- <a href='#fp2'>2. CNN输入层前向传播到卷积层</a>\n",
    "- <a href='#fp3'>3. 隐藏层前向传播到卷积层</a>\n",
    "- <a href='#fp4'>4. 隐藏层前向传播到池化层</a>\n",
    "- <a href='#fp5'>5. 影藏层前向传播到全连接层</a>\n",
    "- <a href='#fp6'>6. CNN前向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#bp'>三、卷积神经网络CNN反向传播算法</a>\n",
    "- <a href='#bp1'>1. 回顾DNN的反向传播算法</a>\n",
    "- <a href='#bp2'>2. CNN的反向传播算法思想</a>\n",
    "- <a href='#bp3'>3. 已知池化层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n",
    "- <a href='#bp4'>4. 已知卷积层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n",
    "- <a href='#bp5'>5. 已知卷积层的$\\delta^l$，推导该层的W,b的梯度</a>\n",
    "- <a href='#bp6'>6. CNN反向传播算法总结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='cnn'>一、卷积神经网络CNN的模型结构</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面我们讲述了DNN的模型与前向反向传播算法。\n",
    "\n",
    "而在DNN大类中，**卷积神经网络(Convolutional Neural Networks，以下简称CNN)是最为成功的DNN特例之一。**\n",
    "\n",
    "CNN广泛的应用于**图像识别**，当然现在也应用于**NLP等其他领域**，本文我们就对CNN的模型结构做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn1'>1. CNN的基本结构</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们来看看CNN的基本结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个常见的CNN例子如下图：\n",
    "\n",
    "<img src='./images/cnn_basic.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中是一个图形识别的CNN模型。\n",
    "\n",
    "可以看出最左边的船的图像就是我们的**输入层**，计算机理解为**输入若干个矩阵**，这点和DNN基本相同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着是**卷积层（Convolution Layer）**，这个是CNN特有的，我们后面专门来讲。\n",
    "卷积层的激活函数使用的是ReLU。我们在DNN中介绍过ReLU的激活函数，它其实很简单，就是ReLU(x)=max(0,x)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在卷积层后面是**池化层(Pooling layer)**，这个也是CNN特有的，我们后面也会专门来讲。\n",
    "需要注意的是，**池化层没有激活函数**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积层+池化层的组合**可以**在隐藏层出现很多次**，上图中出现两次。\n",
    "\n",
    "而实际上这个次数是根据模型的需要而来的。\n",
    "\n",
    "当然我们也可以灵活使用使用**卷积层+卷积层**，或者**卷积层+卷积层+池化层**的组合，这些在构建模型的时候没有限制。\n",
    "\n",
    "但是最常见的CNN都是**若干卷积层+池化层的组合**，如上图中的CNN结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在若干卷积层+池化层后面是**全连接层（Fully Connected Layer, 简称FC）**，全连接层其实就是我们前面讲的DNN结构，只是**输出层使用了Softmax激活函数来做图像识别的分类**，这点我们在DNN中也有讲述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面CNN的模型描述可以看出，CNN相对于DNN，**比较特殊的是卷积层和池化层**，如果我们熟悉DNN，只要把卷积层和池化层的原理搞清楚了，那么搞清楚CNN就容易很多了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn2'>2. 初识卷积</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们去学习卷积层的模型原理，在学习卷积层的模型原理前，我们需要了解**什么是卷积**，以及**CNN中的卷积是什么样子的**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家学习数学时都有学过卷积的知识，微积分中卷积的表达式为：\n",
    "$$S(t) = \\int x(t-a)w(a)da$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "离散形式是：\n",
    "$$s(t) = \\sum_a x(t-a)w(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个式子如果用矩阵表示可以为：\n",
    "$$s(t)=(X*W)(t)$$\n",
    "\n",
    "其中星号\\*表示卷积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是二维的卷积，则表示式为：\n",
    "<img src='./images/convolutional1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CNN中，虽然我们也是说卷积，但是我们的卷积公式和严格意义数学中的定义稍有不同,比如对于二维的卷积，定义为：\n",
    "\n",
    "<img src='./images/convolutional2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个式子虽然从数学上讲不是严格意义上的卷积，但是大牛们都这么叫了，那么我们也跟着这么叫了。后面讲的CNN的卷积都是指的上面的最后一个式子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，我们叫W为我们的**卷积核**，而X则为我们的输入。\n",
    "\n",
    "如果X是一个二维输入的矩阵，而W也是一个二维的矩阵。  \n",
    "\n",
    "但是如果X是多维张量，那么W也是一个多维的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn3'>3. CNN中的卷积层</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了卷积的基本知识，我们现在来**看看CNN中的卷积**。\n",
    "\n",
    "假如是对图像卷积，回想我们的上一节的卷积公式，其实就是**对输入的图像的不同局部的矩阵和卷积核矩阵各个位置的元素相乘，然后相加得到**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子如下，图中的输入是一个二维的3x4的矩阵，而卷积核是一个2x2的矩阵。\n",
    "\n",
    "这里我们假设卷积是**一次移动一个像素**来卷积的。\n",
    "\n",
    "那么，首先我们对输入的左上角2x2局部和卷积核卷积，即各个位置的元素相乘再相加，得到的输出矩阵S的$S_{00}$的元素，值为$aw+bx+ey+fz$。\n",
    "\n",
    "接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核来卷积，这样我们得到了输出矩阵S的$S_{01}$的元素，同样的方法，我们可以得到输出矩阵S的$S_{02}$，$S_{10}$，$S_{11}$，$S_{12}$的元素。\n",
    "\n",
    "最终我们得到卷积输出的矩阵为一个2x3的矩阵S。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/convolutional3.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再举一个动态的卷积过程的例子如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有下面这个绿色的5x5输入矩阵，卷积核是一个下面这个黄色的3x3的矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3x3的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/convolutional4.png' width='20%'/>\n",
    "<img src='./images/convolutional5.png' width='10%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/convolutional7.gif' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面举的例子都是二维的输入，卷积的过程比较简单，那么**如果输入是多维的呢？** \n",
    "\n",
    "比如在前面一组**卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？**\n",
    "\n",
    "又比如**输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢**？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在斯坦福大学的cs231n的课程上，有一个<a href='http://cs231n.github.io/assets/conv-demo/index.html'>动态的例子，链接在这</a>。建议大家对照着例子中的动图看下面的讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家打开这个例子可以看到，这里面输入是3个7x7的矩阵。\n",
    "\n",
    "实际上原输入是3个5x5的矩阵。只是在原来的输入周围加上了**1的padding**，即将周围都填充一圈的0，变成了3个7x7的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子里面使用了**两个卷积核**，我们先关注于卷积核$W_0$。\n",
    "\n",
    "和上面的例子相比，由于输入是3个7x7的矩阵，或者说是7x7x3的张量，则我们对应的卷积核$W_0$也必须最后一维是3的张量，这里卷积核$W_0$的单个子矩阵维度为3x3。那么卷积核$W_0$实际上是一个3x3x3的张量。\n",
    "\n",
    "同时和上面的例子比，这里的步幅为2，也就是每次卷积后会移动2个像素的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的卷积过程和上面的2维矩阵类似，上面是矩阵的卷积，即**两个矩阵对应位置的元素相乘后相加**。这里是张量的卷积，即两个张量的3个子矩阵卷积后，再把卷积的结果相加后再加上偏倚b。\n",
    "\n",
    "两个张量分别是W0和W1，是3x3x3的张量。  \n",
    "一个张量得到一个Output Volume，是一个3x3x2的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7x7x3的张量和3x3x3的卷积核张量W0卷积的结果是一个3x3的矩阵。\n",
    "\n",
    "由于我们有两个卷积核W0和W1，因此最后卷积的结果是两个3x3的矩阵。\n",
    "\n",
    "或者说卷积的结果是一个3x3x2的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细回味下卷积的过程，输入是7x7x3的张量，卷积核是两个3x3x3的张量。\n",
    "\n",
    "卷积步幅为2，最后得到了输出是3x3x2的张量。\n",
    "\n",
    "如果把上面的卷积过程用数学公式表达出来就是：\n",
    "<img src='./images/convolutional6.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$n_{in}$为输入矩阵的个数，或者是张量的最后一维的维数。   \n",
    "$X_k$代表第k个输入矩阵。  \n",
    "$W_k$代表卷积核的第k个子卷积核矩阵。   \n",
    "$s(i,j)$即卷积核W对应的输出矩阵的对应位置元素的值。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的例子，相信大家对CNN的卷积层的卷积过程有了一定的了解。\n",
    "\n",
    "对于卷积后的输出，一般会通过**ReLU激活函数**，将输出的张量中的小于0的位置对应的元素值都变为0。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn4'>4. CNN中的池化层</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比卷积层的复杂，池化层则要简单的多，所谓的池化，个人理解就是**对输入张量的各个子矩阵进行压缩。**\n",
    "\n",
    "假如是2x2的池化，那么就将子矩阵的每2x2个元素变成一个元素，如果是3x3的池化，那么就将子矩阵的每3x3个元素变成一个元素，这样输入矩阵的维度就变小了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想将输入子矩阵的每nxn个元素变成一个元素，那么需要一个池化标准。\n",
    "\n",
    "常见的池化标准有2个：\n",
    "- MAX\n",
    "- 或者是Average。\n",
    "\n",
    "即**取对应区域的最大值或者平均值作为池化后的元素值**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面这个例子采用取最大值的池化方法。同时采用的是2x2的池化。步幅为2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对红色2x2区域进行池化，由于此2x2区域的最大值为6.   \n",
    "那么对应的池化输出位置的值为6，由于步幅为2，此时移动到绿色的位置去进行池化，输出的最大值为8.\n",
    "同样的方法，可以得到黄色区域和蓝色区域的输出值。 \n",
    "\n",
    "最终，我们的输入4x4的矩阵在池化后变成了2x2的矩阵。进行了压缩。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/pooling1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='cnn5'>5. CNN模型结构小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解了CNN模型中的卷积层和池化层，就基本理解了CNN的基本原理，后面再去理解CNN模型的前向传播算法和反向传播算法就容易了。下一篇我们就来讨论CNN模型的前向传播算法。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='fp'>二、卷积神经网络CNN前向传播算法</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在卷积神经网络(CNN)模型结构中，我们对CNN的模型结构做了总结，这里我们就在CNN的模型基础上，看看CNN的前向传播算法是什么样子的。重点会和传统的DNN比较讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp1'>1. 回顾CNN的结构</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一篇里，我们已经讲到了CNN的结构，包括：\n",
    "- 输入层，\n",
    "- 若干的卷积层+ReLU激活函数，\n",
    "- 若干的池化层，\n",
    "- DNN全连接层，\n",
    "- 以及最后的用Softmax激活函数的输出层。\n",
    "\n",
    "这里我们用一个彩色的汽车样本的图像识别再从感官上回顾下CNN的结构。   \n",
    "图中的CONV即为卷积层，POOL即为池化层，而FC即为DNN全连接层，包括了我们上面最后的用Softmax激活函数的输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/cnn_struc1.jpg' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，要理顺CNN的前向传播算法，重点是:\n",
    "- 输入层的前向传播，\n",
    "- 卷积层的前向传播\n",
    "- 以及池化层的前向传播。\n",
    "\n",
    "而DNN全连接层和用Softmax激活函数的输出层的前向传播算法我们在讲DNN时已经讲到了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp2'>2. CNN输入层前向传播到卷积层</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入层的前向传播是CNN前向传播算法的第一步。\n",
    "\n",
    "一般输入层对应的都是卷积层，因此我们标题是输入层前向传播到卷积层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们这里还是以图像识别为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先考虑最简单的，样本都是二维的黑白图片。这样输入层X就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核W就也是矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果样本都是有RGB的彩色图片，这样输入X就是3个矩阵，即分别对应R，G和B的矩阵，或者说是一个张量。这时和卷积层相连的卷积核W就也是张量，对应的最后一维的维度为3。即每个卷积核都是3个子矩阵组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的方法，对于3D的彩色图片之类的样本，我们的输入X可以是4维，5维的张量，那么对应的卷积核W也是个高维的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不管维度多高，对于我们的输入，前向传播的过程可以表示为：\n",
    "\n",
    "<img src='./images/cnn_fp1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, σ为激活函数，这里一般都是ReLU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和DNN的前向传播比较一下，其实形式非常的像，只是我们这儿是**张量的卷积**，而不是矩阵的乘法。同时由于W是张量，那么同样的位置，W参数的个数就比DNN多很多了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化我们的描述，本文后面如果没有特殊说明，我们都默认输入是3维的张量，即用RBG可以表示的彩色图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要我们自己定义的CNN模型参数是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1） 一般我们的卷积核不止一个，比如有K个，那么我们输入层的输出，或者说第二层卷积层的对应的输入就K个。\n",
    "\n",
    "2） 卷积核中每个子矩阵的的大小，一般我们都用子矩阵为方阵的卷积核，比如FxF的子矩阵。\n",
    "\n",
    "3） 填充padding（以下简称P），我们卷积的时候，为了可以更好的识别边缘，一般都会在输入矩阵在周围加上若干圈的0再进行卷积，加多少圈则P为多少。\n",
    "\n",
    "4） 步幅stride（以下简称S），即在卷积过程中每次移动的像素距离大小。\n",
    "\n",
    "这些参数我们在上一篇都有讲述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp3'>3. 隐藏层前向传播到卷积层</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们再来看普通隐藏层前向传播到卷积层时的前向传播算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设隐藏层的输出是M个矩阵对应的三维张量，则输出到卷积层的卷积核也是M个子矩阵对应的三维张量。这时表达式和输入层的很像，也是\n",
    "<img src='./images/cnn_fp2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, σ为激活函数，这里一般都是ReLU。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以写成M个子矩阵子矩阵卷积后对应位置相加的形式，即：\n",
    "<img src='./images/cnn_fp3.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和上一节唯一的区别仅仅在于，这里的输入是隐藏层来的，而不是我们输入的原始图片样本形成的矩阵。\n",
    "\n",
    "需要我们定义的CNN模型参数也和上一节一样，这里我们需要定义卷积核的个数K，卷积核子矩阵的维度F，填充大小P以及步幅S。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp4'>4. 隐藏层前向传播到池化层</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层的处理逻辑是比较简单的，我们的目的就是对输入的矩阵进行缩小概括。比如输入的若干矩阵是NxN维的，而我们的池化大小是kxk的区域，则输出的矩阵都是$\\frac{N}{k}×\\frac{N}{k}$维的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要需要我们定义的CNN模型参数是：\n",
    "\n",
    "1）池化区域的大小k\n",
    "\n",
    "2）池化的标准，一般是MAX或者Average。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp5'>5. 隐藏层前向传播到全连接层</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于全连接层就是普通的DNN模型结构，因此我们可以直接使用DNN的前向传播算法逻辑，即：\n",
    "$$a^l = \\sigma(z^l) = \\sigma(W^l a^{l-1}+b^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的激活函数一般是sigmoid或者tanh。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过了若干全连接层之后，最后的一层为Softmax输出层。   \n",
    "\n",
    "此时输出层和普通的全连接层唯一的区别是，激活函数是softmax函数。\n",
    "\n",
    "这里需要需要我们定义的CNN模型参数是：\n",
    "\n",
    "1）全连接层的激活函数\n",
    "\n",
    "2）全连接层各层神经元的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='fp6'>6. CNN前向传播算法小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上面的基础，我们现在总结下CNN的前向传播算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：\n",
    "- 1个图片样本，\n",
    "- CNN模型的层数L\n",
    "- 和所有隐藏层的类型，\n",
    "    - 对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。\n",
    "    - 对于池化层，要定义池化区域大小k和池化标准（MAX或Average），\n",
    "    - 对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　输出：CNN模型的输出$a^L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 根据输入层的填充大小P，填充原始图片的边缘，得到输入张量$a^1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2）初始化所有隐藏层的参数W,b　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3）for l=2 to L−1:   \n",
    "a) 如果第l层是卷积层,则输出为:\n",
    "$$a^l = ReLU(z^l) = ReLU(a^{l-1}*W^l +b^l)$$\n",
    "\n",
    "b) 如果第l层是池化层，则输出为$a^l=pool(a^{l−1})$, 这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。\n",
    "\n",
    "c) 如果第l层是全连接层,则输出为\n",
    "$$a^l = \\sigma(z^l) = \\sigma(W^l a^{l-1} + b^l)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)对于输出层第L层:  \n",
    "$$a^L = softmax(z^L) = softmax(W^La^{L-1}+b^L)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是CNN前向传播算法的过程总结。有了CNN前向传播算法的基础，我们后面再来理解CNN的反向传播算法就简单多了。\n",
    "\n",
    "下一篇我们来讨论CNN的反向传播算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='bp'>三、卷积神经网络CNN反向传播算法</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在卷积神经网络(CNN)前向传播算法中，我们对CNN的前向传播算法做了总结，基于CNN前向传播算法的基础，我们下面就对CNN的反向传播算法做一个总结。在阅读本文前，建议先研究DNN的反向传播算法：深度神经网络（DNN）反向传播算法(BP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp1'>1. 回顾DNN的反向传播算法</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先回顾DNN的反向传播算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在DNN中，我们是首先计算出输出层的$\\delta^L$:\n",
    "$$\\delta^L = \\frac{\\partial J(W,b)}{\\partial z^L} = \\frac{\\partial J(W,b)}{\\partial a^L} \\odot \\sigma'(z^L)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用数学归纳法，对$\\delta^{l+1}$的值一步步的向前求出第$l$层的$\\delta^l$，表达式为：\n",
    "$$\\delta^l = \\delta^{l+1}\\frac{\\partial z^{l+1}}{\\partial z^l} = (W^{l+1})\\delta^{l+1} \\odot \\sigma'(z^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了$δ^l$的表达式，从而求出W,b的梯度表达式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/w_b.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了W,b梯度表达式，就可以用梯度下降法来优化W,b,求出最终的所有W,b的值。\n",
    "\n",
    "现在我们想把同样的思想用到CNN中，很明显，CNN有些不同的地方，不能直接去套用DNN的反向传播算法的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp2'>2. CNN的反向传播算法思想</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要套用DNN的反向传播算法到CNN，有几个问题需要解决：\n",
    "\n",
    "1）池化层没有激活函数，这个问题倒比较好解决，我们可以令池化层的激活函数为σ(z)=z，即激活后就是自己本身。这样池化层激活函数的导数为1.\n",
    "\n",
    "2）池化层在前向传播的时候，对输入进行了压缩，那么我们现在需要向前反向推导$δ^{l−1}$，这个推导方法和DNN完全不同。\n",
    "\n",
    "3) 卷积层是通过张量卷积，或者说若干个矩阵卷积求和而得的当前层的输出，这和DNN很不相同，DNN的全连接层是直接进行矩阵乘法得到当前层的输出。这样在卷积层反向传播的时候，上一层的$δ^{l−1}$递推计算方法肯定有所不同。\n",
    "\n",
    "4）对于卷积层，由于W使用的运算是卷积，那么从$δ^l$推导出该层的所有卷积核的W,b的方式也不同。\n",
    "\n",
    "从上面可以看出，问题1比较好解决，但是问题2,3,4就需要好好的动一番脑筋了，而问题2,3,4也是解决CNN反向传播算法的关键所在。另外大家要注意到的是，DNN中的$a^l$,$z^l$都只是一个向量，而我们CNN中的$a^l$,$z^l$都是一个张量，这个张量是三维的，即由若干个输入的子矩阵组成。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们就针对问题2,3,4来一步步研究CNN的反向传播算法。\n",
    "\n",
    "在研究过程中，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp3'>3. 已知池化层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先解决上面的问题2，如果已知池化层的$δ^l$，推导出上一隐藏层的$δ^{l−1}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。\n",
    "\n",
    "现在我们反过来，要从缩小后的误差$δ^l$，还原前一次较大区域对应的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在反向传播时，我们首先会把$δ^l$的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把$δ^l$的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。\n",
    "\n",
    "如果是Average，则把$δ^l$的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做**upsample**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用一个例子可以很方便的表示：假设我们的池化区域大小是2x2。\n",
    "\n",
    "$δ^l$的第k个子矩阵为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/upsample1.png' width='20%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于池化区域为2x2，我们先讲$δ^l_k$做还原，即变成："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/upsample2.png' width='20%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是MAX，假设我们之前在前向传播时记录的最大值位置分别是左上，右下，右上，左下，则转换后的矩阵为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/upsample3.png' width='20%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是Average，则进行平均：转换后的矩阵为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/upsample4.png' width='20%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/upsample5.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp4'>4. 已知卷积层的$\\delta^l$，推导上一隐藏层的$\\delta^{l-1}$</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于卷积层的反向传播，我们首先回忆下卷积层的前向传播公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^l = \\sigma(z^l) =\\sigma(a^{l-1}*W^l+b^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$n_{in}$为上一隐藏层的输入子矩阵个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在DNN中，我们知道$δ^{l−1}$和$δ^l$的递推关系为：\n",
    "\n",
    "<img src='./images/cv1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，要推导出$\\delta^{l-1}$和$\\delta^l$的递推关系，必须计算$\\frac{\\partial z^l}{\\partial z^{l-1}}$的梯度表达式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到$z^l$和$z^{l-1}$的关系为：\n",
    "$$z^l = a^{l-1}*W^l + b^l = \\sigma(z^{l-1})*W^l+b^l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们有：\n",
    "\n",
    "<img src='./images/cv2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的式子其实和DNN的类似，区别在于：  \n",
    "对于含有卷积的式子求导时，**卷积核被旋转了180度**。即式子中的rot180()，翻转180度的意思是上下翻转一次，接着左右翻转一次。  \n",
    "\n",
    "在DNN中这里只是矩阵的转置。  \n",
    "那么为什么呢？  \n",
    "由于这里都是张量，直接推演参数太多了。\n",
    "我们以一个简单的例子说明为啥这里求导后卷积核要翻转。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们$l−1$层的输出$a^{l−1}$是一个3x3矩阵，第$l$层的卷积核$W_l$是一个2x2矩阵，采用1像素的步幅，则输出$z^l$是一个2x2的矩阵。我们简化$b^l$都是0,则有：\n",
    "$$a^{l-1}*W^l=z^l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们列出a,W,z的矩阵表达式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rot1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用卷积的定义，很容易得出：\n",
    "\n",
    "<img src='./images/rot2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们模拟反向求导：\n",
    "<img src='./images/rot3.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上式可以看出，对于$a^{l−1}$的梯度误差$∇a^{l−1}$，等于第$l$层的梯度误差乘以$\\frac{∂z^l}{∂a^{l−1}}$，而$\\frac{∂z^l}{∂a^{l−1}}$对应上面的例子中相关联的w的值。\n",
    "\n",
    "假设我们的z矩阵对应的反向传播误差是$δ_{11}$,$δ_{12}$,$δ_{21}$,$δ_{22}$组成的2x2矩阵，则利用上面梯度的式子和4个等式，我们可以分别写出$∇a^{l−1}$的9个标量的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如对于$a_{11}$的梯度，由于在4个等式中$a_{11}$只和$z_{11}$有乘积关系，从而我们有：\n",
    "$$∇a_{11}=δ_{11}w_{11}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$a_{12}$的梯度，由于在4个等式中$a_{12}$和$z_{12}$，$z_{11}$有乘积关系，从而我们有：\n",
    "$$∇a_{12}=δ_{11}w_{12}+δ_{12}w_{11}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的道理我们得到："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$∇a_{13}=δ_{12}w_{12}$$\n",
    "$$∇a_{21}=δ_{11}w_{21}+δ_{21}w_{11}$$\n",
    "$$∇a_{22}=δ_{11}w_{22}+δ_{12}w_{21}+δ_{21}w_{12}+δ_{22}w_{11}$$\n",
    "$$∇a_{23}=δ_{12}w_{22}+δ_{22}w_{12}$$\n",
    "$$∇a_{31}=δ_{21}w_{21}$$\n",
    "$$∇a_{32}=δ_{21}w_{22}+δ_{22}w_{21}$$\n",
    "$$∇a_{33}=δ_{22}w_{22}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这上面9个式子其实可以用一个矩阵卷积的形式表示，即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rot4.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。\n",
    "\n",
    "这个例子直观的介绍了为什么对含有卷积的式子反向传播时，卷积核要翻转180度的原因。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是卷积层的误差反向传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp5'>5. 已知卷积层的$\\delta^l$，推导该层的W,b的梯度</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，我们现在已经可以递推出每一层的梯度误差$δ^l$了，对于全连接层，可以按DNN的反向传播算法求该层W,b的梯度，而池化层并没有W,b,也不用求W,b的梯度。\n",
    "\n",
    "只有卷积层的W,b需要求出。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到卷积层z和W,b的关系为：\n",
    "$$z^l = a^{l-1}*W^l+b^l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们有：\n",
    "<img src='./images/wb1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到此时卷积核并没有反转，主要是此时是层内的求导，而不是反向传播到上一层的求导。具体过程我们可以分析一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和第4节一样的一个简化的例子，这里输入是矩阵，不是张量，那么对于第l层，某个卷积核矩阵W的导数可以表示如下：\n",
    "\n",
    "<img src='./images/wb2.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们输入a是4x4的矩阵，卷积核W是3x3的矩阵，输出z是2x2的矩阵,那么反向传播的z的梯度误差δ也是2x2的矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么根据上面的式子，我们有：\n",
    "<img src='./images/wb3.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们可以一共得到9个式子。整理成矩阵形式后可得："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/wb4.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从而可以清楚的看到这次我们为什么没有反转的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于b,则稍微有些特殊，因为$δ^l$是三维张量，而b只是一个向量，不能像DNN那样直接和$δ^l$相等。\n",
    "\n",
    "通常的做法是将$δ^l$的各个子矩阵的项分别求和，得到一个误差向量，即为b的梯度："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/wb5.png' width='30%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='bp6'>6. CNN反向传播算法总结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们总结下CNN的反向传播算法，以最基本的批量梯度下降法为例来描述反向传播算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：\n",
    "- m个图片样本，\n",
    "- CNN模型的层数L\n",
    "- 和所有隐藏层的类型，\n",
    "    - 对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。\n",
    "    - 对于池化层，要定义池化区域大小k和池化标准（MAX或Average），\n",
    "    - 对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。\n",
    "- 梯度迭代参数迭代步长α,最大迭代次数MAX与停止迭代阈值ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：CNN模型各隐藏层与输出层的W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 初始化各隐藏层与输出层的各W,b的值为一个随机值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2）for iter to 1 to MAX："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1) for i =1 to m：\n",
    "\n",
    "a) 将CNN输入$a^1$设置为$x_i$对应的张量　　　　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) for l=2 to L-1，根据下面3种情况进行前向传播算法计算：\n",
    "\n",
    "b-1) 如果当前是全连接层：则有$a^{i,l}=σ(z^{i,l})=σ(W^la^{i,l−1}+b^l)$  \n",
    "\n",
    "b-2) 如果当前是卷积层：则有$a^{i,l}=σ(z^{i,l})=σ(W^l∗a^{i,l−1}+b^l)$\n",
    "\n",
    "b-3) 如果当前是池化层：则有$a^{i,l}=pool(a^{i,l−1})$, 这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) 对于输出层第L层: $a^{i,L}=softmax(z^{i,L})=softmax(W^La^{i,L−1}+b^{L})$\n",
    "\n",
    "通过损失函数计算输出层的$δ^{i,L}$\n",
    "　　　　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) for l= L-1 to 2, 根据下面3种情况进行进行反向传播算法计算:\n",
    "\n",
    "d-1)  如果当前是全连接层：$δ^{i,l}=(W^{l+1})^Tδ^{i,l+1}⊙σ′(z^{i,l})$  \n",
    "d-2) 如果当前是卷积层：$δ^{i,l}=δ^{i,l+1}∗rot180(W^{l+1})⊙σ′(z^{i,l})$  \n",
    "d-3) 如果当前是池化层：$δ^{i,l}=upsample(δ^{i,l+1})⊙σ′(z^{i,l})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2) for l = 2 to L，根据下面2种情况更新第l层的$W^l$,$b^l$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2-1) 如果当前是全连接层：$W^l=W^l−α∑_{i=1}^mδ^{i,l}(a^{i,l−1})T$， $b^l=b^l−α∑_{i=1}^mδ^{i,l}$\n",
    "\n",
    "2-2-2) 如果当前是卷积层，对于每一个卷积核有：$W^l=W^l−α∑_{i=1}^mδ^{i,l}∗rot180(a^{i,l−1})$，$ b^l=b^l−α∑_{i=1}^m∑_{u,v}(δ^{i,l})_{u,v}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-3) 如果所有W，b的变化值都小于停止迭代阈值ϵ，则跳出迭代循环到步骤3。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3） 输出各隐藏层与输出层的线性关系系数矩阵W和偏倚向量b。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
