{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络RNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#rnn'>一、RNN与前向反向传播算法</a>\n",
    "- <a href='#rnn1'>1. RNN概述</a>\n",
    "- <a href='#rnn2'>2. RNN模型</a>\n",
    "- <a href='#rnn3'>3. RNN前向传播算法</a>\n",
    "- <a href='#rnn4'>4. RNN反向传播算法推导</a>\n",
    "- <a href='#rnn5'>5. RNN小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#lstm'>二、LSTM与前向反向算法</a>\n",
    "- <a href='#lstm1'>1. 从RNN到LSTM</a>\n",
    "- <a href='#lstm2'>2. LSTM模型结构剖析</a>\n",
    "- <a href='#lstm3'>3. LSTM之遗忘门</a>\n",
    "- <a href='#lstm4'>4. LSTM之输入门</a>\n",
    "- <a href='#lstm5'>5. LSTM之细胞状态更新</a>\n",
    "- <a href='#lstm6'>6. LSTM之输出门</a>\n",
    "- <a href='#lstm7'>7. LSTM前向传播算法</a>\n",
    "- <a href='#lstm8'>8. LSTM反向传播算法推导关键点</a>\n",
    "- <a href='#lstm9'>9. LSTM小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='rnn'>一、RNN与前向反向传播算法</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面我们讲到了DNN，以及DNN的特例CNN的模型和前向反向传播算法，这些算法都是前向反馈的，模型的输出和模型本身没有关联关系。\n",
    "\n",
    "今天我们就讨论另一类**输出和模型间有反馈的神经网络**：\n",
    "循环神经网络(Recurrent Neural Networks ，以下简称RNN)，它广泛的用于自然语言处理中的语音识别，手写书别以及机器翻译等领域。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='rnn1'>1. RNN概述</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面讲到的DNN和CNN中，训练样本的输入和输出是比较的确定的。\n",
    "\n",
    "但是有一类问题DNN和CNN不好解决，就是**训练样本输入是连续的序列，且序列的长短不一**。\n",
    "比如：\n",
    "- 基于时间的序列：\n",
    "    - 一段段连续的语音，\n",
    "    - 一段段连续的手写文字。\n",
    "\n",
    "这些序列比较长，且长度不一，比较难直接的拆分成一个个独立的样本来通过DNN/CNN进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而对于这类问题，RNN则比较的擅长。\n",
    " \n",
    "那么RNN是怎么做到的呢？   \n",
    "\n",
    "**RNN假设我们的样本是基于序列的。**   \n",
    "比如：  \n",
    "是从序列索引1到序列索引$τ$的。  \n",
    "\n",
    "对于这其中的任意序列索引号$t$,它对应的输入是对应的样本序列中的$x^{(t)}$。而模型在序列索引号$t$位置的隐藏状态$h^{(t)}$，则由$x^{(t)}$和在$t−1$位置的隐藏状态$h^{(t−1)}$共同决定。\n",
    "\n",
    "在任意序列索引号$t$，我们也有对应的模型预测输出$o^{(t)}$。通过预测输出$o^{(t)}$和训练序列真实输出$y^{(t)}$,以及损失函数$L^{(t)}$，我们就可以用DNN类似的方法来训练模型，接着用来预测测试序列中的一些位置的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='rnn2'>2. RNN模型</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN模型有比较多的变种，这里介绍最主流的RNN模型结构如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/rnn_base.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中左边是RNN模型没有按时间展开的图，如果按时间序列展开，则是上图中的右边部分。我们重点观察右边部分的图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这幅图描述了在序列索引号$t$附近RNN的模型。\n",
    "其中：\n",
    "\n",
    "1）$x^{(t)}$代表在序列索引号$t$时训练样本的输入。同样的，$x^{(t−1)}$和$x^{(t+1)}$代表在序列索引号$t−1$和$t+1$时训练样本的输入。\n",
    "\n",
    "2）$h^{(t)}$代表在序列索引号$t$时模型的隐藏状态。$h^{(t)}$由$x^{(t)}$和$h^{(t−1)}$共同决定。\n",
    "\n",
    "3）$o^{(t)}$代表在序列索引号$t$时模型的输出。$o^{(t)}$只由模型当前的隐藏状态$h^{(t)}$决定。\n",
    "\n",
    "4）$L^{(t)}$代表在序列索引号$t$时模型的损失函数。\n",
    "\n",
    "5）$y^{(t)}$代表在序列索引号$t$时训练样本序列的真实输出。\n",
    "\n",
    "6）$U,W,V$这三个矩阵是我们的模型的线性关系参数，**它在整个RNN网络中是共享的，这点和DNN很不相同。** 也正因为是共享了，它体现了RNN的模型的**“循环反馈”**的思想。　　\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='rnn3'>3. RNN前向传播算法</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上面的模型，RNN的前向传播算法就很容易得到了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任意一个序列索引号$t$，我们隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t−1)}$得到：\n",
    "$$h^{(t)}=σ(z^{(t)})=σ(Ux^{(t)}+Wh^{(t−1)}+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中σ为RNN的激活函数，一般为tanh, b为线性关系的偏倚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列索引号$t$时模型的输出$o^{(t)}$的表达式比较简单：\n",
    "$$o^{(t)}=Vh^{(t)}+c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在最终在序列索引号t时我们的预测输出为:\n",
    "$$ŷ ^{(t)}=σ(o^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常由于RNN是识别类的分类模型，所以上面这个激活函数一般是softmax。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过损失函数$L^{(t)}$，比如**对数似然损失函数**，我们可以量化模型在当前位置的损失，即$ŷ^{(t)}$和$y^{(t)}$的差距。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='rnn4'>4. RNN反向传播算法推导</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了RNN前向传播算法的基础，就容易推导出RNN反向传播算法的流程了。\n",
    "\n",
    "RNN反向传播算法的思路和DNN是一样的，即通过**梯度下降法一轮轮的迭代，得到合适的RNN模型参数U,W,V,b,c。**\n",
    "\n",
    "由于我们是**基于时间反向传播**，所以RNN的反向传播有时也叫做**BPTT(back-propagation through time)**。\n",
    "\n",
    "当然这里的BPTT和DNN也有很大的不同点，即**这里所有的U,W,V,b,c在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简化描述，这里的损失函数我们为**对数损失函数**，输出的**激活函数为softmax函数**，隐藏层的激活函数为**tanh函数**。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于RNN，由于我们在序列的每个位置都有损失函数，因此最终的损失$L$为：\n",
    "$$L=∑_{t=1}^τL^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$V$,$c$,的梯度计算是比较简单的：\n",
    "\n",
    "<img src='./images/bptt_vc.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是W,U,b的梯度计算就比较的复杂了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从RNN的模型可以看出，在反向传播时，在某一序列位置$t$的梯度损失由当前位置的输出对应的梯度损失和序列索引位置$t+1$时的梯度损失两部分共同决定。\n",
    "\n",
    "对于W在某一序列位置$t$的梯度损失需要反向传播一步步的计算。\n",
    "\n",
    "我们定义序列索引t位置的隐藏状态的梯度为：\n",
    "$$δ^{(t)}=\\frac{∂L}{∂h^{(t)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们可以像DNN一样从$δ^{(t+1)}$递推$δ^{(t)}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bptt_w.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$δ^{(τ)}$，由于它的后面没有其他的序列索引了，因此有：\n",
    "\n",
    "<img src='./images/bptt_o.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了$δ^{(t)}$,计算W,U,b就容易了，这里给出W,U,b的梯度计算表达式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/bptt_wub.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了梯度表达式不同，RNN的反向传播算法和DNN区别不大，因此这里就不再重复总结了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='rnn5'>5. RNN小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面总结了通用的RNN模型和前向反向传播算法。当然，有些RNN模型会有些不同，自然前向反向传播的公式会有些不一样，但是原理基本类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN虽然理论上可以很漂亮的解决序列数据的训练，但是它也像DNN一样有**梯度消失**时的问题，当序列很长的时候问题尤其严重。\n",
    "\n",
    "因此，上面的RNN模型一般不能直接用于应用领域。\n",
    "\n",
    "**在语音识别，手写书别以及机器翻译等NLP领域**实际应用比较广泛的是**基于RNN模型的一个特例LSTM**，下一篇我们就来讨论LSTM模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='lstm'>二、LSTM与前向反向算法</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在循环神经网络(RNN)模型与前向反向传播算法中，我们总结了对RNN模型做了总结。\n",
    "\n",
    "由于RNN也有梯度消失的问题，因此很难处理长序列的数据，大牛们对RNN做了改进，得到了RNN的特例LSTM（Long Short-Term Memory），它可以避免常规RNN的梯度消失，因此在工业界得到了广泛的应用。\n",
    "\n",
    "下面我们就对LSTM模型做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm1'>1. 从RNN到LSTM</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RNN模型里，我们讲到了RNN具有如下的结构，每个序列索引位置$t$都有一个隐藏状态$h^{(t)}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lstm1.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们略去每层都有的$o^{(t)}$,$L^{(t)}$,$y^{(t)}$，则RNN的模型可以简化成如下图的形式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lstm2.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中可以很清晰看出在隐藏状态$h^{(t)}$由$x^{(t)}$和$h^{(t−1)}$得到。\n",
    "\n",
    "得到$h^{(t)}$后一方面用于当前层的模型损失计算，另一方面用于计算下一层的$h^{(t+1)}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于RNN梯度消失的问题，大牛们对于序列索引位置$t$的隐藏结构做了改进，可以说通过一些技巧让**隐藏结构复杂了起来，来避免梯度消失的问题**，这样的特殊RNN就是我们的LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于LSTM有很多的变种，这里我们以最常见的LSTM为例讲述。LSTM的结构如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lstm3.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到LSTM的结构要比RNN的复杂的多，真佩服牛人们怎么想出来这样的结构，然后这样居然就可以解决RNN梯度消失的问题？\n",
    "\n",
    "由于LSTM怎么可以解决梯度消失是一个比较难讲的问题，我也不是很熟悉，这里就不多说，重点回到LSTM的模型本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm2'>2. LSTM模型结构剖析</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们给出了LSTM的模型结构，下面我们就一点点的剖析LSTM模型在每个序列索引位置t时刻的内部结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/lstm3.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图中可以看出，在每个序列索引位置$t$时刻向前传播的除了和RNN一样的隐藏状态$h^{(t)}$，还多了另一个隐藏状态，如图中上面的长横线。这个隐藏状态我们一般称为**细胞状态(Cell State)**，记为$C^{(t)}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示：\n",
    "\n",
    "<img src='./images/lstm4.png' width='60%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了细胞状态，LSTM图中还有了很多奇怪的结构，这些结构一般称之为**门控结构(Gate)**。\n",
    "\n",
    "LSTM在在每个序列索引位置$t$的门一般包括:\n",
    "- 遗忘门，\n",
    "- 输入门和\n",
    "- 输出门三种。\n",
    "\n",
    "下面我们就来研究上图中LSTM的遗忘门，输入门和输出门以及细胞状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a  name='lstm3'>3. LSTM之遗忘门</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗忘门（forget gate）顾名思义，是控制是否遗忘的，在LSTM中即以一定的概率控制是否遗忘上一层的隐藏细胞状态。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗忘门子结构如下图所示：\n",
    "<img src='./images/forget_gate.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中输入的有上一序列的隐藏状态$h^{(t−1)}$和本序列数据$x^{(t)}$，通过一个激活函数，一般是sigmoid，得到遗忘门的输出$f^{(t)}$。\n",
    "\n",
    "由于sigmoid的输出$f^{(t)}$在$[0,1]$之间，因此这里的**输出$f^{(t)}$代表了遗忘上一层隐藏细胞状态的概率。**\n",
    "\n",
    "用数学表达式即为：\n",
    "<img src='./images/fg.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$W_f$,$U_f$,$b_f$为线性关系的系数和偏倚，和RNN中的类似。  \n",
    "σ为sigmoid激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm4'>4. LSTM之输入门</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入门（input gate）负责处理当前序列位置的输入，它的子结构如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/input_gate.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm4'>4. LSTM之输入门</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm5'>5. LSTM之细胞状态更新</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm6'>6. LSTM之输出门</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm7'>7. LSTM前向传播算法</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm8'>8. LSTM反向传播算法推导关键点</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='lstm9'>9. LSTM小结</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
