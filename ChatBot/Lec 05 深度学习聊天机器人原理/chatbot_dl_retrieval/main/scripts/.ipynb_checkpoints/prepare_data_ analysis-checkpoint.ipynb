{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 针对prepare_data.py脚本文件的解析\n",
    "prepare_data.py用于将csv文件转换为TFRecords文件。\n",
    "\n",
    "内部的步骤清晰，是个完整的工程项目的写法。除了调试TensorFlow版本之间的变化外，还将完整的步骤的注解，在本文件内完整的说明。  \n",
    "\n",
    "具体的调用还是使用prepare_data.py即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python的内建模块**itertools**提供了非常有用的用于操作迭代对象的函数。\n",
    "<a href='https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001415616001996f6b32d80b6454caca3d33c965a07611f000'>使用参见链接</a>\n",
    "\n",
    "**functools**该模块为**高阶函数**提供支持——作用于或返回函数的函数被称为高阶函数。在该模块看来，一切可调用的对象均可视为本模块中所说的“函数”。\n",
    "<a href='https://www.cnblogs.com/Security-Darren/p/4168310.html'>使用参见链接</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: 通过tf.flags.DEFINE_xxx()设置 命令行可选参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.flags.DEFINE_xxx\n",
    "# FLAGS=tf.flags.FLAGES\n",
    "# 添加命令行的可选参数\n",
    "tf.flags.DEFINE_integer(\n",
    "    'min_word_frequency', 5, 'Minimum frequency of words in the vocabulary'\n",
    ")\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "    'max_sentence_len', 160, \"Maximum Sentence Length\"\n",
    ")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'input_dir', os.path.abspath('../../ubuntu_dataset'),\n",
    "    \"Input directory containing original CSV data files\"\n",
    ")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'output_dir', os.path.abspath('../../ubuntu_dataset'),\n",
    "    'Output directory for TFRecord files'\n",
    ")\n",
    "\n",
    "# 仅是在jupyter的时候会出现的报错信息。\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关tf.flags的知识注解，可见<a href='../../../../../TF/教程/小知识.ipynb#flags'>链接</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: 设置可选参数input_dir的路径，数据集加载路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(FLAGS.input_dir, 'train.csv')\n",
    "VALIDATION_PATH = os.path.join(FLAGS.input_dir, 'valid.csv')\n",
    "TEST_PATH = os.path.join(FLAGS.input_dir, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: 自定义分词函数\n",
    "此处是简单的以空格\" \"作为分词。如果是中文或不能通过这种方式分词的，可自定义更改相应的预处理分词方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_fn(iterator):\n",
    "    '''\n",
    "    将可迭代对象，逐个按空格分词（英文可以），返回的分词存在元组中。\n",
    "    '''\n",
    "    return (x.split(\" \") for x in iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: 将读取csv文件变成迭代器\n",
    "这样在读取过程中可以按一行一行的方式读取，并一行一行地进行数据预处理，如：分词等。\n",
    "这样可以避免内存的开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_iter(filename):\n",
    "    '''\n",
    "    将CSV文件内容变成生成器，可迭代对象。\n",
    "    Returns an iterator over a CSV file. Skips the header.\n",
    "    '''\n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "\n",
    "        # Skip the header\n",
    "        next(reader)\n",
    "\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过测试，在读取test.csv文件后，第一行（已经过滤掉了实际文件第一行的title）的数据格式为：\n",
    "\n",
    "['anyone knows why my stock oneiric exports env var \\'USERNAME\\'?  I mean what is that used for?  I know of $USER but not $USERNAME .  My precise install doesn\\'t export USERNAME __eou__ __eot__ looks like it used to be exported by lightdm, but the line had the comment \"// FIXME: Is this required?\" so I guess it isn\\'t surprising it is gone __eou__ __eot__ thanks!  How the heck did you figure that out? __eou__ __eot__ https://bugs.launchpad.net/lightdm/+bug/864109/comments/3 __eou__ __eot__ ', 'nice thanks! __eou__', 'wrong channel for it, but check efnet.org, unofficial page. __eou__', 'every time the kernel changes, you will lose video __eou__ yep __eou__', 'ok __eou__', \"!nomodeset > acer __eou__ I'm assuming it is a driver issue. __eou__ !pm > acer __eou__ i DON'T pm. ;) __eou__ OOPS SORRY FOR THE CAPS __eou__\", 'http://www.ubuntu.com/project/about-ubuntu/derivatives  (some call them derivatives, others call them flavors, same difference) __eou__', \"thx __eou__ unfortunately the program isn't installed from the repositories __eou__\", 'how can I check? By doing a recovery for testing? __eou__', 'my humble apologies __eou__', '#ubuntu-offtopic __eou__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面通过pd.read_csv的方式读取test.csv文件，对比第一行数据的内容和上面的csv的迭代器读取的内容的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context                   anyone knows why my stock oneiric exports env ...\n",
       "Ground Truth Utterance                                 nice thanks! __eou__\n",
       "Distractor_0              wrong channel for it, but check efnet.org, uno...\n",
       "Distractor_1              every time the kernel changes, you will lose v...\n",
       "Distractor_2                                                     ok __eou__\n",
       "Distractor_3              !nomodeset > acer __eou__ I'm assuming it is a...\n",
       "Distractor_4              http://www.ubuntu.com/project/about-ubuntu/der...\n",
       "Distractor_5              thx __eou__ unfortunately the program isn't in...\n",
       "Distractor_6              how can I check? By doing a recovery for testi...\n",
       "Distractor_7                                    my humble apologies __eou__\n",
       "Distractor_8                                       #ubuntu-offtopic __eou__\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../ubuntu_dataset/test.csv')\n",
    "test_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: 创建词汇表\n",
    "create_vocab的参数由input_iter和min_frequency决定。\n",
    "\n",
    "input_iter是文本内容\n",
    "min_frequency是最低词频的限制，只有超过这个min_frequency的词汇才会被加入到词汇表中，降低了词汇表的大小。\n",
    "\n",
    "由于create_vocab中采用的构建词汇表的方式已经即将被移除，所以方程需要更新到新方法的实现。\n",
    "下面先介绍之前的基于tf.contrib.learn.preprocessing.VocabularyProcessor的实现。  \n",
    "再介绍tensorflow/transform or tf.data的实现方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(input_iter, min_frequency):\n",
    "    '''\n",
    "    创建词汇表，最低词频由min_frequency确定\n",
    "    Creates and returns a VocabularyProcessor object with the vocabulary\n",
    "  for the input iterator.\n",
    "    '''\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "        FLAGS.max_sentence_len,\n",
    "        min_frequency=min_frequency,\n",
    "        tokenizer_fn=tokenizer_fn\n",
    "    )\n",
    "    vocab_processor.fit(input_iter)\n",
    "    return vocab_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.contrib.learn.preprocessing.VocabularyProcessor (max_document_length, min_frequency=0, vocabulary=None, tokenizer_fn=None)\n",
    "\n",
    "    - 参数：\n",
    "\n",
    "        - max_document_length: 文档的最大长度。如果文本的长度大于最大长度，那么它会被剪切，反之则用0填充。 \n",
    "        - min_frequency: 词频的最小值，出现次数小于最小词频则不会被收录到词表中。 \n",
    "        - vocabulary: CategoricalVocabulary 对象。 \n",
    "        - tokenizer_fn：分词函数\n",
    "\n",
    "**重要！！！！ tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Please use tensorflow/transform or tf.data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-cf96e9102862>:8: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/zoe/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/zoe/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor at 0xb1c72f8d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "max_document_length =  4\n",
    "x_text  = ['I love you', 'me too']\n",
    "\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "    max_document_length)\n",
    "vocab_processor.fit(x_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4 5 0]\n"
     ]
    }
   ],
   "source": [
    "for i in vocab_processor.transform([' i me too']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../ubuntu_dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Context', 'Utterance', 'Label'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocabulary...\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor object at 0x1c2e57add8>\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating vocabulary...\")\n",
    "input_iter = create_csv_iter(TRAIN_PATH)\n",
    "# train.csv文件的列为['Context', 'Utterance', 'Label'\n",
    "input_iter = (x[0] + \" \" + x[1] for x in input_iter)\n",
    "vocab = create_vocab(input_iter, min_frequency=FLAGS.min_word_frequency)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************这是开始用tf.data API的分割线*************************\n"
     ]
    }
   ],
   "source": [
    "print('这是开始用tf.data API的分割线'.center(70,'*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未找到相关范例，后续自己撸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 自定义句子向量化函数\n",
    "将矩阵一句句地输入函数，对已经生成的vocab处理器，通过vocab.transform方法，对valid和test数据进行编码。    \n",
    "vocab.transform生成的对象是一个迭代器，所以无法像sklearn中的TfidfVectorizer等方法一样直接toarray()的方式获取，而是需要用next(generator).tolist()的方式去拼成列表。   \n",
    "如需转换为ndarray则通过numpy操作即可。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(sequence, vocab_processor):\n",
    "    '''\n",
    "    Maps a single sentence into integer vocabulary. \n",
    "    Returns a python array.\n",
    "    '''\n",
    "    return next(vocab_processor.transform([sequence])).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_sequence_feature(fl, sentence, sentence_len, vocab):\n",
    "    '''\n",
    "    Writes a sentence to FeatureList protocol buffer.\n",
    "    '''\n",
    "    sentence_transformed = transform_sentence(sentence, vocab)\n",
    "    for word_id in sentence_transformed:\n",
    "        fl.feature.add().int64_list.value.extend([word_id])\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 1029,\n",
       " 827,\n",
       " 8849,\n",
       " 9337,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
