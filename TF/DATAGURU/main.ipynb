{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一周\n",
    "截图安装anaconda和tensorflow的过程，以及安装成功的结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装：\n",
    "- CPU 版本：\n",
    "pip install tensorflow\n",
    "\n",
    "- GPU 版本：\n",
    "pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新：\n",
    "pip uninstall tensorflow   \n",
    "pip install tensorflow   \n",
    "\n",
    "pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二周\n",
    "把视频中的程序都自己写一遍。把程序和运行结果截图上传。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 的特点：\n",
    "- 使用**图 (graph)** 来表示计算任务.\n",
    "- 在被称之为 **会话 (Session) 的上下文 (context) **中执行图.\n",
    "- 使用**张量tensor** 表示数据.\n",
    "- 通过 **变量 (Variable) **维护状态.\n",
    "- 使用 **feed** 和 **fetch** 可以**为任意的操作(arbitrary operation) 赋值**或者从其中**获取数据**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 是一个编程系统, 使用**图graphs**来表示计算任务。\n",
    "\n",
    "图grahs中的节点被称之为 **op (operation 的缩写)**。  \n",
    "一个 op 获得 0 个或多个Tensor, 执行计算, 产生 0 个或多个 Tensor. \n",
    "\n",
    "每个 Tensor 是一个类型化的多维数组。\n",
    "例如：  \n",
    "你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是\n",
    "[batch, height, width, channels]\n",
    "\n",
    "图必须在会话Session里被启动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow结构：图graphs\n",
    "\n",
    "<img src='./images/graphs1.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三周\n",
    "优化mnist分类的程序，把测试准确率提升到95%。把程序和运行结果截图上传。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节课主要讲解线性回归和分类的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有MNIST手写数据集的实现。  \n",
    "\n",
    "### MNIST数据集\n",
    "官网：<a href='http://yann.lecun.com/exdb/mnist/'>Yann leCun's website   </a>\n",
    "\n",
    "下载下来的数据集被分成两部分：  \n",
    "- 6w行的训练数据集(mnist.train)\n",
    "- 1w行的测试数据集(mnist.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/mnist.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/mnist_image_matrix.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/mnist_image_ys.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络构建："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/mnist_nn.png' width='20%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节要搭建的这个mnist的神经网络只有两层，第一层是输入层，有784个神经元（因为输入数据有784个feature），第二层是输出层，有10个神经元（因为输出的数字是对应了0-9的十个数字的one-hot编码）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax函数："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道MNIST的结果是0-9，我们的模型可能推测出一张图片是数字9的概率是80%，是数字8的概率是10%，然后其他数字的概率更小，总体概率加起来等于1。   \n",
    "\n",
    "这是一个使用Softmax回归模型的经典案例。  \n",
    "Softmax模型可以用来给不同的对象分配概率。\n",
    "$$softmax(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/softmax_example1.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四周\n",
    "继续优化手写数字分类程序，把Testing Accuracy提升到98%以上。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二次代价函数（quadratic cost）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C=\\frac{1}{2n} \\sum_x ||y(x)-a^L(x)||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$C$表示代价，$x$表示样本，$y$表示真实值，$a$表示输出值，n表示样本的总数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了简单期间，同样一个样本为例进行说明，此时二次代价函数为：\n",
    "$$C=\\frac{(y-a)^2}{2}$$\n",
    "\n",
    "其中：\n",
    "- $a=\\sigma(z)$，$z = \\sum W_j * X_j +b$\n",
    "- $\\sigma()$是激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们使用**梯度下降法（Gradient Descent）**来调整权值参数的大小，权值w和偏置b的梯度推导公式如下：\n",
    "$$\\frac{\\partial C}{\\partial w} = (a-y)\\sigma'(z)x$$\n",
    "$$\\frac{\\partial C}{\\partial b} = (a-y)\\sigma'(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$z$表示神经元的输入，$\\sigma$表示激活函数。   \n",
    "**w和b的梯度跟激活函数的梯度成正比**，激活函数的梯度越大，w和b的大小调整就越快，训练收敛得就越快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/sigmoid.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上图的B点处，其导数几乎等于0，那么w和b就基本上没有变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们目标是收敛到1。   \n",
    "- A点为0.82，离目标比较远，梯度比较大，权值调整比较大。\n",
    "- B点为0.98，离目标比较近，梯度比较小，权值调整比较小，调整方案合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们目标是收敛到0。   \n",
    "- A点为0.82，离目标比较近，梯度比较大，权值调整比较大。\n",
    "- B点为0.98，离目标比较远，梯度比较小，权值调整比较小，调整方案不合理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵代价函数(cross-entropy):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换一个思路，我们不改变激活函数，而是改变代价函数，改用交叉熵代缴函数：\n",
    "$$C= -\\frac{1}{n}\\sum_{x}[y\\ln a +(1-y)\\ln(1-a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$C$表示代价，$x$表示样本，$y$表示实际值，$a$表示输出值，$n$表示样本总数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a=\\sigma(z)$，$z=\\sum W_j*X_j +b$     \n",
    "$\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，w和b的梯度分别是：\n",
    "$$\\frac{\\partial C}{\\partial w_j} = -\\frac{1}{n}\\sum_x (\\frac{y}{\\sigma(z)} - \\frac{1-y}{1-\\sigma(z)})\\frac{\\partial \\sigma}{\\partial w_j}$$\n",
    "$$=-\\frac{1}{n}\\sum_{x} (\\frac{y}{\\sigma(z)}-\\frac{1-y}{1-\\sigma(z)})\\sigma'(z)x_j$$\n",
    "$$=\\frac{1}{n}\\sum_x \\frac{\\sigma'(z)x_j}{\\sigma(z)(1-\\sigma(z))}(\\sigma(z)-y)$$\n",
    "$$=\\frac{1}{n}\\sum_x x_j(\\sigma(z)-y)$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = \\frac{1}{n}\\sum_x (\\sigma(z)-y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C}{\\partial w_j} =\\frac{1}{n}\\sum_x x_j(\\sigma(z)-y)$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = \\frac{1}{n}\\sum_x (\\sigma(z)-y)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过观察w和b的梯度公式，可以发现：\n",
    "- 权值和偏置值的调整与$\\sigma'(z)$无关，另外，梯度公式中的$\\sigma(z)-y$表示输出值与实际值的误差。 所以当误差越大时，梯度就越大，参数w和b的调整就越快，训练的速度也就越快。\n",
    "- 如果输出神经元是线性的，那么二次代价函数就是一种合适的选择。\n",
    "- 如果输出神经元是Sigmoid函数，那么比较适合用交叉熵代价函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数似然代价函数(log-likelihood cost):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数似然函数常用来作为softmax回归的代价函数。\n",
    "\n",
    "如果输出层神经元是sigmoid函数，可以采用交叉熵代价函数。\n",
    "\n",
    "而深度学习中更普遍的做法是将softmax作为最后一层，此时常用的代价函数是对数似然代价函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对数似然函数与softmax的组合** 和 **交叉熵与sigmoid函数**的组合非常相似。\n",
    "\n",
    "对数似然代价函数在二分类时，可以化简为**交叉熵**代价函数的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Tensorflow中用：\n",
    "- tf.nn.sigmoid_cross_entropy_with_logits()来表示跟sigmoid搭配使用的交叉熵；\n",
    "- tf.nn.softmax_cross_entropy_with_logits()来表示跟softmax搭配使用的交叉熵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处的理论部分如需再回忆和补充：\n",
    "https://www.cnblogs.com/pinard/p/6437495.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/fit.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/fiting.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 防止过拟合："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/overfitting.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考理论整理页面：https://www.cnblogs.com/pinard/p/6472666.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器Optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.train.GradientDescentOptimizer\n",
    "- tf.train.AdadeltaOptimizer\n",
    "- tf.train.AdagradOptimizer\n",
    "- tf.train.AdagradDAOptimizer\n",
    "- tf.train.MomentumOptimizer\n",
    "- tf.train.AdamOptimizer\n",
    "- tf.train.FtrlOptimizer\n",
    "- tf.train.ProximalGradientDescentOptimizer\n",
    "- tf.train.ProximalAdagradOptimizer\n",
    "- tf.train.RMSPropOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法参考页面：\n",
    "https://www.cnblogs.com/pinard/p/5970503.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种优化器对比：\n",
    "\n",
    "标准梯度下降法：   \n",
    "- 标准梯度下降法先计算所有样本汇总误差，然后根据总误差来更新权值\n",
    "\n",
    "随机梯度下降法：\n",
    "- 随机梯度下降法，随机抽取一个样本来计算误差，然后更新权值\n",
    "\n",
    "批量梯度下降法：\n",
    "- 批量梯度下降算是一种折中的方案，从总样本中选取一个批次（比如：一共有10000个样本，随机选取100个样本作为一个batch），然后计算这个batch的总误差，根据总误差来更新权值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/gradient_descent.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $W$：要训练的参数\n",
    "- $J(W)$：代价函数\n",
    "- $\\nabla_w J(W)$：代价函数的梯度\n",
    "- $\\eta$：学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD:\n",
    "$$W=W-\\eta \\cdot \\nabla_W J(W;x^{(i)};y^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum:\n",
    "$gamma$：动力，通常设置为0.9\n",
    "$$V_t = \\gamma V_{t-1} + \\eta \\nabla_W J(W)$$\n",
    "$$W = W-V_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum的迭代公式中，除了梯度乘以学习率，还有$\\gamma V_{t-1}$，是“动力项”。\n",
    "\n",
    "当前权值的改变会受到上一次权值改变的影响，类似于小球向下滚动的时候带上了惯性。  \n",
    "这样可以加快小球的向下的速度。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAG( Nesterov accelarated gradient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_t = \\gamma V_{t-1} + \\eta \\nabla_W J(W-\\gamma V_{t-1})$$\n",
    "$$W = W-V_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAG在TensorFlow中跟Momentum合并在同一个函数tf.train.MomentumOptimizer中，可以通过参数配置启动。\n",
    "\n",
    "在Momentun中，小球会盲目地跟从下坡的梯度，容易发生错误，所以我们需要一个更聪明的小球，这个小球提前知道它要去哪里，它还要知道走到坡底的时候速度慢下来而不是又冲上另一个坡。 \n",
    "\n",
    "$\\gamma V_{t-1}$会用来修改W的值，计算$W-\\gamma V_{t-1}$可以表示小球下一个位置大概在哪里。从而，我们可以提前计算下一个位置的梯度，然后使用到当前位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g_{t,i}=\\nabla_W J(W_i)$$\n",
    "\n",
    "$$W_{t+1} = W_t -\\frac{\\eta}{\\sqrt{\\sum_{t'=1}^t (g_{t',i})^2 + \\epsilon}} \\odot g_t$$\n",
    "\n",
    "- i： 表示第i个分类\n",
    "- t：表示出现次数\n",
    "- $\\epsilon$：的作用是避免分母为0，取值一般为1e-8\n",
    "- $\\eta$：取值一般为0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它是基于SGD的一种算法，它的核心思想是对比较常见的数据给予它比较小的学习率去调整参数，对于比较罕见的数据给予它比较大的学习率去调整参数。   \n",
    "\n",
    "它很适合应用于数据稀疏的数据集（比如一个图片数据集，有10000张狗的照片，10000张猫的照片，只有100张大象的图片）。\n",
    "\n",
    "Adagrad主要的优势在于不需要人为的调节学习率，它可以自动调节。  \n",
    "\n",
    "它的缺点在于，随着迭代次数的增多，学习率也会越来越低，最终会趋向于0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS(Root Mean Square) 是 均方根 的缩写。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g_t = \\nabla_W J(W)$$\n",
    "\n",
    "$$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g^2_t$$\n",
    "\n",
    "$$W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\odot g_t$$\n",
    "\n",
    "- $\\gamma$：动力，通常设置为0.9\n",
    "- $\\eta$：学习率，取值一般为0.001\n",
    "- $E[g^2]_t$：表示前t次的梯度平方的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop借鉴了一些Adagrad的思想，不过这里RMSprop只用到了前（t-1）次梯度平方的平均值加上当前梯度的平方的和的开平方作为学习率的分母。  \n",
    "\n",
    "这样RMSprop不会出现学习率越来越低的问题，而且也能自己调节学习率，并且可以有一个比较好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adadelta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g_t = \\nabla_W J(W)$$\n",
    "\n",
    "$$\\delta W_t = - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\odot g_t$$\n",
    "\n",
    "$$\\delta W_t = - \\frac{\\eta}{RMS[g]_t} \\odot g_t$$\n",
    "\n",
    "$$W_{t+1} = W_t - \\frac{RMS[\\delta W]_{t-1}}{RMS[g]_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Adadelta，我们甚至不需要设置一个默认学习率，在Adadelta不需要使用学习率也可以达到一个非常好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "\n",
    "$$V_t = \\beta_2 V_{t-1} + (1-\\beta_2)g^2_t$$\n",
    "\n",
    "$$\\hat m_t = \\frac{m_t}{1-\\beta_1^t}$$\n",
    "\n",
    "$$\\hat V_t = \\frac{V_t}{1-\\beta_2^t}$$\n",
    "\n",
    "$$W_{t+1} = W_t -\\frac{\\eta}{\\sqrt{\\hat v_t - \\epsilon}} \\hat m_t$$\n",
    "\n",
    "- $\\beta_1$：一般取值为0.9\n",
    "- $\\beta_2$：一般取值为0.999\n",
    "- $\\epsilon$：避免分母为0，一般取值为$10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "就像Adadelta和RMSprop一样，Adam会存储之前衰减的平方梯度，同时它也会保存之前衰减的梯度。  \n",
    "\n",
    "经过一些处理之后再使用类似Adadelta和RMSprop的方式更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五周\n",
    "根据课程代码，自己动手使用tensorboard绘制网络结构，各种参数的曲线，以及模型运行的可视化。截图上传。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六周\n",
    "1.完成卷积神经网络的程序。\n",
    "2.定义卷积神经网络的结构，训练网络，记录accuracy和loss值的变化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七周\n",
    "1.解释outputs和final_state有多少个维度，每个维度是代表什么。\n",
    "2.根据ppt中的公式，解释lstm网络的运行。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第八周\n",
    "讨论inception模型的特点和优点。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第九周\n",
    "自己收集图片，完成图片分类模型。整个过程截图上传。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十周\n",
    "使用课程中提到的验证码识别方法一来训练验证码模型，并测试。上传程序和结果。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十一周\n",
    "使用训练好的Word2vec初始化词汇表，用于文本分类。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十二周\n",
    "考虑一个人工智能的产品。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
