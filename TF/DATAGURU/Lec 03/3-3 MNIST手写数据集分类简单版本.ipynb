{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层版本：仅输入层和输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.8317\n",
      "Iter 1,Testing Accuracy 0.871\n",
      "Iter 2,Testing Accuracy 0.8811\n",
      "Iter 3,Testing Accuracy 0.8878\n",
      "Iter 4,Testing Accuracy 0.8936\n",
      "Iter 5,Testing Accuracy 0.8965\n",
      "Iter 6,Testing Accuracy 0.8996\n",
      "Iter 7,Testing Accuracy 0.9023\n",
      "Iter 8,Testing Accuracy 0.9035\n",
      "Iter 9,Testing Accuracy 0.9047\n",
      "Iter 10,Testing Accuracy 0.9063\n",
      "Iter 11,Testing Accuracy 0.9076\n",
      "Iter 12,Testing Accuracy 0.9084\n",
      "Iter 13,Testing Accuracy 0.9095\n",
      "Iter 14,Testing Accuracy 0.9099\n",
      "Iter 15,Testing Accuracy 0.9111\n",
      "Iter 16,Testing Accuracy 0.9122\n",
      "Iter 17,Testing Accuracy 0.9127\n",
      "Iter 18,Testing Accuracy 0.9131\n",
      "Iter 19,Testing Accuracy 0.9134\n",
      "Iter 20,Testing Accuracy 0.9137\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(os.path.join(os.getcwd(), 'MNIST_data'), one_hot=True)\n",
    "\n",
    "# 定义每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size  # 55000//100=550\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(prediction - y))\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在一个布尔型的列表中\n",
    "# 正确与否的True or False的列表\n",
    "# tf.argmax返回一维张量中最大的值所在的索引位置\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "\n",
    "# 求准确率\n",
    "# 通过tf.cast(x, dtype)将布尔型转换为浮点型\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化空间：\n",
    "\n",
    "- 神经网络结构\n",
    "    - 隐层的神经元大小\n",
    "    - 隐层数量\n",
    "    - 激活函数\n",
    "    \n",
    "- 二次代价函数：\n",
    "    - MSE\n",
    "    - 交叉熵\n",
    "    \n",
    "- 迭代优化器：梯度下降法，最小二乘法，牛顿法，拟牛顿法\n",
    "     - batch_size\n",
    "     - learning_rate\n",
    "     - epoches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面主要调整了：\n",
    "- 第一次调整：将二次代价函数改为了交叉熵代价函数\n",
    " >二次代价函数\n",
    " >loss = tf.reduce_mean(tf.square(prediction - y))\n",
    "#### 使用交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "**注意：要使用tf.reduce_mean(）方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较迭代的效果，使用交叉熵的结果收敛更快，效果更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.8249\n",
      "Iter 1,Testing Accuracy 0.8868\n",
      "Iter 2,Testing Accuracy 0.9021\n",
      "Iter 3,Testing Accuracy 0.905\n",
      "Iter 4,Testing Accuracy 0.9086\n",
      "Iter 5,Testing Accuracy 0.9098\n",
      "Iter 6,Testing Accuracy 0.912\n",
      "Iter 7,Testing Accuracy 0.9127\n",
      "Iter 8,Testing Accuracy 0.9145\n",
      "Iter 9,Testing Accuracy 0.9165\n",
      "Iter 10,Testing Accuracy 0.9167\n",
      "Iter 11,Testing Accuracy 0.9193\n",
      "Iter 12,Testing Accuracy 0.9185\n",
      "Iter 13,Testing Accuracy 0.9204\n",
      "Iter 14,Testing Accuracy 0.9197\n",
      "Iter 15,Testing Accuracy 0.9206\n",
      "Iter 16,Testing Accuracy 0.9203\n",
      "Iter 17,Testing Accuracy 0.921\n",
      "Iter 18,Testing Accuracy 0.9218\n",
      "Iter 19,Testing Accuracy 0.9217\n",
      "Iter 20,Testing Accuracy 0.9218\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(os.path.join(os.getcwd(), 'MNIST_data'), one_hot=True)\n",
    "\n",
    "# 定义每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size  # 55000//100=550\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(prediction - y))\n",
    "# 使用交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在一个布尔型的列表中\n",
    "# 正确与否的True or False的列表\n",
    "# tf.argmax返回一维张量中最大的值所在的索引位置\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "\n",
    "# 求准确率\n",
    "# 通过tf.cast(x, dtype)将布尔型转换为浮点型\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加一层隐藏层\n",
    "256个神经元(tanh)，21轮，100batch, 0.2 learning rate ==> 0.11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cde22362475c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 二次代价函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# loss = tf.reduce_mean(tf.square(prediction - y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# 使用梯度下降法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               instructions)\n\u001b[0;32m--> 306\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    308\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1969\u001b[0m   \"\"\"\n\u001b[1;32m   1970\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel, labels,\n\u001b[0;32m-> 1971\u001b[0;31m                     logits)\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m   with ops.name_scope(name, \"softmax_cross_entropy_with_logits_sg\",\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1781\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1783\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1784\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(os.path.join(os.getcwd(), 'MNIST_data'), one_hot=True)\n",
    "\n",
    "# 定义每个批次的大小\n",
    "batch_size = 100\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size  # 55000//100=550\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 隐藏层\n",
    "W_L1 = tf.Variable(tf.zeros([784, 128]))\n",
    "b_L1 = tf.Variable(tf.zeros([1, 128]))\n",
    "L1_output = tf.nn.tanh(tf.matmul(x, W_L1) + b_L1)\n",
    "\n",
    "# 输出层\n",
    "W_L2 = tf.Variable(tf.zeros([128, 10]))\n",
    "b_L2 = tf.Variable(tf.zeros([1, 10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(L1_output, W_L2) + b_L2)\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(prediction - y))\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(prediction, y)\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在一个布尔型的列表中\n",
    "# 正确与否的True or False的列表\n",
    "# tf.argmax返回一维张量中最大的值所在的索引位置\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "\n",
    "# 求准确率\n",
    "# 通过tf.cast(x, dtype)将布尔型转换为浮点型\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(51):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if epoch % 10 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改了batch_size 由100降为50.\n",
    "epoches由21添加到了101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(os.path.join(os.getcwd(), 'MNIST_data'), one_hot=True)\n",
    "\n",
    "# 定义每个批次的大小\n",
    "batch_size = 50\n",
    "# 计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size  # 55000//100=550\n",
    "\n",
    "# 定义两个placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(prediction - y))\n",
    "# 使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 结果存放在一个布尔型的列表中\n",
    "# 正确与否的True or False的列表\n",
    "# tf.argmax返回一维张量中最大的值所在的索引位置\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
    "\n",
    "# 求准确率\n",
    "# 通过tf.cast(x, dtype)将布尔型转换为浮点型\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(101):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        \n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.argmax就是返回最大的那个数值所在的下标。 \n",
    "\n",
    "tf.argmax(array, 1)和tf.argmax(array, 0)有啥区别呢？    \n",
    "    这里面就涉及到一个概念：axis。   \n",
    "    上面例子中的1和0就是axis。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
    "\n",
    "print(test)\n",
    "print(np.argmax(test, 0))\n",
    "print(np.argmax(test, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.cast(x, dtype, name=None)    \n",
    "将x的数据格式转化成dtype.   \n",
    "例如，原来x的数据格式是bool，那么将其转化成float以后，就能够将其转化成0和1的序列。  \n",
    "反之也可以。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable([1,0,0,1,1])\n",
    "b = tf.cast(a, dtype=tf.bool)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(b))\n",
    "    #[ True False False  True  True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/zoe/Documents/GitHub/July-NLP/TF/DATAGURU/Lec 03/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.8693\n",
      "Iter 50,Testing Accuracy 0.9258\n",
      "Iter 100,Testing Accuracy 0.9286\n",
      "Iter 150,Testing Accuracy 0.9299\n",
      "Iter 200,Testing Accuracy 0.9306\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os \n",
    "\n",
    "mnist = input_data.read_data_sets(os.path.join(os.getcwd(), 'MNIST_data'), one_hot=True)\n",
    "\n",
    "batch_size = 50 \n",
    "n_batch = mnist.train.num_examples // batch_size \n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(prediction -y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y,1), tf.argmax(prediction,1)), tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(201):\n",
    "        for _ in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        if epoch % 50 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
