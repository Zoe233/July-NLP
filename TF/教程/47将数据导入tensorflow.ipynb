{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将数据导入TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：将数据导入到 TensorFlow 程序的首选方法是使用数据集 API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外还有三种方法可以将数据导入到 TensorFlow 程序中：\n",
    "\n",
    "- <a href='#feeding'>Feeding</a>：Python的代码在运行每个步骤时提供数据。\n",
    "- <a href='#load_file'>从文件读取</a>：输入管道从 TensorFlow 图的开始处读取文件中的数据。\n",
    "- <a href='#pre_load'>预加载数据</a>：TensorFlow 图中的常量或变量保存所有数据（对于小型数据集）。\n",
    "\n",
    "1.一次性从内存中读取数据到矩阵中，直接输入；  \n",
    "2.从文件中边读边输入，而且已经给设计好了多线程读写模型；  \n",
    "3.把网络或者内存中的数据转化为tensorflow的专用格式tfRecord,存文件后再读取。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='feeding'>Feeding</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 的 feed 机制允许您在计算图中向任何张量注入数据。因此， python 计算可以直接将数据导入到图中。\n",
    "\n",
    "通过 feed_dict 参数向启动计算的 run() 或 eval () 调用提供 feed 数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注意：“Feeding” 是将数据传送到 TensorFlow 程序的最有效的方式，只能用于小型实验和调试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Session() as sess:   \n",
    "    input = tf.placeholder(tf.float32)   \n",
    "    classifier = ...   \n",
    "    print(classifier.eval(feed_dict={input: my_python_preprocessing_fn()}))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然可以使用 Feed 数据（包括变量和常量）替换任何 Tensor，但最佳做法是使用 tf.placeholder 节点。\n",
    "\n",
    "placeholder（占位符）只是作为 feed 的目标存在。  \n",
    "它未初始化，不包含任何数据如果占位符在没有 Feed 的情况下执行，则会产生错误，因此您不会忘记将其遗忘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 tensorflow/examples/tutorials/mnist/fully_connected_feed.py 中可以找到在 MNIST 数据上使用占位符和 Feeding 训练的示例, 并在 MNIST 教程中进行了说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.multiply(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([output], feed_dict={input1:[7.], input2:[2.] }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='load_file'>从文件导入</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从文件导入记录的典型管道有以下几个阶段：\n",
    "- 文件名列表\n",
    "- 可选文件名洗牌\n",
    "- 可选时期限制\n",
    "- 文件名队列\n",
    "- 用于文件格式的读取器\n",
    "- 读者用于读取记录的解码器\n",
    "- 可选预处理\n",
    "- 示例队列\n",
    "\n",
    "> 注意：本节讨论使用基于队列的API实现输入管道，该 API 可以被 \\${\\$datasets$Dataset API\\} 完整地替换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件名，shuffling 和 epoch 限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于文件名列表，请使用常量字符串张量。\n",
    "\n",
    "如：\n",
    "- [\"file0\", \"file1\"]或[(\"file%d\" % i) for i in range(2)]\n",
    "- 或函数：tf.train.match_filenames_once。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文件名列表传递给 tf.train.string_input_producer 函数。  \n",
    "\n",
    "string_input_producer 创建一个 FIFO 队列，用于保存文件名，直到读取器需要它们为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string_input_producer 有选择的 shuffling 和设置一个最大的 epoch 数。\n",
    "\n",
    "队列运行程序为每个 epoch 将文件名的整个列表添加到队列中一次，如果 shuffling = True，则在一个 epoch 中重新排列文件名。此过程提供了一个统一的文件取样，以便相对于彼此不会对示例进行低估或过度采样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "队列运行程序在与从队列中抽取文件名的读取器分开的线程中工作，因此，shuffling 和  enqueuing 进程不会阻止读取器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文件格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择与您的输入文件格式相匹配的读取器，并将文件名队列传递给读取器的读取方法。\n",
    "\n",
    "read 方法输出一个标识文件和记录的密钥 (如果有一些奇怪的记录，则对调试有用) 和一个标量字符串值。使用一个 (或多个) 解码器和转换 ops 将此字符串解码为构成示例的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. CSV文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若要以逗号分隔值 (CSV) 格式读取文本文件, 请使用 tf.TextLineReader 与 tf.decode_csv 操作。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'col1' [b'col1' b'col2' b'col3']\n",
      "b'1' [b'1' b'1' b'0']\n",
      "b'2' [b'2' b'4' b'0.602059991']\n",
      "b'3' [b'3' b'9' b'0.954242509']\n",
      "b'4' [b'4' b'16' b'1.204119983']\n",
      "b'5' [b'5' b'25' b'1.397940009']\n",
      "b'6' [b'6' b'36' b'1.556302501']\n",
      "b'7' [b'7' b'49' b'1.69019608']\n",
      "b'8' [b'8' b'64' b'1.806179974']\n",
      "b'9' [b'9' b'81' b'1.908485019']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# string_input_producer 创建一个 FIFO 队列，用于保存文件名，直到读取器需要它们为止。\n",
    "filename_queue = tf.train.string_input_producer(['./tf_file.csv'])\n",
    "\n",
    "# 使用 tf.TextLineReader 读取csv文件\n",
    "# 每次一行\n",
    "reader = tf.TextLineReader()\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "# 指定数据类型：record_defaults里面的数据类型决定了读取的数据类型，而且必须是list形式\n",
    "# 指定默认值：如果读取的值为空，则按record_defaults中对应的值作为 默认值\n",
    "record_defaults = [['1.0'], ['1.0'], ['1.0']]\n",
    "\n",
    "# 解析出的每一个属性都是rank为0的标量\n",
    "col1, col2, col3 = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "# tf.stack是拼接，此处是上下拼接\n",
    "features = tf.stack([col1, col2, col3])\n",
    "\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    # 线程协调器\n",
    "    coord = tf.train.Coordinator()\n",
    "    # 启动线程\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for i in range(10):\n",
    "        # 提取一个instance\n",
    "        example, label= sess.run([features, col1])\n",
    "\n",
    "        print(label, example)\n",
    "\n",
    "    #循环结束后，请求关闭所有线程\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次读取的执行都从文件中读取一行。   \n",
    "然后，decode_csv 操作将结果解析为张量列表。   \n",
    "该 record_defaults 参数确定生成的张量的类型，并设置在输入字符串中缺少值时要使用的默认值。\n",
    "\n",
    "在调用 run 或 eval 执行读取之前，必须调用 tf.train.start_queue_runners 来填充队列。否则，读取将在等待队列中的文件名时阻止。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 固定长度记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要读取每条记录是固定字节数的二进制文件, 请使用 tf.FixedLengthRecordReader 与 tf. decode_raw 操作。decode_raw 操作从一个字符串转换为 uint8 张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，CIFAR-10 数据集使用一种文件格式，其中每个记录使用固定的字节数表示：1 字节的标签，后跟3072字节的图像数据。一旦你有一个 uint8 张量，标准操作可以分割出每一块和并根据需要重新格式化。 \n",
    "对于 CIFAR-10，您可以在 tensorflow_models/tutorials/image/cifar10/cifar10_input.py 中了解如何进行阅读和解码，并在本教程中介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 标准Tensorflow格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种方法是将您拥有的任何数据转换为受支持的格式。   \n",
    "这种方法**使混合和匹配数据集和网络体系结构**变得更加容易。\n",
    "\n",
    "TensorFlow 的推荐格式是包含 tf.train.Example 协议缓冲区 (包含作为字段的功能) 的 **TFRecords 文件**。\n",
    "\n",
    "您编写了一个小程序来获取您的数据，将它放在一个示例协议缓冲区中，将协议缓冲区序列化为一个字符串，然后使用 tf. python_io. TFRecordWriter 将该字符串写入 TFRecords 文件。\n",
    "\n",
    "例如，tensorflow/examples/how_tos/reading_data/convert_to_records.py 将 MNIST 数据转换为此格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要读取 TFRecords 的文件，请使用 tf.TFRecordReader 与 tf. parse_single_example 解码器。\n",
    "\n",
    "parse_single_example 操作将示例协议缓冲区解码为张量。  \n",
    "使用 convert_to_records 生成的数据的 MNIST 示例可以在 tensorflow/examples/how_tos/reading_data/fully_connected_reader.py 中找到。\n",
    "\n",
    "您可以与 fully_connected_feed 版本进行比较。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，您可以对所需的这些示例进行任何预处理。这将是任何不依赖于训练参数的处理。\n",
    "\n",
    "示例包括数据正常化、选择随机切片、增加噪声或失真等。\n",
    "\n",
    "有关示例，请参见 tensorflow_models/tutorials/image/cifar10/cifar10_input.py。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 批处理\n",
    "在管道的最后，我们使用另一个队列来为训练，评估或推断一起批处理示例。   \n",
    "为此，我们使用一个随机化的示例顺序的队列：tf.train.shuffle_batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def read_my_file_format(filename_queue):\n",
    "    reader = tf.SomeReader()\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    example, label = tf.some_decoder(record_string)\n",
    "    processed_example = some_processing(example)\n",
    "    return processed_example, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, num_epochs=True):\n",
    "    filename_queue = tf.train.string_input_producer(filenames, \n",
    "                                                    num_epochs=num_epochs, shuffle=True)\n",
    "    example, label = read_my_file_format(filename_queue)\n",
    "    \n",
    "    # min_after_dequeue defines how big a buffer we will randomly sample\n",
    "    #   from -- bigger means better shuffling but slower start up and more memory used.\n",
    "  # capacity must be larger than min_after_dequeue and the amount larger\n",
    "  #   determines the maximum we will prefetch.  Recommendation:\n",
    "  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
    "    min_after_dequeue = 10000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    \n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "        [example, label], batch_size=batch_size, capacity=capacity,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您需要在文件之间进行更多的并行性或示例的 shuffling，请使用多个读取器实例 tf.train.shuffle_batch_join。\n",
    "\n",
    "例如:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3cschool.cn/tensorflow_python/tensorflow_python-urli2dhu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
