{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow的小知识（随笔）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href='#flags'>1. tf.flags.DEFINE_xxx()</a>\n",
    "- <a href='#tf_trans'>2. tf.Transform</a>\n",
    "- <a href='#tf_data'>3. tf.data API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='flags'>1. tf.flags.DEFINE_xxx()</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.flags.DEFINE_xxx() \n",
    "- FLAGS = tf.flags.FLAGS \n",
    "- FLAGS.flag_values_dict()    \n",
    "FLAGS._parse_flags()  这种用法，改成了用FLAGS.flag_values_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的说：  \n",
    "用于帮助我们添加命令行的可选参数。    \n",
    "也就是说利用该函数我们可以实现在命令行中选择需要设定的参数来运行程序， \n",
    "可以不用反复修改源代码中的参数，直接在命令行中进行参数的设定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 举个例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_string('name', 'default', 'name of the model')\n",
    "tf.flags.DEFINE_integer('num_seqs', 100, 'number of seqs in one batch')\n",
    "tf.flags.DEFINE_integer('num_steps', 100, 'length of one seq')\n",
    "tf.flags.DEFINE_integer('lstm_size', 128, 'size of hidden state of lstm')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'number of lstm layers')\n",
    "tf.flags.DEFINE_boolean('use_embedding', False, 'whether to use embedding')\n",
    "tf.flags.DEFINE_integer('embedding_size', 128, 'size of embedding')\n",
    "tf.flags.DEFINE_float('learning_rate', 0.001, 'learning_rate')\n",
    "tf.flags.DEFINE_float('train_keep_prob', 0.5, 'dropout rate during training')\n",
    "tf.flags.DEFINE_string('input_file', '', 'utf8 encoded text file')\n",
    "tf.flags.DEFINE_integer('max_steps', 100000, 'max steps to train')\n",
    "tf.flags.DEFINE_integer('save_every_n', 1000, 'save the model every n steps')\n",
    "tf.flags.DEFINE_integer('log_every_n', 10, 'log to the screen every n steps')\n",
    "tf.flags.DEFINE_integer('max_vocab', 3500, 'max char number')\n",
    "#全局参数设置，显示在命令行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在命令行中我们为了执行train.py文件，在命令行中输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py \\\n",
    "  --input_file data/shakespeare.txt  \\\n",
    "  --name shakespeare \\\n",
    "  --num_steps 50 \\\n",
    "  --num_seqs 32 \\\n",
    "  --learning_rate 0.01 \\\n",
    "  --max_steps 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过输入不同的文件名、参数，可以快速完成程序的调参和更换训练集的操作，不需要进入源码中更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_seqs': 100, 'num_steps': 100, 'lstm_size': 128}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#取上述代码中一部分进行实验\n",
    "tf.flags.DEFINE_integer('num_seqs', 100, 'number of seqs in one batch')\n",
    "tf.flags.DEFINE_integer('num_steps', 100, 'length of one seq')\n",
    "tf.flags.DEFINE_integer('lstm_size', 128, 'size of hidden state of lstm')\n",
    "\n",
    "#通过print()确定下面内容的功能\n",
    "FLAGS = tf.flags.FLAGS  # FLAGS保存命令行参数的数据\n",
    "FLAGS.flag_values_dict() #将其解析成字典存储到FLAGS.__flags中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FLAGS.flag_values_dict())\n",
    "\n",
    "# print(FLAGS.num_seqs)\n",
    "\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.flag_values_dict().items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error信息：UnrecognizedFlagError: Unknown command line flag 'f'\n",
    "jupyter 下载cifar数据问题：\n",
    "\n",
    "添加 tf.app.flags.DEFINE_string('f', '', 'kernel') 完美解决！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as np\n",
    "\n",
    "# tf.flags.DEFINE_xxx\n",
    "# FLAGS=tf.flags.FLAGES\n",
    "# 添加命令行的可选参数\n",
    "tf.flags.DEFINE_integer(\n",
    "    'min_word_frequency', 5, 'Minimum frequency of words in the vocabulary'\n",
    ")\n",
    "\n",
    "tf.flags.DEFINE_integer(\n",
    "    'max_sentence_len', 160, \"Maximum Sentence Length\"\n",
    ")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'input_dir', os.path.abspath('../../ubuntu_dataset'),\n",
    "    \"Input directory containing original CSV data files\"\n",
    ")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "    'output_dir', os.path.abspath('../../ubuntu_dataset'),\n",
    "    'Output directory for TFRecord files'\n",
    ")\n",
    "\n",
    "# 仅是在jupyter的时候会出现的报错信息。\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "TRAIN_PATH = os.path.join(FLAGS.input_dir, 'train.csv')\n",
    "VALIDATION_PATH = os.path.join(FLAGS.input_dir, 'valid.csv')\n",
    "TEST_PATH = os.path.join(FLAGS.input_dir, 'test.csv')\n",
    "\n",
    "\n",
    "print(FLAGS.min_word_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='tf_trans'>2. tf.Transform</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "谷歌发布全新TensorFlow库“tf.Transform” 简化机器学习数据预处理过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际的机器学习开发中，开发者通常需要对数据集进行大量的耗时费力的预处理过程，以适应各种不同标准的机器学习模型（例如神经网络）。\n",
    "\n",
    "这些预处理过程根据待解问题的不同和原始数据的组织形式而各不相同，包括不同**格式之间的转换**，**分词**、**词干提取**和**形成词汇**，以及包括**归一化在内的各种数值操作**等等。\n",
    "\n",
    "实际上，数据的预处理已经成为了机器学习开发中无法回避的一个难题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对这一难题，谷歌于 22 日通过开发者博客正式发布了一个基于 TensorFlow 的全新功能组件 —— **tf.Transform**。\n",
    "\n",
    "它允许用户在大规模数据处理框架中：\n",
    "- 定义预处理流水线（preprocessing pipelines），\n",
    "- 同时用户还可以将这些流水线导出，\n",
    "- 并将其作为 TensorFlow 计算图（TensorFlow graph）的一部分。\n",
    "\n",
    "用户可以通过组合 Python 函数来定义该流水线，然后在 Apache Beam 框架下通过 tf.Transform 执行。   \n",
    "（注：Apache Beam 是一个用于大规模的、高效的、分布式的数据处理的开源框架）目前，基于 Apache Beam 框架的流水线可以在 Google Cloud Dataflow 平台上运行，并计划在未来支持更多的平台（可能包括 Apache Apex，Apache Flink 和 Apache Spark 等）。\n",
    "\n",
    "值得一提的是，通过 tf.Transform 导出的 TensorFlow 计算图还可以在模型预测阶段将这种数据预处理步骤复用（例如，通过 Tensorflow Serving 提供模型时）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一方面，开发者最终在产品形态运行机器学习模型时通常还会遇到**“训练服务偏差”（training-serving skew）**，即由于服务中处理的数据与模型训练中使用的数据存在一定的差异，从而造成的预测质量下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/training_serve_skew.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "面对这一问题，此次新发布的 tf.Transform 组件还能保证预处理过程的零偏差，即**保证服务中的数据预处理和训练中的数据预处理执行完全相同的操作**，特别是当模型训练和服务运行在不同的框架时，例如训练在 TensorFlow，服务在 Apache Beam 的情况。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了便于**数据的预处理**之外，tf.Transform 还允许用户计算其数据集的**统计概要（summary statistics）**。\n",
    "\n",
    "对于每个机器学习项目的开发者而言，深刻理解其数据都非常重要，因为任何对底层数据做出的错误假设都可能会产生一些微妙的错误。\n",
    "\n",
    "通过更简单和高效地计算数据的统计概要，tf.Transform 可以帮助开发者更好地检查他们关于原始数据和预处理数据的假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装：\n",
    "pip install tensorflow-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官方文档介绍：\n",
    "https://www.tensorflow.org/tfx/transform/get_started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本指南介绍了它们的基本概念tf.Transform和使用方法。\n",
    "\n",
    "它会：\n",
    "- 定义预处理功能，管道的逻辑描述，将原始数据转换为用于训练机器学习模型的数据。\n",
    "- 通过将预处理功能转换为 Beam管道，显示用于转换数据的Apache Beam实现。\n",
    "- 显示其他用法示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.定义预处理功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该预处理功能是最重要的概念**tf.Transform**。\n",
    "\n",
    "预处理功能是数据集转换的逻辑描述。\n",
    "预处理函数接受并返回张量的字典，其中张量是指Tensor或SparseTensor。\n",
    "\n",
    "有两种函数用于定义预处理函数：   \n",
    "1. 任何接受和返回张量的函数。这些将TensorFlow操作添加到图形中，将原始数据转换为转换后的数据。\n",
    "2. 任何分析仪提供tf.Transform。分析仪也接受并返回张量，但与TensorFlow函数不同，它们不会向图形添加操作。相反，分析器会导致tf.Transform计算TensorFlow之外的全通操作。它们使用整个数据集上的输入张量值来生成作为输出返回的常量张量。例如，tft.min计算数据集上的张量的最小值。tf.Transform提供一组固定的分析仪，但这将在未来的版本中进行扩展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预处理功能示例\n",
    "通过组合分析器和常规TensorFlow功能，用户可以创建灵活的管道来转换数据。\n",
    "\n",
    "以下预处理功能以不同方式转换三个特征中的每一个，并结合了两个特征："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4fad79bf462e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_transform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_transform'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乌龙！安装不上tensorflow_transform\n",
    "tensorflow-transform requires Python '>=2.7,<3' but the running Python is 3.6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name='tf_data'>3. tf.data API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 tf.data API，您可以根据简单的可重用片段构建复杂的输入管道。\n",
    "\n",
    "例如：\n",
    "- 图片模型的管道可能会汇聚分布式文件系统中的文件中的数据、对每个图片应用随机扰动，并将随机选择的图片合并成用于训练的批次。\n",
    "- 文本模型的管道可能包括从原始文本数据中提取符号、根据对照表将其转换为嵌入标识符，以及将不同长度的序列组合成批次数据。\n",
    "\n",
    "使用 tf.data API 可以轻松处理大量数据、不同的数据格式以及复杂的转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义来源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
