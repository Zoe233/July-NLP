{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯与应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.贝叶斯理论简单回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们有一大堆样本（包含**特征**和**类别**）的时候，我们非常容易通过统计得到$p(特征|类别)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x)p(y|x) = p(y)p(x|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变化公式得到：\n",
    "$$p(特征)p(类别|特征) = p(类别)p(特征|类别)$$$$p(类别|特征) = \\frac{p(类别)p(特征|类别)}{p(特征)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.独立假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>看起来很简单，但实际上，你的特征可能是很多维的</p>\n",
    "$$p(features|class) = p({f_0, f_1, \\ldots ,f_n}|c)$$<p>就算是2个维度吧，可以简单写成</p>\n",
    "$$p({f_0, f_1}|c) = p(f_1|c, f_0)p(f_0|c)$$<p>这时候我们加一个特别牛逼的假设：特征之间是独立的。这样就得到了</p>\n",
    "$$p({f_0, f_1}|c) = p(f_1|c)p(f_0|c)$$<p>其实也就是：</p>\n",
    "$$p({f_0, f_1, \\ldots, f_n}|c) = \\Pi^n_i p(f_i|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.贝叶斯分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>OK，回到机器学习，其实我们就是对每个类别计算一个概率$p(c_i)$，然后再计算所有特征的条件概率$p(f_j|c_i)$，那么分类的时候我们就是依据贝叶斯找一个最可能的类别：</p>\n",
    "$$p(class_i|{f_0, f_1, \\ldots, f_n})= \\frac{p(class_i)}{p({f_0, f_1, \\ldots, f_n})} \\Pi^n_j p(f_j|c_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 文本分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来看一个文本分类问题，经典的新闻主题分类，用朴素贝叶斯怎么做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_set(words_file):\n",
    "    '''\n",
    "    读取指定文本文件，获取词汇，汇集词汇关键字。\n",
    "    '''\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip()\n",
    "            if len(word)>0 and word not in words_set: # 去重\n",
    "                words_set.add(word)\n",
    "    return words_set\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(folder_path, test_size=0.2):\n",
    "    '''\n",
    "    文本预处理，遍历加载Database目录下的新闻语料。\n",
    "    根据test_size参数，手动建立测试集和训练集文本语料。\n",
    "    '''\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = []\n",
    "    class_list = []\n",
    "    \n",
    "    # 遍历文件夹，读取新闻正文并分词，获取对应的新闻类别标签列表\n",
    "    for folder in folder_list:\n",
    "        if folder == '.DS_Store':\n",
    "            continue\n",
    "        else:\n",
    "            new_folder_path = os.path.join(folder_path, folder)\n",
    "            files = os.listdir(new_folder_path)\n",
    "        \n",
    "            # 读取文件\n",
    "            j = 1\n",
    "            for file in files:\n",
    "                if j > 100: # 怕内存爆掉，只取100个样本文件\n",
    "                    break\n",
    "                with open(os.path.join(new_folder_path, file), 'r') as fp:\n",
    "                    raw = fp.read()\n",
    "\n",
    "                # 开启jieba并行分词模式，参数为并行进程数。不支持windows操作系统\n",
    "                jieba.enable_parallel(4)\n",
    "\n",
    "                word_cut = jieba.cut(raw, cut_all=False) # 精确模式，返回的结构式一个可迭代的generator\n",
    "                word_list = list(word_cut)\n",
    "\n",
    "                # 关闭并行分词模式\n",
    "                jieba.disable_parallel()\n",
    "\n",
    "                data_list.append(word_list)  # 训练集list\n",
    "                # class_list.append(folder.decode('utf-8')) # 类别Label\n",
    "                class_list.append(folder)\n",
    "                j += 1\n",
    "    \n",
    "    # 手动拆分训练集和测试集\n",
    "    data_class_list = list(zip(data_list, class_list))\n",
    "    random.shuffle(data_class_list) # 将序列的所有元素随机排序。\n",
    "    index = int(len(data_class_list) * test_size) + 1\n",
    "    train_list = data_class_list[index: ]\n",
    "    test_list = data_class_list[ :index]\n",
    "    train_data_list, train_class_list = zip(*train_list)\n",
    "    test_data_list, test_class_list = zip(*test_list)\n",
    "    \n",
    "    # 或者使用sklearn自带的部分拆分训练集和测试集\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    # train_data_list, test_data_list, train_class_list, test_class_list = train_test_split(data_list, class_list, test_size=test_size)\n",
    "    \n",
    "    # 统计词频\n",
    "    all_words_dict = {}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if all_words_dict.get(word):\n",
    "                all_words_dict[word] += 1\n",
    "            else:\n",
    "                all_words_dict[word] = 1\n",
    "    \n",
    "    # key函数利用词频进行降序排序\n",
    "    all_words_tuple_list = sorted(all_words_dict.items(), key=lambda f:f[1], reverse=True)\n",
    "    all_words_list = list(zip(*all_words_tuple_list))[0]\n",
    "    \n",
    "    return all_words_list, train_data_list, test_data_list, train_class_list, test_class_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    '''\n",
    "    处理词汇表，构建矩阵特征词汇表\n",
    "    去重中止词，去除纯数字组成的字符串，词汇长度在2-4之间。\n",
    "    '''\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for t in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 1000:  # feature_words的维度大小限制在1000以内\n",
    "            break\n",
    "        \n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t]) < 5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n += 1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features(train_data_list, test_data_list, feature_words, flag='nltk'):\n",
    "    '''\n",
    "    文本特征处理，构造词袋。\n",
    "    根据特征词汇feature_words，构造1000维度的1-hot向量。\n",
    "    '''\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        if flag == 'nltk':\n",
    "            features= {word:1 if word in text_words else 0 for word in feature_words}\n",
    "        elif flag == 'sklearn':\n",
    "            features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        else:\n",
    "            features = []\n",
    "        return features\n",
    "    \n",
    "    train_feature_list = [text_features(text, feature_words) for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words) for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag='nltk'):\n",
    "    '''\n",
    "    文本分类器，返回测试集准确率。\n",
    "    '''\n",
    "    if flag == 'nltk':\n",
    "        # 使用nltk分类器\n",
    "        train_flist = zip(train_feature_list, train_class_list)\n",
    "        test_flist = zip(test_feature_list, test_class_list)\n",
    "        classifier = nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy = nltk.classify.accuracy(classifier, test_flist)\n",
    "    \n",
    "    elif flag =='sklearn':\n",
    "        classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "        test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "    \n",
    "    else:\n",
    "        test_accuray = []\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------start-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('start'.center(66,'-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w2/qnnfb62x2g760j3nkh9q4ptr0000gn/T/jieba.cache\n",
      "Loading model cost 0.723 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. 文本预处理\n",
    "folder_path = './Database/SogouC/Sample'\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, test_class_list = text_processing(folder_path, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 生成stopwords set\n",
    "stopword_file = './files/stopwords_cn.txt'\n",
    "stopwords_set = make_word_set(stopword_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 文本特征提取和分类\n",
    "# flag = 'nltk'\n",
    "flag = 'sklearn'\n",
    "deleteNs = range(0, 1000, 20)  # 每20个，交叉验证，选取多大的特征维度效果更佳。\n",
    "test_accuracy_list = []\n",
    "for deleteN in deleteNs:\n",
    "    feature_words = words_dict(all_words_list, deleteN, stopwords_set)\n",
    "    train_feature_list, test_feature_list = text_features(train_data_list, test_data_list, feature_words, flag)\n",
    "    test_accuracy = text_classifier(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "print(test_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 结果评价\n",
    "plt.figure()\n",
    "plt.plot(deleteNs, test_accuracy_list)\n",
    "plt.title('Relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
