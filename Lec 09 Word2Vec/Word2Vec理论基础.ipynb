{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec理论基础：词向量到Word2Vec与相关应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录：\n",
    "- <a href='#common_tasks'>1. NLP常见任务</a>\n",
    "- <a href='#methods'>2. NLP处理方法</a>\n",
    "    - <a href='#encoding_similarity'>2.1 词编码需要保证词的相似性</a>\n",
    "    - <a href='#vector'>2.2 简单 词/短语 翻译</a>\n",
    "        - 向量空间分布的相似性\n",
    "        - 向量空间子结构\n",
    "        - 在计算机中表示一个词\n",
    "- <a href='#distributed_representation'>3. 词编码：离散表示</a>\n",
    "    - <a href='#one_hot'>离散表示：One-hot表示</a>\n",
    "    - <a href='#bag_of_words'>离散表示：Bag of Words</a>\n",
    "        - 文档的向量表示可以直接将各词的词向量表示加和\n",
    "            - Bag Of Words 和 Set of words\n",
    "            - Hash Trick 降维\n",
    "        - 词权重（<a href='#tf_idf'>TF-IDF</a>, Binary weghting）\n",
    "    - <a href='#n-gram'>离散表示：Bi-gram和N-gram</a>\n",
    "        - 语言模型\n",
    "    - <a href='#problems'>离散表示的问题</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='common_tasks'>1.NLP常见任务</a></h2>\n",
    "- 自动摘要：百度等搜索引擎，都会做**自动摘要**，获取关键字。\n",
    "- 指代消解：“小明放学了，妈妈去接他”，这个“他”指的是谁。\n",
    "- 机器翻译：小心地滑 ==> Slide carefully\n",
    "- 词性标注：heat(v.) water(n.) in(p.) a(det.) pot(n.)\n",
    "- 分词（中文，日文等）：大水沟/很/难/过\n",
    "- 主题识别\n",
    "- 文本分类：新闻内容自动分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='methods'>2. NLP处理方法</a></h2>\n",
    "- 传统：基于规则\n",
    "    - 国内工业界大厂主要是有大的Dict，可用性很强。\n",
    "- 现代：基于统计机器学习\n",
    "    - HMM, CRF, SVM, LDA, CNN\n",
    "    - “规则”隐含在模型参数里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='encoding_similarity'>2.1 词编码需要保证词的相似性</a></h3>\n",
    "Nearest words to **frog**:\n",
    "    1. frogs\n",
    "    2. toad\n",
    "    3. litoria\n",
    "    4. leptodactylidae\n",
    "    5. rana\n",
    "    6. lizard\n",
    "    7. eleutherodactylus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/wv01.png' width='50%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><a name='vector'>2.2 简单 词/短语 翻译</a></h3>\n",
    "#### 向量空间分布的相似性：\n",
    "<img src='./images/wv02.png' width='80%'/>\n",
    "\n",
    "文明有不同的形态，但是不同的文明之间，都有相通点。  \n",
    "比如：   \n",
    "不同的语言的都有1-10的数字。参考上面图上，英文和西班牙语的1-10的数字表示的词在二维空间内应该在差不多的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量空间子结构：\n",
    "$$V_{King}-V_{Queen}+V_{Women}=V_{Man}$$\n",
    "$$V_{Paris}-V_{France}+V_{German}=V_{Berlin}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“King的向量 减去 Queen的向量” 和“Man的向量 减去 Women的向量”的关系是几乎一样的。\n",
    "更加充分地表达文本，文本之间的关系等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最终目标：词向量表示作为机器学习，特别是深度学习的输入和表示空间。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在计算机中表示一个词\n",
    "<img src='./images/wv03.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建专家词典，**上位词**，**同义词**等。  \n",
    "问题在于，专家词典是人做的，需要大量人力维护，主观的，不能分辨细节的差别，比方说：“美丽”和“漂亮”之间的相近程度，词意随着时代发生变化，无法发现新词。\n",
    "这些的问题，主要在于专家的背景不一样，受主观的影响。\n",
    "\n",
    "WordNet就是这样一个字典。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='distributed_representation'>3.词编码：离散表示</a></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词袋模型(Bag of Words,简称BoW)。**   \n",
    "词袋模型**假设我们不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重**。   \n",
    "而权重与词在文本中出现的频率有关。  \n",
    "\n",
    "词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。\n",
    "\n",
    "总结下词袋模型的三部曲：**分词（tokenizing），统计修订词特征值（counting）与标准化（normalizing）。**\n",
    "\n",
    "与词袋模型非常类似的一个模型是**词集模型(Set of Words,简称SoW)**，和词袋模型唯一的不同是它**仅仅考虑词是否在文本中出现，而不考虑词频**。也就是一个词在文本在文本中出现1次和多次特征处理是一样的。在大多数时候，我们使用词袋模型，后面的讨论也是以词袋模型为主。\n",
    "\n",
    "当然，词袋模型有很大的局限性，因为它仅仅考虑了词频，没有考虑上下文的关系，因此会丢失一部分文本的语义。但是大多数时候，如果我们的目的是分类聚类，则词袋模型表现的很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='one_hot'>离散表示：One-hot 表示</a></h4>\n",
    "语料库：   \n",
    "John likes to watch movies. Mary likes too.    \n",
    "John also likes to watch football games.\n",
    "\n",
    "词典：   \n",
    "{'John':1, 'likes':2, 'to':3, 'watch':4, 'movies':5, 'also':6, 'football':7, 'games':8, 'Mary':9, 'too':10}  \n",
    "\n",
    "One-hot表示：  \n",
    "John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "...\n",
    "too : [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "- 词典包含10个单词，每个单词有唯一索引\n",
    "- 在词典中的顺序和在句子中的顺序没有关联"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='bag_of_words'>离散表示：Bag of Words</a></h4>\n",
    "- 文档（句子）的向量表示可以直接将各词的词向量表示加和\n",
    "\n",
    "<img src='./images/wv04.png' width='70%'/>\n",
    "\n",
    "- 词权重：词在文档中的顺序没有被考虑\n",
    "    - TF-IDF(Term Frequency - Inverse Document Frequency) \n",
    "        信息检索的词t的IDF weight $\\log(1+\\frac{N}{n_t})$，N: 文档总数，$n_t$：含有词t的文档数。\n",
    "        <img src='./images/wv05.png' width='70%'/>\n",
    "    - Binary weighting 短文本相似性，Bernoulli Naive Bayes\n",
    "        <img src='./images/wv06.png' width='30%'/>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在词袋模型的统计词频这一步，我们会得到该文本中所有词的词频，有了词频，我们就可以用词向量表示这个文本。   \n",
    "这里我们举一个例子，例子直接用scikit-learn的CountVectorizer类来完成，这个类可以帮我们完成文本的词频统计与向量化，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 15)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus =[\"I come to China to travel\", \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    "\n",
    "print(vectorizer.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(文本的序号, 词的序号<Total>)\\ \\  词频$$  \n",
    "\n",
    "我们看看对于上面4个文本的处理输出。\n",
    "\n",
    "可以看出4个文本的词频已经统计出，在输出中，左边的括号中的第一个数字是**文本的序号**，第2个数字是**词的序号**，注意词的序号是基于所有的文档的。第三个数字就是我们的**词频**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以进一步看看每个文本的词向量特征和各个特征代表的词，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'apple',\n",
       " 'car',\n",
       " 'china',\n",
       " 'come',\n",
       " 'in',\n",
       " 'is',\n",
       " 'love',\n",
       " 'papers',\n",
       " 'polupar',\n",
       " 'science',\n",
       " 'some',\n",
       " 'tea',\n",
       " 'the',\n",
       " 'this',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'work',\n",
       " 'write']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到我们一共有19个词，所以4个文本都是19维的特征向量。\n",
    "而每一维的向量依次对应了下面的19个词。\n",
    "另外由于词\"I\"在英文中是停用词，不参加词频的统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(corpus).toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于大部分的文本都只会使用词汇表中的很少一部分的词，因此我们的词向量中会有大量的0。也就是说词向量是**稀疏**的。在实际应用中一般使用**稀疏矩阵**来存储。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本做了**词频统计**后，我们一般会通过**TF-IDF**进行词特征值修订，这部分我们后面再讲。\n",
    "\n",
    "向量化的方法很好用，也很直接，但是在有些场景下很难使用，比如分词后的词汇表非常大，达到100万+，此时如果我们直接使用向量化的方法，将对应的样本对应特征矩阵载入内存，有可能将内存撑爆，在这种情况下我们怎么办呢？\n",
    "第一反应是我们要进行**特征的降维**，说的没错！而**Hash Trick**就是非常常用的文本特征降维方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大规模的文本处理中，由于特征的维度对应分词词汇表的大小，所以维度可能非常恐怖，此时需要进行**降维**，不能直接用我们上一节的向量化方法。\n",
    "\n",
    "而**最常用的文本降维方法是Hash Trick**。说到Hash，一点也不神秘，学过数据结构的同学都知道。这里的Hash意义也类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Hash Trick里，我们会定义一个特征Hash后对应的哈希表的大小，这个哈希表的维度会远远小于我们的词汇表的特征维度，因此可以看成是降维。    \n",
    "\n",
    "具体的方法是:\n",
    "- 对应任意一个特征名，我们会用Hash函数找到对应哈希表的位置，\n",
    "- 然后将该特征名对应的词频统计值累加到该哈希表位置。\n",
    "\n",
    "如果用数学语言表示，假如哈希函数$h$使第$i$个特征哈希到位置$j$，即$h(i)=j$，则第$i$个原始特征的词频数值$\\phi (i)$将累加到哈希后的第$j$个特征的词频数值$\\overline \\phi$上，即：\n",
    "\n",
    "$$\\overline \\phi (j) = \\sum_{i \\in J; h(i)=j} \\phi (i)$$\n",
    "　其中$J$是原始特征的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是上面的方法有一个问题，有可能两个原始特征的哈希后位置在一起导致词频累加特征值突然变大，为了解决这个问题，出现了Hash Trick的变种**Signed Hash Trick**，此时除了哈希函数$h$，我们多了一个哈希函数：\n",
    "$$ξ: N \\to \\pm 1$$\n",
    "\n",
    "此时我们有\n",
    "$$\\overline \\phi(j) = \\sum_{i \\in J;h(i)=j} ξ(i)\\phi (i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样做的好处是，哈希后的特征仍然是一个无偏的估计，不会导致某些哈希位置的值过大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，大家会有疑惑，**这种方法来处理特征，哈希后的特征是否能够很好的代表哈希前的特征呢？**\n",
    "\n",
    "从实际应用中说，由于文本特征的高稀疏性，这么做是可行的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在scikit-learn的HashingVectorizer类中，实现了基于signed hash trick的算法，这里我们就用HashingVectorizer来实践一下Hash Trick，为了简单，我们使用上面的19维词汇表，并哈希降维到6维。\n",
    "\n",
    "当然在实际应用中，19维的数据根本不需要Hash Trick，这里只是做一个演示，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t2.0\n",
      "  (0, 2)\t-1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t-1.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 2)\t-1.0\n",
      "  (1, 5)\t-1.0\n",
      "  (2, 0)\t2.0\n",
      "  (2, 5)\t-2.0\n",
      "  (3, 0)\t0.0\n",
      "  (3, 1)\t4.0\n",
      "  (3, 2)\t-1.0\n",
      "  (3, 3)\t1.0\n",
      "  (3, 5)\t-1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer2 = HashingVectorizer(n_features=6, norm=None)\n",
    "\n",
    "print(vectorizer2.fit_transform(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家可以看到结果里面有负数，这是因为我们的哈希函数ξ可以哈希到1或者-1导致的。\n",
    "\n",
    "和PCA类似，Hash Trick降维后的特征我们已经不知道它代表的特征名字和意义。\n",
    "\n",
    "此时我们不能像上一节向量化时候可以知道每一列的意义，所以Hash Trick的解释性不强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。\n",
    "\n",
    "而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong><a name='tf_idf'>TF-IDF</a></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在文本挖掘的预处理中，向量化之后一般都伴随着TF-IDF的处理，那么什么是TF-IDF，为什么一般我们要加这一步预处理呢？这里就对TF-IDF的原理做一个总结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**文本向量化特征的不足：普遍词所占权重比例过大**\n",
    "\n",
    "在将文本分词并向量化后，我们可以得到词汇表中每个词在各个文本中形成的词向量。\n",
    "\n",
    "corpus=[\"I come to China to travel\",   \n",
    "    \"This is a car polupar in China\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science\"] \n",
    " \n",
    "如果我们直接将统计词频后的N维特征做为文本分类的输入，会发现有一些问题。\n",
    "\n",
    "比如第一个文本，我们发现\"come\",\"China\"和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的\"China\"和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们讲到几乎所有文本都会出现的\"to\"其词频虽然高，但是重要性却应该比词频低的\"China\"和“Travel”要低。我们的**IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概括来讲，** IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“to”。** 而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专业的名词如“Machine Learning”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下：\n",
    "$$IDF(t)= \\log \\frac{N}{N_t}$$\n",
    "\n",
    "其中，$N$代表语料库中文本的总数，而$N(t)$代表语料库中包含词$t$的文本总数。为什么IDF的基本公式应该是是上面这样的而不是像$N/N(t)$这样的形式呢？这就涉及到信息论相关的一些知识了。感兴趣的朋友建议阅读吴军博士的《数学之美》第11章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个**生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。**\n",
    "\n",
    "所以常用的IDF我们需要做一些**平滑**，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为：\n",
    "\n",
    "$$IDF(t)= \\log \\frac{N+1}{N_t+1} +1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：\n",
    "$$TF-IDF(t) = TF(t) * IDF(t)$$\n",
    "其中，$TF(T)$指词$t$在当前文本中的词频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在scikit-learn中，有两种方法进行TF-IDF的预处理。\n",
    "\n",
    "- 第一种方法是在用CountVectorizer类向量化之后再调用TfidfTransformer类进行预处理。\n",
    "- 第二种方法是直接用TfidfVectorizer完成向量化与TF-IDF预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们来看第一种方法，CountVectorizer+TfidfTransformer的组合，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='n-gram'>离散表示：Bi-gram和N-gram</a></h4>\n",
    "为2-gram建索引：  \n",
    "    {\"John likes”: 1,    \n",
    "    \"likes to”: 2,    \n",
    "    \"to watch”: 3,    \n",
    "    \"watch movies”: 4,    \n",
    "    \"Mary likes”: 5,    \n",
    "    \"likes too”: 6,    \n",
    "    \"John also”: 7,    \n",
    "    \"also likes”: 8,    \n",
    "    “watch football”: 9,  \n",
    "    \"football games\": 10}  \n",
    "  \n",
    "  <img src='./images/wv07.png' width='40%'/>\n",
    "  \n",
    "  - 优点：考虑了词的顺序\n",
    "  - 缺点：词表的膨胀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src='./images/wv10.png' width='40%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 语言模型\n",
    "- 一句话（词组合）出现的概率\n",
    "$$P(w_1,w_2,...,w_n) =\\prod_{i=1}^m P(w_i|w_1,...,w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/wv08.png' width='70%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><a name='problems'>离散表示的问题</a></h4>\n",
    "- 无法衡量词向量之间的关系：各种度量（与或非、距离）都不合适\n",
    "酒店  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
    "宾馆  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
    "旅舍  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
    "    - 太系数，很难捕捉文本的含义\n",
    "    \n",
    "- 词表维度随着语料库增长膨胀\n",
    "- n-gram词序列岁语料库膨胀更快\n",
    "- 数据稀疏问题\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
