{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec+CNN做文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文详见《Convolutional Neural Networks for Sentence Classification》\n",
    "http://arxiv.org/abs/1408.5882\n",
    "\n",
    "Theano完成的代码版本：\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "\n",
    "TensorFlow改写的代码版本：\n",
    "https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\n",
    "添加分词和中文词向量映射之后，可用于中文文本分类(情感分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 本节内容主要记录TensorFlow的版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow版本介绍：\n",
    "本节的代码实现主要基于“<a href='http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/'>在TensorFlow中实现CNN进行文本分类</a>”的论文。\n",
    "\n",
    "具体的思想和内容，参见论文，此处会酌情提取相关内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将实现一个类似于Kim Yoon的<a href='https://arxiv.org/abs/1408.5882'>用于句子分类</a>的<a href='https://arxiv.org/abs/1408.5882'>卷积神经网络</a>的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文提出的模型在一系列文本分类任务（如情感分析）中实现了良好的分类性能，并已成为新文本分类体系结构的标准基线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我假设您已经熟悉应用于NLP的卷积神经网络的基础知识。如果没有，我建议首先阅读了解<a href='http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/'>NLP的卷积神经网络</a>，以获得必要的背景知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: 数据和预处理 Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在这篇文章中使用的数据集是来自<a href='http://www.cs.cornell.edu/people/pabo/movie-review-data/'>烂番茄的电影评论数据</a>- 原始论文中也使用的数据集之一。\n",
    "\n",
    "数据集包含10,662个示例评论句子，半正面和半负面。数据集的大小约为20k。请注意，由于此数据集非常小，我们可能会过度使用强大的模型。此外，数据集没有官方训练集/测试集拆分，因此我们只使用10％的数据作为开发集dev set。原始论文显示了对数据进行10倍交叉验证(10-fold cross-validation)的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我不会在这篇文章中讨论数据预处理代码，但它可以在Github上获得并执行以下操作：\n",
    "\n",
    "1. 从原始数据文件中加载正面和负面的句子。\n",
    "2. 使用与原始论文<a href='https://github.com/yoonkim/CNN_sentence'>Github: 相同的代码</a>清理文本数据。\n",
    "3. 将每个句子填充到最大句子长度59。我们将特殊<PAD\\>标记附加到所有其他句子，使它们成为59个单词的句子。将句子填充到相同长度是有用的，因为它允许我们有效地批量处理我们的数据，因为批处理中的每个示例必须具有相同的长度。\n",
    "4. 构建词汇索引并将每个单词映射到0到18,765之间的整数（词汇量大小）。每个句子都成为整数向量Each sentence becomes a vector of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sys, re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, TREC=False):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip() if TREC else string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_sst(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for the SST dataset\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)   \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_cv(data_folder, cv=10, clean_string=True):\n",
    "    '''\n",
    "    Loads data and split into 10 folds.\n",
    "    '''\n",
    "    revs = []\n",
    "    pos_file = data_folder[0]\n",
    "    neg_file = data_folder[1]\n",
    "    \n",
    "    vocab = defaultdict(float)\n",
    "    \n",
    "    with open(pos_file, 'r') as f:\n",
    "        for line in f:\n",
    "            rev = []\n",
    "            rev.append(line.strip())\n",
    "            if clean_string:\n",
    "                orig_rev = clean_str(' '.join(rev))\n",
    "            else:\n",
    "                orig_rev = ' '.join(rev).lower()\n",
    "            \n",
    "            words = set(orig_rev.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "                \n",
    "            datum = {'y':1, 'text':orig_rev, 'num_words':len(orig_rev.split()), 'split':np.random.randint(0, cv)}\n",
    "            revs.append(datum)\n",
    "    \n",
    "    with open(neg_file, 'r') as f:\n",
    "        for line in f:\n",
    "            rev = []\n",
    "            rev.append(line.strip())\n",
    "            if clean_string:\n",
    "                orig_rev = clean_str(' '.join(rev))\n",
    "            else:\n",
    "                orig_rev = ' '.join(rev).lower()\n",
    "            \n",
    "            words = set(orig_rev.split())\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "            datum = {'y':0, 'text': orig_rev, 'num_words':len(orig_rev.split()), 'split':np.random.randint(0,cv)}\n",
    "            revs.append(datum)\n",
    "    return revs, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(word_vecs, k=300):\n",
    "    '''\n",
    "    Get word matrix. W[i] is the vector for word indexed by i.\n",
    "    '''\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size + 1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    \n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bin_vec(fname, vocab):\n",
    "    '''\n",
    "    Loads 300*1 word vecs from Google (Mikolov) word2vec\n",
    "    '''\n",
    "    word_vecs = {}\n",
    "    \n",
    "    with open(fname, 'rb') as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)   \n",
    "            if word in vocab:\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "                \n",
    "    return word_vecs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unknown_words(word_vecs, vocab, min_df =1, k=300):\n",
    "    '''\n",
    "    For words that occur in at least min_df documents, create a separate word vector. \n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones.\n",
    "    '''\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：加载正面和负面评论，清理文本，将其添加到一个字典中，并通过np.random.randint(0,10)生成的随机值来为将来的k-fold交叉验证数据抽取，并详细记录其信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "data_folder = [\"./rt-polaritydata/rt-polarity.pos\",\"./rt-polaritydata/rt-polarity.neg\"]    \n",
    "print(\"loading data...\")\n",
    "\n",
    "revs, vocab = build_data_cv(data_folder, cv=10, clean_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': \"enigma is well made , but it 's just too dry and too placid\",\n",
       " 'num_words': 14,\n",
       " 'split': 5}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"data loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：记录数据集基本特征信息：词汇表大小，语料的句子数量，最常句子长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>the rock is destined to be the 21st century 's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_words  split                                               text  y\n",
       "0         34      9  the rock is destined to be the 21st century 's...  1\n",
       "1         38      4  the gorgeously elaborate continuation of the l...  1\n",
       "2          5      5                     effective but too tepid biopic  1\n",
       "3         20      8  if you sometimes like to go to the movies to h...  1\n",
       "4         22      3  emerges as something rare , an issue movie tha...  1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(revs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 10662\n",
      "vocab size: 18764\n",
      "max sentence length: 56\n"
     ]
    }
   ],
   "source": [
    "max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
    "\n",
    "print(\"number of sentences: \" + str(len(revs)))\n",
    "print(\"vocab size: \" + str(len(vocab)))\n",
    "print(\"max sentence length: \" + str(max_l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载现成的已经预训练好的word2vec模型，并将词汇在word2vec模型中找到对应的词汇向量，构造我们数据集的word2vec矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = sys.argv[1]  # 1.5 G\n",
    "print(\"loading word2vec vectors...\")\n",
    "\n",
    "w2v = load_bin_vec(w2v_file, vocab)\n",
    "print(\"word2vec loaded!\")\n",
    "print(\"num words already in word2vec: \" + str(len(w2v)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_unknown_words(w2v, vocab)\n",
    "W, word_idx_map = get_W(w2v)\n",
    "rand_vecs = {}\n",
    "add_unknown_words(rand_vecs, vocab)\n",
    "W2, _ = get_W(rand_vecs)\n",
    "cPickle.dump([revs, W, W2, word_idx_map, vocab], open(\"mr.p\", \"wb\"))\n",
    "print(\"dataset created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: 模型 The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将在这篇文章中构建的网络大致如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/sclf01.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layers embeds words into low-dimensional vectors.   \n",
    "第一层将单词嵌入到低维向量中。 \n",
    "\n",
    "The next layer performs convolutions over the embedded word vectors using multiple filter sizes. For example, sliding over 3, 4 or 5 words at a time.  \n",
    "下一层使用多个滤波器大小对嵌入的字向量执行卷积。例如，一次滑动3个，4个或5个字。\n",
    "\n",
    "Next, we max-pool the result of the convolutional layer into a long feature vector, add dropout regularization, and classify the result using a softmax layer.   \n",
    "接下来，我们将卷积层的结果最大化为长特征向量，添加丢失正则化，并使用softmax层对结果进行分类。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为这是一篇教育性的帖子，所以我决定从原始论文中简化模型：\n",
    "\n",
    "- 我们不会使用预先训练过的word2vec向量进行单词嵌入。相反，我们从头开始学习嵌入。\n",
    "- 我们不会对权重向量强制执行L2范数约束。对句子分类的卷积神经网络（和从业者指南）的敏感性分析发现，约束对最终结果影响不大。\n",
    "- 原始论文用两个输入数据通道进行实验 - 静态和非静态字向量。我们只使用一个channel。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里向代码添加上述扩展是相对简单的（几十行代码）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了允许各种超参数配置，我们将代码放入一个TextCNN类中，在init函数中生成模型图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    '''\n",
    "    A CNN for text classification. \n",
    "    Uses an embedding layers, followed by a convolutional, max-pooling and softmax layer.\n",
    "    '''\n",
    "    def __int__(self, sequence_length, num_classes, vocab_size, embedding_size, filter_sizes, num_filters):\n",
    "        # Implementation...\n",
    "        # Placeholders for input, output and dropout\n",
    "        \n",
    "        # usage: placeholder(dtype, shape=None, name=None)\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "        self.dropuout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实例化该类，我们传递以下参数：\n",
    "\n",
    "- sequence_length – 我们句子的长度。请记住，我们将所有句子填充为相同的长度（我们的数据集为59）。\n",
    "- num_classes – 输出层中的类数，在我们的例子中为两个（positive和negative）。\n",
    "- vocab_size – 我们词汇量的大小。这需要定义嵌入层 embedding layer的大小，嵌入层将具有形状[vocabulary_size, embedding_size]。\n",
    "- embedding_size – 嵌入的维度。\n",
    "- filter_sizes – 我们希望卷积filters覆盖的单词数。我们将为num_filters此处指定的每个尺寸。例如，[3, 4, 5]意味着我们将分别对3个，4个和5个单词过滤，总共进行3 * num_filters过滤\n",
    "- num_filters – 每个过滤器大小的过滤器数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.placeholder创建一个占位符变量，当我们在train或test时执行它时，我们将其提供给网络。\n",
    "\n",
    "第二个参数是输入张量的形状。\n",
    "**None意味着该维度的长度可以是任何东西。**      \n",
    "在我们的例子中，第一个维度是batch size，并且使用None允许网络处理任意大小的批次。\n",
    "\n",
    "The probability of keeping a neuron in the dropout layer is also an input to the network because we enable dropout only during training.   \n",
    "在dropout layer中保留神经元的概率也是网络的输入，因为我们仅在训练期间启用丢失。我们在评估模型时禁用它（稍后会详细介绍）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
