{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录：\n",
    "- bag_of_words 情感分析\n",
    "- <a href='#word2vec'>word2vec训练词向量</a>\n",
    "- <a href='#sentiment'>在Word2vec上训练情感分析模型</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用pandas读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "training_file_path = os.path.join(BASE_PATH, 'data/labeledTrainData.tsv')\n",
    "\n",
    "df = pd.read_csv(training_file_path, sep='\\t', escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "print('Num of reviews: {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The Classic War of the Worlds\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \"critics\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \"critics\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \"critics\" perceive to be its shortcomings.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1表示正面评论，0表示负面评论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对影评数据做预处理，大概有以下环节：\n",
    "\n",
    "1. 去掉html标签\n",
    "1. 移除标点\n",
    "1. 切分成词/token\n",
    "1. 去掉停用词\n",
    "1. 重组为新的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(text, title):\n",
    "    print(title)\n",
    "    print('\\n-----我是分割线-----\\n')\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据\n",
      "\n",
      "-----我是分割线-----\n",
      "\n",
      "\"The Classic War of the Worlds\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \"critics\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \"critics\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \"critics\" perceive to be its shortcomings.\n"
     ]
    }
   ],
   "source": [
    "raw_example = df['review'][1]\n",
    "display(raw_example, '原始数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去掉HTML标签的数据\n",
      "\n",
      "-----我是分割线-----\n",
      "\n",
      "\"The Classic War of the Worlds\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \"critics\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \"critics\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \"critics\" perceive to be its shortcomings.\n"
     ]
    }
   ],
   "source": [
    "example = BeautifulSoup(raw_example, 'html.parser').get_text()\n",
    "display(example, '去掉HTML标签的数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去掉标点的数据\n",
      "\n",
      "-----我是分割线-----\n",
      "\n",
      " The Classic War of the Worlds  by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H  G  Wells  classic book  Mr  Hines succeeds in doing so  I  and those who watched his film with me  appreciated the fact that it was not the standard  predictable Hollywood fare that comes out every year  e g  the Spielberg version with Tom Cruise that had only the slightest resemblance to the book  Obviously  everyone looks for different things in a movie  Those who envision themselves as amateur  critics  look only to criticize everything they can  Others rate a movie on more important bases like being entertained  which is why most people never agree with the  critics   We enjoyed the effort Mr  Hines put into being faithful to H G  Wells  classic novel  and we found it to be very entertaining  This made it easy to overlook what the  critics  perceive to be its shortcomings \n"
     ]
    }
   ],
   "source": [
    "example_letters = re.sub(r'[^a-zA-X]', ' ', example)\n",
    "display(example_letters, '去掉标点的数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "纯词列表数据\n",
      "\n",
      "-----我是分割线-----\n",
      "\n",
      "['the', 'classic', 'war', 'of', 'the', 'worlds', 'by', 'timothy', 'hines', 'is', 'a', 'very', 'entertaining', 'film', 'that', 'obviously', 'goes', 'to', 'great', 'effort', 'and', 'lengths', 'to', 'faithfully', 'recreate', 'h', 'g', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'in', 'doing', 'so', 'i', 'and', 'those', 'who', 'watched', 'his', 'film', 'with', 'me', 'appreciated', 'the', 'fact', 'that', 'it', 'was', 'not', 'the', 'standard', 'predictable', 'hollywood', 'fare', 'that', 'comes', 'out', 'every', 'year', 'e', 'g', 'the', 'spielberg', 'version', 'with', 'tom', 'cruise', 'that', 'had', 'only', 'the', 'slightest', 'resemblance', 'to', 'the', 'book', 'obviously', 'everyone', 'looks', 'for', 'different', 'things', 'in', 'a', 'movie', 'those', 'who', 'envision', 'themselves', 'as', 'amateur', 'critics', 'look', 'only', 'to', 'criticize', 'everything', 'they', 'can', 'others', 'rate', 'a', 'movie', 'on', 'more', 'important', 'bases', 'like', 'being', 'entertained', 'which', 'is', 'why', 'most', 'people', 'never', 'agree', 'with', 'the', 'critics', 'we', 'enjoyed', 'the', 'effort', 'mr', 'hines', 'put', 'into', 'being', 'faithful', 'to', 'h', 'g', 'wells', 'classic', 'novel', 'and', 'we', 'found', 'it', 'to', 'be', 'very', 'entertaining', 'this', 'made', 'it', 'easy', 'to', 'overlook', 'what', 'the', 'critics', 'perceive', 'to', 'be', 'its', 'shortcomings']\n"
     ]
    }
   ],
   "source": [
    "words = example_letters.lower().split()\n",
    "display(words, '纯词列表数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载停用词和其他语料会用到\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {}.fromkeys([ line.strip() for line in open('./stopwords.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\": None,\n",
       " \"'ll\": None,\n",
       " \"'m\": None,\n",
       " \"'re\": None,\n",
       " \"'s\": None,\n",
       " \"'t\": None,\n",
       " \"'ve\": None,\n",
       " 'ZT': None,\n",
       " 'ZZ': None,\n",
       " 'a': None,\n",
       " \"a's\": None,\n",
       " 'able': None,\n",
       " 'about': None,\n",
       " 'above': None,\n",
       " 'abst': None,\n",
       " 'accordance': None,\n",
       " 'according': None,\n",
       " 'accordingly': None,\n",
       " 'across': None,\n",
       " 'act': None,\n",
       " 'actually': None,\n",
       " 'added': None,\n",
       " 'adj': None,\n",
       " 'adopted': None,\n",
       " 'affected': None,\n",
       " 'affecting': None,\n",
       " 'affects': None,\n",
       " 'after': None,\n",
       " 'afterwards': None,\n",
       " 'again': None,\n",
       " 'against': None,\n",
       " 'ah': None,\n",
       " \"ain't\": None,\n",
       " 'all': None,\n",
       " 'allow': None,\n",
       " 'allows': None,\n",
       " 'almost': None,\n",
       " 'alone': None,\n",
       " 'along': None,\n",
       " 'already': None,\n",
       " 'also': None,\n",
       " 'although': None,\n",
       " 'always': None,\n",
       " 'am': None,\n",
       " 'among': None,\n",
       " 'amongst': None,\n",
       " 'an': None,\n",
       " 'and': None,\n",
       " 'announce': None,\n",
       " 'another': None,\n",
       " 'any': None,\n",
       " 'anybody': None,\n",
       " 'anyhow': None,\n",
       " 'anymore': None,\n",
       " 'anyone': None,\n",
       " 'anything': None,\n",
       " 'anyway': None,\n",
       " 'anyways': None,\n",
       " 'anywhere': None,\n",
       " 'apart': None,\n",
       " 'apparently': None,\n",
       " 'appear': None,\n",
       " 'appreciate': None,\n",
       " 'appropriate': None,\n",
       " 'approximately': None,\n",
       " 'are': None,\n",
       " 'area': None,\n",
       " 'areas': None,\n",
       " 'aren': None,\n",
       " \"aren't\": None,\n",
       " 'arent': None,\n",
       " 'arise': None,\n",
       " 'around': None,\n",
       " 'as': None,\n",
       " 'aside': None,\n",
       " 'ask': None,\n",
       " 'asked': None,\n",
       " 'asking': None,\n",
       " 'asks': None,\n",
       " 'associated': None,\n",
       " 'at': None,\n",
       " 'auth': None,\n",
       " 'available': None,\n",
       " 'away': None,\n",
       " 'awfully': None,\n",
       " 'b': None,\n",
       " 'back': None,\n",
       " 'backed': None,\n",
       " 'backing': None,\n",
       " 'backs': None,\n",
       " 'be': None,\n",
       " 'became': None,\n",
       " 'because': None,\n",
       " 'become': None,\n",
       " 'becomes': None,\n",
       " 'becoming': None,\n",
       " 'been': None,\n",
       " 'before': None,\n",
       " 'beforehand': None,\n",
       " 'began': None,\n",
       " 'begin': None,\n",
       " 'beginning': None,\n",
       " 'beginnings': None,\n",
       " 'begins': None,\n",
       " 'behind': None,\n",
       " 'being': None,\n",
       " 'beings': None,\n",
       " 'believe': None,\n",
       " 'below': None,\n",
       " 'beside': None,\n",
       " 'besides': None,\n",
       " 'best': None,\n",
       " 'better': None,\n",
       " 'between': None,\n",
       " 'beyond': None,\n",
       " 'big': None,\n",
       " 'biol': None,\n",
       " 'both': None,\n",
       " 'brief': None,\n",
       " 'briefly': None,\n",
       " 'but': None,\n",
       " 'by': None,\n",
       " 'c': None,\n",
       " \"c'mon\": None,\n",
       " \"c's\": None,\n",
       " 'ca': None,\n",
       " 'came': None,\n",
       " 'can': None,\n",
       " \"can't\": None,\n",
       " 'cannot': None,\n",
       " 'cant': None,\n",
       " 'case': None,\n",
       " 'cases': None,\n",
       " 'cause': None,\n",
       " 'causes': None,\n",
       " 'certain': None,\n",
       " 'certainly': None,\n",
       " 'changes': None,\n",
       " 'clear': None,\n",
       " 'clearly': None,\n",
       " 'co': None,\n",
       " 'com': None,\n",
       " 'come': None,\n",
       " 'comes': None,\n",
       " 'concerning': None,\n",
       " 'consequently': None,\n",
       " 'consider': None,\n",
       " 'considering': None,\n",
       " 'contain': None,\n",
       " 'containing': None,\n",
       " 'contains': None,\n",
       " 'corresponding': None,\n",
       " 'could': None,\n",
       " \"couldn't\": None,\n",
       " 'couldnt': None,\n",
       " 'course': None,\n",
       " 'currently': None,\n",
       " 'd': None,\n",
       " 'date': None,\n",
       " 'definitely': None,\n",
       " 'describe': None,\n",
       " 'described': None,\n",
       " 'despite': None,\n",
       " 'did': None,\n",
       " \"didn't\": None,\n",
       " 'differ': None,\n",
       " 'different': None,\n",
       " 'differently': None,\n",
       " 'discuss': None,\n",
       " 'do': None,\n",
       " 'does': None,\n",
       " \"doesn't\": None,\n",
       " 'doing': None,\n",
       " \"don't\": None,\n",
       " 'done': None,\n",
       " 'down': None,\n",
       " 'downed': None,\n",
       " 'downing': None,\n",
       " 'downs': None,\n",
       " 'downwards': None,\n",
       " 'due': None,\n",
       " 'during': None,\n",
       " 'e': None,\n",
       " 'each': None,\n",
       " 'early': None,\n",
       " 'ed': None,\n",
       " 'edu': None,\n",
       " 'effect': None,\n",
       " 'eg': None,\n",
       " 'eight': None,\n",
       " 'eighty': None,\n",
       " 'either': None,\n",
       " 'else': None,\n",
       " 'elsewhere': None,\n",
       " 'end': None,\n",
       " 'ended': None,\n",
       " 'ending': None,\n",
       " 'ends': None,\n",
       " 'enough': None,\n",
       " 'entirely': None,\n",
       " 'especially': None,\n",
       " 'et': None,\n",
       " 'et-al': None,\n",
       " 'etc': None,\n",
       " 'even': None,\n",
       " 'evenly': None,\n",
       " 'ever': None,\n",
       " 'every': None,\n",
       " 'everybody': None,\n",
       " 'everyone': None,\n",
       " 'everything': None,\n",
       " 'everywhere': None,\n",
       " 'ex': None,\n",
       " 'exactly': None,\n",
       " 'example': None,\n",
       " 'except': None,\n",
       " 'f': None,\n",
       " 'face': None,\n",
       " 'faces': None,\n",
       " 'fact': None,\n",
       " 'facts': None,\n",
       " 'far': None,\n",
       " 'felt': None,\n",
       " 'few': None,\n",
       " 'ff': None,\n",
       " 'fifth': None,\n",
       " 'find': None,\n",
       " 'finds': None,\n",
       " 'first': None,\n",
       " 'five': None,\n",
       " 'fix': None,\n",
       " 'followed': None,\n",
       " 'following': None,\n",
       " 'follows': None,\n",
       " 'for': None,\n",
       " 'former': None,\n",
       " 'formerly': None,\n",
       " 'forth': None,\n",
       " 'found': None,\n",
       " 'four': None,\n",
       " 'from': None,\n",
       " 'full': None,\n",
       " 'fully': None,\n",
       " 'further': None,\n",
       " 'furthered': None,\n",
       " 'furthering': None,\n",
       " 'furthermore': None,\n",
       " 'furthers': None,\n",
       " 'g': None,\n",
       " 'gave': None,\n",
       " 'general': None,\n",
       " 'generally': None,\n",
       " 'get': None,\n",
       " 'gets': None,\n",
       " 'getting': None,\n",
       " 'give': None,\n",
       " 'given': None,\n",
       " 'gives': None,\n",
       " 'giving': None,\n",
       " 'go': None,\n",
       " 'goes': None,\n",
       " 'going': None,\n",
       " 'gone': None,\n",
       " 'good': None,\n",
       " 'goods': None,\n",
       " 'got': None,\n",
       " 'gotten': None,\n",
       " 'great': None,\n",
       " 'greater': None,\n",
       " 'greatest': None,\n",
       " 'greetings': None,\n",
       " 'group': None,\n",
       " 'grouped': None,\n",
       " 'grouping': None,\n",
       " 'groups': None,\n",
       " 'h': None,\n",
       " 'had': None,\n",
       " \"hadn't\": None,\n",
       " 'happens': None,\n",
       " 'hardly': None,\n",
       " 'has': None,\n",
       " \"hasn't\": None,\n",
       " 'have': None,\n",
       " \"haven't\": None,\n",
       " 'having': None,\n",
       " 'he': None,\n",
       " \"he's\": None,\n",
       " 'hed': None,\n",
       " 'hello': None,\n",
       " 'help': None,\n",
       " 'hence': None,\n",
       " 'her': None,\n",
       " 'here': None,\n",
       " \"here's\": None,\n",
       " 'hereafter': None,\n",
       " 'hereby': None,\n",
       " 'herein': None,\n",
       " 'heres': None,\n",
       " 'hereupon': None,\n",
       " 'hers': None,\n",
       " 'herself': None,\n",
       " 'hes': None,\n",
       " 'hi': None,\n",
       " 'hid': None,\n",
       " 'high': None,\n",
       " 'higher': None,\n",
       " 'highest': None,\n",
       " 'him': None,\n",
       " 'himself': None,\n",
       " 'his': None,\n",
       " 'hither': None,\n",
       " 'home': None,\n",
       " 'hopefully': None,\n",
       " 'how': None,\n",
       " 'howbeit': None,\n",
       " 'however': None,\n",
       " 'hundred': None,\n",
       " 'i': None,\n",
       " \"i'd\": None,\n",
       " \"i'll\": None,\n",
       " \"i'm\": None,\n",
       " \"i've\": None,\n",
       " 'id': None,\n",
       " 'ie': None,\n",
       " 'if': None,\n",
       " 'ignored': None,\n",
       " 'im': None,\n",
       " 'immediate': None,\n",
       " 'immediately': None,\n",
       " 'importance': None,\n",
       " 'important': None,\n",
       " 'in': None,\n",
       " 'inasmuch': None,\n",
       " 'inc': None,\n",
       " 'include': None,\n",
       " 'indeed': None,\n",
       " 'index': None,\n",
       " 'indicate': None,\n",
       " 'indicated': None,\n",
       " 'indicates': None,\n",
       " 'information': None,\n",
       " 'inner': None,\n",
       " 'insofar': None,\n",
       " 'instead': None,\n",
       " 'interest': None,\n",
       " 'interested': None,\n",
       " 'interesting': None,\n",
       " 'interests': None,\n",
       " 'into': None,\n",
       " 'invention': None,\n",
       " 'inward': None,\n",
       " 'is': None,\n",
       " \"isn't\": None,\n",
       " 'it': None,\n",
       " \"it'd\": None,\n",
       " \"it'll\": None,\n",
       " \"it's\": None,\n",
       " 'itd': None,\n",
       " 'its': None,\n",
       " 'itself': None,\n",
       " 'j': None,\n",
       " 'just': None,\n",
       " 'k': None,\n",
       " 'keep': None,\n",
       " 'keeps': None,\n",
       " 'kept': None,\n",
       " 'keys': None,\n",
       " 'kg': None,\n",
       " 'kind': None,\n",
       " 'km': None,\n",
       " 'knew': None,\n",
       " 'know': None,\n",
       " 'known': None,\n",
       " 'knows': None,\n",
       " 'l': None,\n",
       " 'large': None,\n",
       " 'largely': None,\n",
       " 'last': None,\n",
       " 'lately': None,\n",
       " 'later': None,\n",
       " 'latest': None,\n",
       " 'latter': None,\n",
       " 'latterly': None,\n",
       " 'least': None,\n",
       " 'less': None,\n",
       " 'lest': None,\n",
       " 'let': None,\n",
       " \"let's\": None,\n",
       " 'lets': None,\n",
       " 'like': None,\n",
       " 'liked': None,\n",
       " 'likely': None,\n",
       " 'line': None,\n",
       " 'little': None,\n",
       " 'long': None,\n",
       " 'longer': None,\n",
       " 'longest': None,\n",
       " 'look': None,\n",
       " 'looking': None,\n",
       " 'looks': None,\n",
       " 'ltd': None,\n",
       " 'm': None,\n",
       " 'made': None,\n",
       " 'mainly': None,\n",
       " 'make': None,\n",
       " 'makes': None,\n",
       " 'making': None,\n",
       " 'man': None,\n",
       " 'many': None,\n",
       " 'may': None,\n",
       " 'maybe': None,\n",
       " 'me': None,\n",
       " 'mean': None,\n",
       " 'means': None,\n",
       " 'meantime': None,\n",
       " 'meanwhile': None,\n",
       " 'member': None,\n",
       " 'members': None,\n",
       " 'men': None,\n",
       " 'merely': None,\n",
       " 'mg': None,\n",
       " 'might': None,\n",
       " 'million': None,\n",
       " 'miss': None,\n",
       " 'ml': None,\n",
       " 'more': None,\n",
       " 'moreover': None,\n",
       " 'most': None,\n",
       " 'mostly': None,\n",
       " 'mr': None,\n",
       " 'mrs': None,\n",
       " 'much': None,\n",
       " 'mug': None,\n",
       " 'must': None,\n",
       " 'my': None,\n",
       " 'myself': None,\n",
       " 'n': None,\n",
       " \"n't\": None,\n",
       " 'na': None,\n",
       " 'name': None,\n",
       " 'namely': None,\n",
       " 'nay': None,\n",
       " 'nd': None,\n",
       " 'near': None,\n",
       " 'nearly': None,\n",
       " 'necessarily': None,\n",
       " 'necessary': None,\n",
       " 'need': None,\n",
       " 'needed': None,\n",
       " 'needing': None,\n",
       " 'needs': None,\n",
       " 'neither': None,\n",
       " 'never': None,\n",
       " 'nevertheless': None,\n",
       " 'new': None,\n",
       " 'newer': None,\n",
       " 'newest': None,\n",
       " 'next': None,\n",
       " 'nine': None,\n",
       " 'ninety': None,\n",
       " 'no': None,\n",
       " 'nobody': None,\n",
       " 'non': None,\n",
       " 'none': None,\n",
       " 'nonetheless': None,\n",
       " 'noone': None,\n",
       " 'nor': None,\n",
       " 'normally': None,\n",
       " 'nos': None,\n",
       " 'not': None,\n",
       " 'noted': None,\n",
       " 'nothing': None,\n",
       " 'novel': None,\n",
       " 'now': None,\n",
       " 'nowhere': None,\n",
       " 'number': None,\n",
       " 'numbers': None,\n",
       " 'o': None,\n",
       " 'obtain': None,\n",
       " 'obtained': None,\n",
       " 'obviously': None,\n",
       " 'of': None,\n",
       " 'off': None,\n",
       " 'often': None,\n",
       " 'oh': None,\n",
       " 'ok': None,\n",
       " 'okay': None,\n",
       " 'old': None,\n",
       " 'older': None,\n",
       " 'oldest': None,\n",
       " 'omitted': None,\n",
       " 'on': None,\n",
       " 'once': None,\n",
       " 'one': None,\n",
       " 'ones': None,\n",
       " 'only': None,\n",
       " 'onto': None,\n",
       " 'open': None,\n",
       " 'opened': None,\n",
       " 'opening': None,\n",
       " 'opens': None,\n",
       " 'or': None,\n",
       " 'ord': None,\n",
       " 'order': None,\n",
       " 'ordered': None,\n",
       " 'ordering': None,\n",
       " 'orders': None,\n",
       " 'other': None,\n",
       " 'others': None,\n",
       " 'otherwise': None,\n",
       " 'ought': None,\n",
       " 'our': None,\n",
       " 'ours': None,\n",
       " 'ourselves': None,\n",
       " 'out': None,\n",
       " 'outside': None,\n",
       " 'over': None,\n",
       " 'overall': None,\n",
       " 'owing': None,\n",
       " 'own': None,\n",
       " 'p': None,\n",
       " 'page': None,\n",
       " 'pages': None,\n",
       " 'part': None,\n",
       " 'parted': None,\n",
       " 'particular': None,\n",
       " 'particularly': None,\n",
       " 'parting': None,\n",
       " 'parts': None,\n",
       " 'past': None,\n",
       " 'per': None,\n",
       " 'perhaps': None,\n",
       " 'place': None,\n",
       " 'placed': None,\n",
       " 'places': None,\n",
       " 'please': None,\n",
       " 'plus': None,\n",
       " 'point': None,\n",
       " 'pointed': None,\n",
       " 'pointing': None,\n",
       " 'points': None,\n",
       " 'poorly': None,\n",
       " 'possible': None,\n",
       " 'possibly': None,\n",
       " 'potentially': None,\n",
       " 'pp': None,\n",
       " 'predominantly': None,\n",
       " 'present': None,\n",
       " 'presented': None,\n",
       " 'presenting': None,\n",
       " 'presents': None,\n",
       " 'presumably': None,\n",
       " 'previously': None,\n",
       " 'primarily': None,\n",
       " 'probably': None,\n",
       " 'problem': None,\n",
       " 'problems': None,\n",
       " 'promptly': None,\n",
       " 'proud': None,\n",
       " 'provides': None,\n",
       " 'put': None,\n",
       " 'puts': None,\n",
       " 'q': None,\n",
       " 'que': None,\n",
       " 'quickly': None,\n",
       " 'quite': None,\n",
       " 'qv': None,\n",
       " 'r': None,\n",
       " 'ran': None,\n",
       " 'rather': None,\n",
       " 'rd': None,\n",
       " 're': None,\n",
       " 'readily': None,\n",
       " 'really': None,\n",
       " 'reasonably': None,\n",
       " 'recent': None,\n",
       " 'recently': None,\n",
       " 'ref': None,\n",
       " 'refs': None,\n",
       " 'regarding': None,\n",
       " 'regardless': None,\n",
       " 'regards': None,\n",
       " 'related': None,\n",
       " 'relatively': None,\n",
       " 'research': None,\n",
       " 'respectively': None,\n",
       " 'resulted': None,\n",
       " 'resulting': None,\n",
       " 'results': None,\n",
       " 'right': None,\n",
       " 'room': None,\n",
       " 'rooms': None,\n",
       " 'run': None,\n",
       " 's': None,\n",
       " 'said': None,\n",
       " 'same': None,\n",
       " 'saw': None,\n",
       " 'say': None,\n",
       " 'saying': None,\n",
       " 'says': None,\n",
       " 'sec': None,\n",
       " 'second': None,\n",
       " 'secondly': None,\n",
       " 'seconds': None,\n",
       " 'section': None,\n",
       " 'see': None,\n",
       " 'seeing': None,\n",
       " 'seem': None,\n",
       " 'seemed': None,\n",
       " 'seeming': None,\n",
       " 'seems': None,\n",
       " 'seen': None,\n",
       " 'sees': None,\n",
       " 'self': None,\n",
       " 'selves': None,\n",
       " 'sensible': None,\n",
       " 'sent': None,\n",
       " 'serious': None,\n",
       " 'seriously': None,\n",
       " 'seven': None,\n",
       " 'several': None,\n",
       " 'shall': None,\n",
       " 'she': None,\n",
       " \"she'll\": None,\n",
       " 'shed': None,\n",
       " 'shes': None,\n",
       " 'should': None,\n",
       " \"shouldn't\": None,\n",
       " 'show': None,\n",
       " 'showed': None,\n",
       " 'showing': None,\n",
       " 'shown': None,\n",
       " 'showns': None,\n",
       " 'shows': None,\n",
       " 'side': None,\n",
       " 'sides': None,\n",
       " 'significant': None,\n",
       " 'significantly': None,\n",
       " 'similar': None,\n",
       " 'similarly': None,\n",
       " 'since': None,\n",
       " 'six': None,\n",
       " 'slightly': None,\n",
       " 'small': None,\n",
       " 'smaller': None,\n",
       " 'smallest': None,\n",
       " 'so': None,\n",
       " 'some': None,\n",
       " 'somebody': None,\n",
       " 'somehow': None,\n",
       " 'someone': None,\n",
       " 'somethan': None,\n",
       " 'something': None,\n",
       " 'sometime': None,\n",
       " 'sometimes': None,\n",
       " 'somewhat': None,\n",
       " 'somewhere': None,\n",
       " 'soon': None,\n",
       " 'sorry': None,\n",
       " 'specifically': None,\n",
       " 'specified': None,\n",
       " 'specify': None,\n",
       " 'specifying': None,\n",
       " 'state': None,\n",
       " 'states': None,\n",
       " 'still': None,\n",
       " 'stop': None,\n",
       " 'strongly': None,\n",
       " 'sub': None,\n",
       " 'substantially': None,\n",
       " 'successfully': None,\n",
       " 'such': None,\n",
       " 'sufficiently': None,\n",
       " 'suggest': None,\n",
       " 'sup': None,\n",
       " 'sure': None,\n",
       " 't': None,\n",
       " \"t's\": None,\n",
       " 'take': None,\n",
       " 'taken': None,\n",
       " 'taking': None,\n",
       " 'tell': None,\n",
       " 'tends': None,\n",
       " 'th': None,\n",
       " 'than': None,\n",
       " 'thank': None,\n",
       " 'thanks': None,\n",
       " 'thanx': None,\n",
       " 'that': None,\n",
       " \"that'll\": None,\n",
       " \"that's\": None,\n",
       " \"that've\": None,\n",
       " 'thats': None,\n",
       " 'the': None,\n",
       " 'their': None,\n",
       " 'theirs': None,\n",
       " 'them': None,\n",
       " 'themselves': None,\n",
       " 'then': None,\n",
       " 'thence': None,\n",
       " 'there': None,\n",
       " \"there'll\": None,\n",
       " \"there's\": None,\n",
       " \"there've\": None,\n",
       " 'thereafter': None,\n",
       " 'thereby': None,\n",
       " 'thered': None,\n",
       " 'therefore': None,\n",
       " 'therein': None,\n",
       " 'thereof': None,\n",
       " 'therere': None,\n",
       " 'theres': None,\n",
       " 'thereto': None,\n",
       " 'thereupon': None,\n",
       " 'these': None,\n",
       " 'they': None,\n",
       " \"they'd\": None,\n",
       " \"they'll\": None,\n",
       " \"they're\": None,\n",
       " \"they've\": None,\n",
       " 'theyd': None,\n",
       " 'theyre': None,\n",
       " 'thing': None,\n",
       " 'things': None,\n",
       " 'think': None,\n",
       " 'thinks': None,\n",
       " 'third': None,\n",
       " 'this': None,\n",
       " 'thorough': None,\n",
       " 'thoroughly': None,\n",
       " 'those': None,\n",
       " 'thou': None,\n",
       " 'though': None,\n",
       " 'thoughh': None,\n",
       " 'thought': None,\n",
       " 'thoughts': None,\n",
       " 'thousand': None,\n",
       " 'three': None,\n",
       " 'throug': None,\n",
       " 'through': None,\n",
       " 'throughout': None,\n",
       " 'thru': None,\n",
       " 'thus': None,\n",
       " 'til': None,\n",
       " 'tip': None,\n",
       " 'to': None,\n",
       " 'today': None,\n",
       " 'together': None,\n",
       " 'too': None,\n",
       " 'took': None,\n",
       " 'toward': None,\n",
       " 'towards': None,\n",
       " 'tried': None,\n",
       " 'tries': None,\n",
       " 'truly': None,\n",
       " 'try': None,\n",
       " 'trying': None,\n",
       " 'ts': None,\n",
       " 'turn': None,\n",
       " 'turned': None,\n",
       " 'turning': None,\n",
       " 'turns': None,\n",
       " 'twice': None,\n",
       " 'two': None,\n",
       " 'u': None,\n",
       " 'un': None,\n",
       " 'under': None,\n",
       " 'unfortunately': None,\n",
       " 'unless': None,\n",
       " 'unlike': None,\n",
       " 'unlikely': None,\n",
       " 'until': None,\n",
       " 'unto': None,\n",
       " 'up': None,\n",
       " 'upon': None,\n",
       " 'ups': None,\n",
       " 'us': None,\n",
       " 'use': None,\n",
       " 'used': None,\n",
       " 'useful': None,\n",
       " 'usefully': None,\n",
       " 'usefulness': None,\n",
       " 'uses': None,\n",
       " 'using': None,\n",
       " 'usually': None,\n",
       " 'uucp': None,\n",
       " 'v': None,\n",
       " 'value': None,\n",
       " 'various': None,\n",
       " 'very': None,\n",
       " 'via': None,\n",
       " 'viz': None,\n",
       " 'vol': None,\n",
       " 'vols': None,\n",
       " 'vs': None,\n",
       " 'w': None,\n",
       " 'want': None,\n",
       " 'wanted': None,\n",
       " 'wanting': None,\n",
       " 'wants': None,\n",
       " 'was': None,\n",
       " \"wasn't\": None,\n",
       " 'way': None,\n",
       " 'ways': None,\n",
       " 'we': None,\n",
       " \"we'd\": None,\n",
       " \"we'll\": None,\n",
       " \"we're\": None,\n",
       " \"we've\": None,\n",
       " 'wed': None,\n",
       " 'welcome': None,\n",
       " 'well': None,\n",
       " 'wells': None,\n",
       " 'went': None,\n",
       " 'were': None,\n",
       " \"weren't\": None,\n",
       " 'what': None,\n",
       " \"what'll\": None,\n",
       " \"what's\": None,\n",
       " 'whatever': None,\n",
       " 'whats': None,\n",
       " 'when': None,\n",
       " 'whence': None,\n",
       " 'whenever': None,\n",
       " 'where': None,\n",
       " \"where's\": None,\n",
       " 'whereafter': None,\n",
       " 'whereas': None,\n",
       " 'whereby': None,\n",
       " 'wherein': None,\n",
       " 'wheres': None,\n",
       " 'whereupon': None,\n",
       " 'wherever': None,\n",
       " 'whether': None,\n",
       " 'which': None,\n",
       " 'while': None,\n",
       " 'whim': None,\n",
       " 'whither': None,\n",
       " 'who': None,\n",
       " \"who'll\": None,\n",
       " \"who's\": None,\n",
       " 'whod': None,\n",
       " 'whoever': None,\n",
       " 'whole': None,\n",
       " 'whom': None,\n",
       " 'whomever': None,\n",
       " 'whos': None,\n",
       " 'whose': None,\n",
       " 'why': None,\n",
       " 'widely': None,\n",
       " 'will': None,\n",
       " 'willing': None,\n",
       " 'wish': None,\n",
       " 'with': None,\n",
       " 'within': None,\n",
       " 'without': None,\n",
       " \"won't\": None,\n",
       " 'wonder': None,\n",
       " 'words': None,\n",
       " 'work': None,\n",
       " 'worked': None,\n",
       " 'working': None,\n",
       " 'works': None,\n",
       " 'world': None,\n",
       " 'would': None,\n",
       " \"wouldn't\": None,\n",
       " 'www': None,\n",
       " 'x': None,\n",
       " 'y': None,\n",
       " 'year': None,\n",
       " 'years': None,\n",
       " 'yes': None,\n",
       " 'yet': None,\n",
       " 'you': None,\n",
       " \"you'd\": None,\n",
       " \"you'll\": None,\n",
       " \"you're\": None,\n",
       " \"you've\": None,\n",
       " 'youd': None,\n",
       " 'young': None,\n",
       " 'younger': None,\n",
       " 'youngest': None,\n",
       " 'your': None,\n",
       " 'youre': None,\n",
       " 'yours': None,\n",
       " 'yourself': None,\n",
       " 'yourselves': None,\n",
       " 'z': None,\n",
       " 'zero': None,\n",
       " 'zt': None,\n",
       " 'zz': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去掉停用词数据\n",
      "\n",
      "-----我是分割线-----\n",
      "\n",
      "['classic', 'war', 'worlds', 'timothy', 'hines', 'entertaining', 'film', 'effort', 'lengths', 'faithfully', 'recreate', 'classic', 'book', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'standard', 'predictable', 'hollywood', 'fare', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'movie', 'envision', 'amateur', 'critics', 'criticize', 'rate', 'movie', 'bases', 'entertained', 'people', 'agree', 'critics', 'enjoyed', 'effort', 'hines', 'faithful', 'classic', 'entertaining', 'easy', 'overlook', 'critics', 'perceive', 'shortcomings']\n"
     ]
    }
   ],
   "source": [
    "words_nostop = [w for w in words if w not in stopwords]\n",
    "#words_nostop = [w for w in words if w not in stopwords.words('english')] # nltk.corpus.stopwords\n",
    "display(words_nostop, '去掉停用词数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_stopwords = set(stopwords.words('english')) # nltk.corpus.stopwords\n",
    "eng_stopwords = set(stopwords)\n",
    "\n",
    "def clean_text(text):\n",
    "    # HTML标记去除\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    # 移除标点\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # 最小化，分词\n",
    "    words = text.lower().split()\n",
    "    # 去掉停用词\n",
    "    words = [w for w in words if w not in eng_stopwords]\n",
    "    # 重新组成新的句子\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classic war worlds timothy hines entertaining film effort lengths faithfully recreate classic book hines succeeds watched film appreciated standard predictable hollywood fare spielberg version tom cruise slightest resemblance book movie envision amateur critics criticize rate movie bases entertained people agree critics enjoyed effort hines faithful classic entertaining easy overlook critics perceive shortcomings'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(raw_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗数据添加到dataframe里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff moment mj ve started listening music wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell investors ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>assumed praised film filmed opera didn read do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  stuff moment mj ve started listening music wat...  \n",
       "1  classic war worlds timothy hines entertaining ...  \n",
       "2  film starts manager nicholas bell investors ro...  \n",
       "3  assumed praised film filmed opera didn read do...  \n",
       "4  superbly trashy wondrously unpretentious explo...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'] = df.review.apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ith stuff moment mj started listening music, watching odd documentary there, watched wiz watched moonwalker again. insight guy cool eighties mind guilty innocent. moonwalker biography, feature film remember cinema originally released. subtle messages mj's feeling press obvious message drugs bad m'kay.visually impressive michael jackson remotely mj hate boring. call mj egotist consenting movie mj fans fans true nice him.the actual feature film bit finally starts 20 minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord. mj dead bad me. mj overheard plans? nah, joe pesci's character ranted people supplying drugs dunno, hates mj's music.lots cool mj car robot speed demon sequence. also, director patience saint filming kiddy bad sequence directors hate kid bunch performing complex dance scene.bottom line, movie people mj level (which people). not, stay away. wholesome message ironically mj's bestest buddy movie girl! michael jackson talented people grace planet guilty? well, attention subject....hmmm people closed doors, fact. extremely nice stupid guy sickest liars. hope latter.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抽取bag of words特征(用sklearn的CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "train_data_features = vectorizer.fit_transform(df.clean_review).toarray()\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(train_data_features, df.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在训练集上做个predict看看效果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12500,     0],\n",
       "       [    0, 12500]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df.sentiment, forest.predict(train_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 删除不用的占内容变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取测试数据进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    }
   ],
   "source": [
    "test_file_path = os.path.join(BASE_PATH, 'data/testData.tsv')\n",
    "\n",
    "df = pd.read_csv(test_file_path, sep='\\t', escapechar='\\\\')\n",
    "\n",
    "print('Number of reviews: {}'.format(len(df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在测试集上应用clean_text，同样的方式清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review'] = df.review.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "      <td>naturally film main themes mortality nostalgia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "      <td>movie disaster disaster film action scenes mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "      <td>movie kids tonight child loved kid excitement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "      <td>afraid dark left impression screenplays writte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "      <td>accurate depiction time mob life filmed jersey...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review  \\\n",
       "0  12311_10  Naturally in a film who's main themes are of m...   \n",
       "1    8348_2  This movie is a disaster within a disaster fil...   \n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...   \n",
       "3    7186_2  Afraid of the Dark left me with the impression...   \n",
       "4   12128_7  A very accurate depiction of small time mob li...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  naturally film main themes mortality nostalgia...  \n",
       "1  movie disaster disaster film action scenes mea...  \n",
       "2  movie kids tonight child loved kid excitement ...  \n",
       "3  afraid dark left impression screenplays writte...  \n",
       "4  accurate depiction time mob life filmed jersey...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在测试集上，对clean_review按照训练集生成的vectorizer，出现最高的5000个词进行自动编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features = vectorizer.transform(df.clean_review).toarray()\n",
    "test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = forest.predict(test_data_features)\n",
    "output = pd.DataFrame({'id': df.id, 'sentiment': result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment\n",
       "0  12311_10          1\n",
       "1    8348_2          0\n",
       "2    5828_4          1\n",
       "3    7186_2          1\n",
       "4   12128_7          1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(os.path.join(BASE_PATH, 'data/Bag_of_Words_model_submission.tsv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del test_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='word2vec'>word2vec训练词向量</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk.data\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    \n",
    "    data_file = os.path.join(BASE_PATH, 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    \n",
    "    print('Number of reviews: {} '.format(len(df)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取unlabeled_train数据\n",
    "用于训练生成word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 50000 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999_0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45057_0</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15561_0</td>\n",
       "      <td>Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7161_0</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43971_0</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             review\n",
       "0   9999_0  Watching Time Chasers, it obvious that it was ...\n",
       "1  45057_0  I saw this film about 20 years ago and remembe...\n",
       "2  15561_0  Minor Spoilers<br /><br />In New York, Joan Ba...\n",
       "3   7161_0  I went to see this film with a great deal of e...\n",
       "4  43971_0  Yes, I agree with everyone on this site this m..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('unlabeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数据review做和上面一样的预处理\n",
    "稍微有点不一样的是，我们留个候选，可以去除停用词，也可以不去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_stopwords = set(stopwords.words('english'))\n",
    "eng_stopwords = {}.fromkeys([line.strip() for line in open('./stopwords.txt')])\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    '''文本预处理的函数'''\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载nltk.data中加载英文的划分句子的模型\n",
    "tokenizers/punkt/ 这里面有好多训练好的模型，只能划分成句子，不能划分成单词\n",
    "老外写文字，单词之间都留空格，split()函数默认是空格，python是他们自己设计的，都是方便了他们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sadly, more downs than ups.\n",
      "The plot was pretty descent.\n"
     ]
    }
   ],
   "source": [
    "# import nltk.data \n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# data =  'Sadly, more downs than ups. The plot was pretty descent.'\n",
    "# for row in tokenizer.tokenize(data):\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_call_counts(f):\n",
    "    '''定义装饰器'''\n",
    "    n = 0 \n",
    "    def wrapped(*args, **kwargs):\n",
    "        nonlocal n\n",
    "        n += 1\n",
    "        if n % 1000 == 1:\n",
    "            print('method {} called {} times'.format(f.__name__, n))\n",
    "        return f(*args, **kwargs)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@print_call_counts\n",
    "def split_sentences(review):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = [clean_text(s) for s in raw_sentences if s]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 1001 times\n",
      "method split_sentences called 2001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 3001 times\n",
      "method split_sentences called 4001 times\n",
      "method split_sentences called 5001 times\n",
      "method split_sentences called 6001 times\n",
      "method split_sentences called 7001 times\n",
      "method split_sentences called 8001 times\n",
      "method split_sentences called 9001 times\n",
      "method split_sentences called 10001 times\n",
      "method split_sentences called 11001 times\n",
      "method split_sentences called 12001 times\n",
      "method split_sentences called 13001 times\n",
      "method split_sentences called 14001 times\n",
      "method split_sentences called 15001 times\n",
      "method split_sentences called 16001 times\n",
      "method split_sentences called 17001 times\n",
      "method split_sentences called 18001 times\n",
      "method split_sentences called 19001 times\n",
      "method split_sentences called 20001 times\n",
      "method split_sentences called 21001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 22001 times\n",
      "method split_sentences called 23001 times\n",
      "method split_sentences called 24001 times\n",
      "method split_sentences called 25001 times\n",
      "method split_sentences called 26001 times\n",
      "method split_sentences called 27001 times\n",
      "method split_sentences called 28001 times\n",
      "method split_sentences called 29001 times\n",
      "method split_sentences called 30001 times\n",
      "method split_sentences called 31001 times\n",
      "method split_sentences called 32001 times\n",
      "method split_sentences called 33001 times\n",
      "method split_sentences called 34001 times\n",
      "method split_sentences called 35001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 36001 times\n",
      "method split_sentences called 37001 times\n",
      "method split_sentences called 38001 times\n",
      "method split_sentences called 39001 times\n",
      "method split_sentences called 40001 times\n",
      "method split_sentences called 41001 times\n",
      "method split_sentences called 42001 times\n",
      "method split_sentences called 43001 times\n",
      "method split_sentences called 44001 times\n",
      "method split_sentences called 45001 times\n",
      "method split_sentences called 46001 times\n",
      "method split_sentences called 47001 times\n",
      "method split_sentences called 48001 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method split_sentences called 49001 times\n",
      "CPU times: user 6min 20s, sys: 15 s, total: 6min 36s\n",
      "Wall time: 6min 38s\n",
      "50000 reviews -> 537851 sentences\n"
     ]
    }
   ],
   "source": [
    "%time sentences = sum(df.review.apply(split_sentences), [])\n",
    "print(\"{} reviews -> {} sentences\".format(len(df), len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "成功地通过nltk.data.load('tokenizers/punkt/english.pickle')模型，将5万条影评划分成537851条句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用gensim训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定词向量训练的参数\n",
    "num_features = 300 # Word Vector dimensionality\n",
    "min_word_count = 40 # Minimum word count\n",
    "num_workers = 4 # Number of threads to run in parrallel\n",
    "context = 10 # Context window size\n",
    "downsampling = 1e-3 # Downsample setting for frequent words 负例采样的\n",
    "\n",
    "model_name = \"{}features_{}minwords_{}context.model\".format(num_features, min_word_count, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainging model...\n"
     ]
    }
   ],
   "source": [
    "print('Trainging model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-11 13:45:23,789: INFO: collecting all words and their counts\n",
      "2018-09-11 13:45:23,790: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-09-11 13:45:23,843: INFO: PROGRESS: at sentence #10000, processed 225072 words, keeping 17237 word types\n",
      "2018-09-11 13:45:23,905: INFO: PROGRESS: at sentence #20000, processed 443536 words, keeping 24570 word types\n",
      "2018-09-11 13:45:23,952: INFO: PROGRESS: at sentence #30000, processed 666343 words, keeping 29785 word types\n",
      "2018-09-11 13:45:23,998: INFO: PROGRESS: at sentence #40000, processed 886903 words, keeping 33939 word types\n",
      "2018-09-11 13:45:24,046: INFO: PROGRESS: at sentence #50000, processed 1103863 words, keeping 37503 word types\n",
      "2018-09-11 13:45:24,111: INFO: PROGRESS: at sentence #60000, processed 1327231 words, keeping 40738 word types\n",
      "2018-09-11 13:45:24,171: INFO: PROGRESS: at sentence #70000, processed 1550828 words, keeping 43603 word types\n",
      "2018-09-11 13:45:24,226: INFO: PROGRESS: at sentence #80000, processed 1772824 words, keeping 46155 word types\n",
      "2018-09-11 13:45:24,278: INFO: PROGRESS: at sentence #90000, processed 1987492 words, keeping 48328 word types\n",
      "2018-09-11 13:45:24,327: INFO: PROGRESS: at sentence #100000, processed 2210772 words, keeping 50551 word types\n",
      "2018-09-11 13:45:24,376: INFO: PROGRESS: at sentence #110000, processed 2435496 words, keeping 52762 word types\n",
      "2018-09-11 13:45:24,423: INFO: PROGRESS: at sentence #120000, processed 2658449 words, keeping 54893 word types\n",
      "2018-09-11 13:45:24,468: INFO: PROGRESS: at sentence #130000, processed 2877962 words, keeping 56598 word types\n",
      "2018-09-11 13:45:24,518: INFO: PROGRESS: at sentence #140000, processed 3098235 words, keeping 58352 word types\n",
      "2018-09-11 13:45:24,570: INFO: PROGRESS: at sentence #150000, processed 3315370 words, keeping 60013 word types\n",
      "2018-09-11 13:45:24,622: INFO: PROGRESS: at sentence #160000, processed 3536039 words, keeping 61691 word types\n",
      "2018-09-11 13:45:24,674: INFO: PROGRESS: at sentence #170000, processed 3758385 words, keeping 63292 word types\n",
      "2018-09-11 13:45:24,721: INFO: PROGRESS: at sentence #180000, processed 3979413 words, keeping 64846 word types\n",
      "2018-09-11 13:45:24,766: INFO: PROGRESS: at sentence #190000, processed 4203546 words, keeping 66403 word types\n",
      "2018-09-11 13:45:24,811: INFO: PROGRESS: at sentence #200000, processed 4429481 words, keeping 67924 word types\n",
      "2018-09-11 13:45:24,858: INFO: PROGRESS: at sentence #210000, processed 4652920 words, keeping 69248 word types\n",
      "2018-09-11 13:45:24,903: INFO: PROGRESS: at sentence #220000, processed 4870835 words, keeping 70567 word types\n",
      "2018-09-11 13:45:24,947: INFO: PROGRESS: at sentence #230000, processed 5093104 words, keeping 71912 word types\n",
      "2018-09-11 13:45:24,990: INFO: PROGRESS: at sentence #240000, processed 5311435 words, keeping 73234 word types\n",
      "2018-09-11 13:45:25,034: INFO: PROGRESS: at sentence #250000, processed 5532195 words, keeping 74486 word types\n",
      "2018-09-11 13:45:25,079: INFO: PROGRESS: at sentence #260000, processed 5751629 words, keeping 75693 word types\n",
      "2018-09-11 13:45:25,130: INFO: PROGRESS: at sentence #270000, processed 5973493 words, keeping 76829 word types\n",
      "2018-09-11 13:45:25,179: INFO: PROGRESS: at sentence #280000, processed 6191000 words, keeping 77953 word types\n",
      "2018-09-11 13:45:25,229: INFO: PROGRESS: at sentence #290000, processed 6415980 words, keeping 79135 word types\n",
      "2018-09-11 13:45:25,281: INFO: PROGRESS: at sentence #300000, processed 6634726 words, keeping 80229 word types\n",
      "2018-09-11 13:45:25,330: INFO: PROGRESS: at sentence #310000, processed 6857605 words, keeping 81308 word types\n",
      "2018-09-11 13:45:25,379: INFO: PROGRESS: at sentence #320000, processed 7077123 words, keeping 82421 word types\n",
      "2018-09-11 13:45:25,426: INFO: PROGRESS: at sentence #330000, processed 7298667 words, keeping 83509 word types\n",
      "2018-09-11 13:45:25,471: INFO: PROGRESS: at sentence #340000, processed 7516302 words, keeping 84445 word types\n",
      "2018-09-11 13:45:25,515: INFO: PROGRESS: at sentence #350000, processed 7735101 words, keeping 85566 word types\n",
      "2018-09-11 13:45:25,558: INFO: PROGRESS: at sentence #360000, processed 7956254 words, keeping 86544 word types\n",
      "2018-09-11 13:45:25,605: INFO: PROGRESS: at sentence #370000, processed 8177574 words, keeping 87489 word types\n",
      "2018-09-11 13:45:25,652: INFO: PROGRESS: at sentence #380000, processed 8395550 words, keeping 88515 word types\n",
      "2018-09-11 13:45:25,699: INFO: PROGRESS: at sentence #390000, processed 8616518 words, keeping 89500 word types\n",
      "2018-09-11 13:45:25,747: INFO: PROGRESS: at sentence #400000, processed 8835616 words, keeping 90470 word types\n",
      "2018-09-11 13:45:25,797: INFO: PROGRESS: at sentence #410000, processed 9055384 words, keeping 91344 word types\n",
      "2018-09-11 13:45:25,849: INFO: PROGRESS: at sentence #420000, processed 9276296 words, keeping 92245 word types\n",
      "2018-09-11 13:45:25,900: INFO: PROGRESS: at sentence #430000, processed 9494459 words, keeping 93176 word types\n",
      "2018-09-11 13:45:25,949: INFO: PROGRESS: at sentence #440000, processed 9719312 words, keeping 94119 word types\n",
      "2018-09-11 13:45:25,995: INFO: PROGRESS: at sentence #450000, processed 9936915 words, keeping 94980 word types\n",
      "2018-09-11 13:45:26,042: INFO: PROGRESS: at sentence #460000, processed 10160053 words, keeping 95781 word types\n",
      "2018-09-11 13:45:26,087: INFO: PROGRESS: at sentence #470000, processed 10380740 words, keeping 96637 word types\n",
      "2018-09-11 13:45:26,132: INFO: PROGRESS: at sentence #480000, processed 10599172 words, keeping 97471 word types\n",
      "2018-09-11 13:45:26,175: INFO: PROGRESS: at sentence #490000, processed 10816559 words, keeping 98279 word types\n",
      "2018-09-11 13:45:26,218: INFO: PROGRESS: at sentence #500000, processed 11032175 words, keeping 99064 word types\n",
      "2018-09-11 13:45:26,262: INFO: PROGRESS: at sentence #510000, processed 11254508 words, keeping 99930 word types\n",
      "2018-09-11 13:45:26,307: INFO: PROGRESS: at sentence #520000, processed 11481357 words, keeping 100836 word types\n",
      "2018-09-11 13:45:26,355: INFO: PROGRESS: at sentence #530000, processed 11704018 words, keeping 101618 word types\n",
      "2018-09-11 13:45:26,392: INFO: collected 102304 word types from a corpus of 11877522 raw words and 537851 sentences\n",
      "2018-09-11 13:45:26,393: INFO: Loading a fresh vocabulary\n",
      "2018-09-11 13:45:26,445: INFO: min_count=40 retains 13056 unique words (12% of original 102304, drops 89248)\n",
      "2018-09-11 13:45:26,445: INFO: min_count=40 leaves 11401019 word corpus (95% of original 11877522, drops 476503)\n",
      "2018-09-11 13:45:26,479: INFO: deleting the raw counts dictionary of 102304 items\n",
      "2018-09-11 13:45:26,483: INFO: sample=0.001 downsamples 48 most-common words\n",
      "2018-09-11 13:45:26,483: INFO: downsampling leaves estimated 8394665 word corpus (73.6% of prior 11401019)\n",
      "2018-09-11 13:45:26,524: INFO: estimated required memory for 13056 words and 300 dimensions: 37862400 bytes\n",
      "2018-09-11 13:45:26,525: INFO: resetting layer weights\n",
      "2018-09-11 13:45:26,714: INFO: training model with 4 workers on 13056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-09-11 13:45:27,729: INFO: EPOCH 1 - PROGRESS: at 11.32% examples, 944403 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:28,732: INFO: EPOCH 1 - PROGRESS: at 23.20% examples, 971697 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:29,735: INFO: EPOCH 1 - PROGRESS: at 34.14% examples, 952464 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:30,743: INFO: EPOCH 1 - PROGRESS: at 46.21% examples, 966284 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:31,745: INFO: EPOCH 1 - PROGRESS: at 57.96% examples, 970163 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:32,748: INFO: EPOCH 1 - PROGRESS: at 69.89% examples, 973782 words/s, in_qsize 8, out_qsize 0\n",
      "2018-09-11 13:45:33,750: INFO: EPOCH 1 - PROGRESS: at 82.09% examples, 980475 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:34,755: INFO: EPOCH 1 - PROGRESS: at 92.32% examples, 964070 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:35,389: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-11 13:45:35,400: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-11 13:45:35,405: INFO: worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-11 13:45:35,408: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-11 13:45:35,408: INFO: EPOCH - 1 : training on 11877522 raw words (8395105 effective words) took 8.7s, 966293 effective words/s\n",
      "2018-09-11 13:45:36,423: INFO: EPOCH 2 - PROGRESS: at 11.24% examples, 939019 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:37,428: INFO: EPOCH 2 - PROGRESS: at 21.47% examples, 897803 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:38,432: INFO: EPOCH 2 - PROGRESS: at 32.73% examples, 912309 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:39,432: INFO: EPOCH 2 - PROGRESS: at 43.95% examples, 920451 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:40,446: INFO: EPOCH 2 - PROGRESS: at 55.05% examples, 919892 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:41,457: INFO: EPOCH 2 - PROGRESS: at 66.67% examples, 927231 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:42,457: INFO: EPOCH 2 - PROGRESS: at 79.08% examples, 942878 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:43,485: INFO: EPOCH 2 - PROGRESS: at 91.64% examples, 953008 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:44,263: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-11 13:45:44,269: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-11 13:45:44,280: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-11 13:45:44,282: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-11 13:45:44,283: INFO: EPOCH - 2 : training on 11877522 raw words (8393985 effective words) took 8.9s, 946667 effective words/s\n",
      "2018-09-11 13:45:45,293: INFO: EPOCH 3 - PROGRESS: at 11.74% examples, 985977 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:46,300: INFO: EPOCH 3 - PROGRESS: at 24.30% examples, 1018390 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:47,304: INFO: EPOCH 3 - PROGRESS: at 37.12% examples, 1036883 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:48,308: INFO: EPOCH 3 - PROGRESS: at 49.47% examples, 1036008 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:49,308: INFO: EPOCH 3 - PROGRESS: at 62.27% examples, 1043276 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:50,311: INFO: EPOCH 3 - PROGRESS: at 75.21% examples, 1048836 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:51,312: INFO: EPOCH 3 - PROGRESS: at 87.95% examples, 1052121 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:52,226: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-11 13:45:52,231: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-11 13:45:52,240: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-11 13:45:52,244: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-11 13:45:52,245: INFO: EPOCH - 3 : training on 11877522 raw words (8394629 effective words) took 8.0s, 1055505 effective words/s\n",
      "2018-09-11 13:45:53,253: INFO: EPOCH 4 - PROGRESS: at 12.56% examples, 1058634 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:54,265: INFO: EPOCH 4 - PROGRESS: at 25.32% examples, 1059363 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:55,265: INFO: EPOCH 4 - PROGRESS: at 37.88% examples, 1058257 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:56,272: INFO: EPOCH 4 - PROGRESS: at 50.66% examples, 1059903 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:57,282: INFO: EPOCH 4 - PROGRESS: at 63.37% examples, 1058869 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:58,292: INFO: EPOCH 4 - PROGRESS: at 76.04% examples, 1057140 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:45:59,296: INFO: EPOCH 4 - PROGRESS: at 88.49% examples, 1054791 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:00,197: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-11 13:46:00,198: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-11 13:46:00,211: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-11 13:46:00,216: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-11 13:46:00,216: INFO: EPOCH - 4 : training on 11877522 raw words (8394181 effective words) took 8.0s, 1054089 effective words/s\n",
      "2018-09-11 13:46:01,225: INFO: EPOCH 5 - PROGRESS: at 12.17% examples, 1021281 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:02,230: INFO: EPOCH 5 - PROGRESS: at 24.63% examples, 1033752 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:03,237: INFO: EPOCH 5 - PROGRESS: at 37.20% examples, 1038895 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:04,245: INFO: EPOCH 5 - PROGRESS: at 49.83% examples, 1041871 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:05,246: INFO: EPOCH 5 - PROGRESS: at 62.36% examples, 1043545 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:06,250: INFO: EPOCH 5 - PROGRESS: at 74.88% examples, 1042978 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:07,262: INFO: EPOCH 5 - PROGRESS: at 87.47% examples, 1043270 words/s, in_qsize 7, out_qsize 0\n",
      "2018-09-11 13:46:08,244: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-09-11 13:46:08,257: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-11 13:46:08,260: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-11 13:46:08,265: INFO: EPOCH 5 - PROGRESS: at 100.00% examples, 1043838 words/s, in_qsize 0, out_qsize 1\n",
      "2018-09-11 13:46:08,265: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-11 13:46:08,266: INFO: EPOCH - 5 : training on 11877522 raw words (8394654 effective words) took 8.0s, 1043675 effective words/s\n",
      "2018-09-11 13:46:08,267: INFO: training on a 59387610 raw words (41972554 effective words) took 41.6s, 1010113 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences, \n",
    "                         workers=num_workers, \n",
    "                         size=num_features, \n",
    "                          min_count=min_word_count, \n",
    "                          window=context, \n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-11 14:26:05,461: INFO: precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-11 14:28:05,235: INFO: saving Word2Vec object under /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model, separately None\n",
      "2018-09-11 14:28:05,236: INFO: not storing attribute vectors_norm\n",
      "2018-09-11 14:28:05,237: INFO: not storing attribute cum_table\n",
      "2018-09-11 14:28:05,604: INFO: saved /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model.save(os.path.join(BASE_PATH, 'data', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看看训练的词向量结果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match('man woman child kitchen'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6461790800094604),\n",
       " ('lad', 0.6023533344268799),\n",
       " ('lady', 0.598419189453125),\n",
       " ('chap', 0.5527979731559753),\n",
       " ('soldier', 0.5452903509140015),\n",
       " ('guy', 0.5335018634796143),\n",
       " ('person', 0.5306960940361023),\n",
       " ('monk', 0.5164638757705688),\n",
       " ('boy', 0.5161195993423462),\n",
       " ('millionaire', 0.4999878406524658)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6499490737915039),\n",
       " ('bride', 0.6323518753051758),\n",
       " ('maid', 0.6304798722267151),\n",
       " ('angela', 0.6170095205307007),\n",
       " ('feisty', 0.6112707853317261),\n",
       " ('mistress', 0.6094938516616821),\n",
       " ('temple', 0.6072278618812561),\n",
       " ('belle', 0.6070975065231323),\n",
       " ('nurse', 0.6022297143936157),\n",
       " ('marlene', 0.6006361842155457)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7814360857009888),\n",
       " ('horrible', 0.7440706491470337),\n",
       " ('atrocious', 0.7384001016616821),\n",
       " ('abysmal', 0.6928448677062988),\n",
       " ('horrid', 0.6792713403701782),\n",
       " ('dreadful', 0.6701364517211914),\n",
       " ('embarrassing', 0.6612033247947693),\n",
       " ('horrendous', 0.6463928818702698),\n",
       " ('appalling', 0.6420878171920776),\n",
       " ('lousy', 0.6373677253723145)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('awful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a name='sentiment'>在Word2vec上训练情感分析模型</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, nrows=None):\n",
    "    datasets = {\n",
    "        'unlabeled_train': 'unlabeledTrainData.tsv',\n",
    "        'labeled_train': 'labeledTrainData.tsv',\n",
    "        'test': 'testData.tsv'\n",
    "    }\n",
    "    if name not in datasets:\n",
    "        raise ValueError(name)\n",
    "    data_file = os.path.join(BASE_PATH, 'data', datasets[name])\n",
    "    df = pd.read_csv(data_file, sep='\\t', escapechar='\\\\', nrows=nrows)\n",
    "    print('Number of reviews: {}'.format(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_stopwords = set(stopwords.words('english'))\n",
    "eng_stopwords = {}.fromkeys([line.strip() for line in open('./stopwords.txt')])\n",
    "\n",
    "def clean_text(text, remove_stopwords=False):\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in eng_stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取上面训练好的word2vec模型\n",
    "名为：300features_40minwords_10context.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-11 14:37:45,316: INFO: loading Word2Vec object from /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model\n",
      "2018-09-11 14:37:45,604: INFO: loading wv recursively from /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model.wv.* with mmap=None\n",
      "2018-09-11 14:37:45,605: INFO: setting ignored attribute vectors_norm to None\n",
      "2018-09-11 14:37:45,606: INFO: loading vocabulary recursively from /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model.vocabulary.* with mmap=None\n",
      "2018-09-11 14:37:45,606: INFO: loading trainables recursively from /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model.trainables.* with mmap=None\n",
      "2018-09-11 14:37:45,607: INFO: setting ignored attribute cum_table to None\n",
      "2018-09-11 14:37:45,608: INFO: loaded /Users/zoe/Documents/GitHub/July-NLP/Lec 09 Word2Vec/Word2Vec应用案例/Kaggle竞赛实例/kaggle_movie_sentiment/data/300features_40minwords_10context.model\n"
     ]
    }
   ],
   "source": [
    "model_name = '300features_40minwords_10context.model'\n",
    "model = Word2Vec.load(os.path.join(BASE_PATH, 'data', model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们可以根据word2vec的结果去对影评文本进行编码\n",
    "编码方式有一点粗暴，简单说来就是：把这句话中的词的词向量做平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('labeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_review_vector(review):\n",
    "    words = clean_text(review, remove_stopwords=True)\n",
    "    array = np.array([model.wv[w] for w in words if w in model.wv])\n",
    "    return pd.Series(array.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.038656</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.012377</td>\n",
       "      <td>-0.006938</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>0.028741</td>\n",
       "      <td>0.021159</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>-0.007790</td>\n",
       "      <td>-0.005402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>-0.019062</td>\n",
       "      <td>0.013020</td>\n",
       "      <td>0.011019</td>\n",
       "      <td>-0.005152</td>\n",
       "      <td>-0.017299</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.026233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.027447</td>\n",
       "      <td>-0.003779</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.012106</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>-0.001523</td>\n",
       "      <td>0.007582</td>\n",
       "      <td>0.011109</td>\n",
       "      <td>0.011646</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033001</td>\n",
       "      <td>-0.001475</td>\n",
       "      <td>0.019913</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.008966</td>\n",
       "      <td>-0.014869</td>\n",
       "      <td>0.008585</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>-0.022355</td>\n",
       "      <td>0.030743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.040628</td>\n",
       "      <td>0.038908</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>-0.005125</td>\n",
       "      <td>-0.004106</td>\n",
       "      <td>0.034407</td>\n",
       "      <td>0.037080</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018228</td>\n",
       "      <td>-0.003175</td>\n",
       "      <td>-0.002990</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.016348</td>\n",
       "      <td>-0.013161</td>\n",
       "      <td>-0.012926</td>\n",
       "      <td>0.014982</td>\n",
       "      <td>0.043757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.046930</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>-0.003514</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>-0.015563</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>-0.006209</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013935</td>\n",
       "      <td>0.021021</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>-0.008581</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>-0.004734</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>0.027569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025354</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.016053</td>\n",
       "      <td>0.007296</td>\n",
       "      <td>0.012562</td>\n",
       "      <td>0.039233</td>\n",
       "      <td>0.024039</td>\n",
       "      <td>-0.011890</td>\n",
       "      <td>-0.004568</td>\n",
       "      <td>-0.009628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016103</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.005870</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>-0.010432</td>\n",
       "      <td>0.024471</td>\n",
       "      <td>0.037066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.038656  0.016665  0.012377 -0.006938  0.017928  0.028741  0.021159   \n",
       "1 -0.027447 -0.003779  0.002941  0.012106  0.008971 -0.001523  0.007582   \n",
       "2 -0.040628  0.038908  0.015016 -0.005125 -0.004106  0.034407  0.037080   \n",
       "3 -0.046930  0.026483  0.015915 -0.001099 -0.003514  0.008214  0.010639   \n",
       "4 -0.025354  0.031252  0.016053  0.007296  0.012562  0.039233  0.024039   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.002569 -0.007790 -0.005402    ...    -0.000844 -0.019062  0.013020   \n",
       "1  0.011109  0.011646  0.030603    ...    -0.033001 -0.001475  0.019913   \n",
       "2  0.004509  0.006941  0.004752    ...    -0.018228 -0.003175 -0.002990   \n",
       "3 -0.015563  0.004354 -0.006209    ...    -0.013935  0.021021  0.019471   \n",
       "4 -0.011890 -0.004568 -0.009628    ...    -0.016103 -0.001034  0.007290   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.011019 -0.005152 -0.017299  0.016229  0.001952  0.004006  0.026233  \n",
       "1 -0.003049 -0.008966 -0.014869  0.008585 -0.000171 -0.022355  0.030743  \n",
       "2  0.009671  0.012360  0.016348 -0.013161 -0.012926  0.014982  0.043757  \n",
       "3  0.005665 -0.008581  0.003367 -0.004734  0.003352  0.010746  0.027569  \n",
       "4  0.012294 -0.002900  0.005870  0.001857 -0.010432  0.024471  0.037066  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features = df.review.apply(to_review_vector)\n",
    "train_data_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用随机森林构建分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest = forest.fit(train_data_features, df.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型在训练集上验证效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12499,     1],\n",
       "       [    0, 12500]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df.sentiment, forest.predict(train_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清理占用内容的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测测试集结果并上传kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('test')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.046155</td>\n",
       "      <td>0.042196</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>0.021927</td>\n",
       "      <td>0.039790</td>\n",
       "      <td>0.034181</td>\n",
       "      <td>-0.014081</td>\n",
       "      <td>-0.026512</td>\n",
       "      <td>-0.047788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025544</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>0.015436</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>-0.003724</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>-0.009434</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>0.051093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012886</td>\n",
       "      <td>-0.004891</td>\n",
       "      <td>0.022420</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.014649</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>0.047358</td>\n",
       "      <td>-0.009926</td>\n",
       "      <td>0.005163</td>\n",
       "      <td>-0.018134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>-0.009565</td>\n",
       "      <td>-0.023542</td>\n",
       "      <td>-0.029679</td>\n",
       "      <td>0.018448</td>\n",
       "      <td>-0.003557</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.010662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.017965</td>\n",
       "      <td>0.014629</td>\n",
       "      <td>0.021074</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.009187</td>\n",
       "      <td>0.013887</td>\n",
       "      <td>0.012884</td>\n",
       "      <td>-0.019920</td>\n",
       "      <td>-0.017748</td>\n",
       "      <td>-0.018572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>-0.008839</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>-0.002652</td>\n",
       "      <td>-0.022080</td>\n",
       "      <td>-0.027755</td>\n",
       "      <td>0.034364</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>0.017472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.037820</td>\n",
       "      <td>0.037639</td>\n",
       "      <td>-0.008661</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>-0.009824</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.040886</td>\n",
       "      <td>-0.007619</td>\n",
       "      <td>-0.003907</td>\n",
       "      <td>-0.019943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018872</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>-0.008786</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.012749</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.017624</td>\n",
       "      <td>0.045917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.059465</td>\n",
       "      <td>0.032887</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>-0.015895</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.027548</td>\n",
       "      <td>-0.007834</td>\n",
       "      <td>-0.007266</td>\n",
       "      <td>-0.013936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011353</td>\n",
       "      <td>0.014154</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.017066</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>-0.012280</td>\n",
       "      <td>0.030550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.046155  0.042196  0.015595  0.008609  0.021927  0.039790  0.034181   \n",
       "1  0.012886 -0.004891  0.022420 -0.000540 -0.014649  0.011687  0.047358   \n",
       "2 -0.017965  0.014629  0.021074  0.000304  0.009187  0.013887  0.012884   \n",
       "3 -0.037820  0.037639 -0.008661  0.004752 -0.009824  0.030751  0.040886   \n",
       "4 -0.059465  0.032887  0.002643 -0.015895  0.001775  0.004698  0.027548   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0 -0.014081 -0.026512 -0.047788    ...    -0.025544 -0.006317  0.015436   \n",
       "1 -0.009926  0.005163 -0.018134    ...    -0.004493  0.002020  0.019044   \n",
       "2 -0.019920 -0.017748 -0.018572    ...     0.002945 -0.008839  0.027904   \n",
       "3 -0.007619 -0.003907 -0.019943    ...    -0.018872 -0.005814  0.005971   \n",
       "4 -0.007834 -0.007266 -0.013936    ...    -0.011353  0.014154  0.001744   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.003830 -0.003724  0.015556 -0.005460 -0.009434  0.014679  0.051093  \n",
       "1 -0.009565 -0.023542 -0.029679  0.018448 -0.003557  0.004657  0.010662  \n",
       "2 -0.002652 -0.022080 -0.027755  0.034364  0.007655 -0.000173  0.017472  \n",
       "3 -0.008786 -0.000320 -0.012749  0.000829  0.001278  0.017624  0.045917  \n",
       "4  0.017066  0.013822 -0.003017  0.009888 -0.001988 -0.012280  0.030550  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features = df.review.apply(to_review_vector)\n",
    "test_data_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = forest.predict(test_data_features)\n",
    "output = pd.DataFrame({'id':df.id, 'sentiment':result})\n",
    "output.to_csv(os.path.join(BASE_PATH,'data','Word2Vec_model.csv'), index=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del test_data_features\n",
    "del forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对词向量进行聚类研究和编码\n",
    "使用KMeans进行聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv.vectors\n",
    "num_clusters = word_vectors.shape[0] // 10\n",
    "\n",
    "print(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n",
      "/Users/zoe/anaconda3/lib/python3.6/site-packages/sklearn/metrics/pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return distances if squared else np.sqrt(distances, out=distances)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 246 ms, total: 1.75 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters, n_jobs=4)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "filename = 'word_centroid_map_10avg.pickle'\n",
    "\n",
    "with open(os.path.join(BASE_PATH, 'data', filename) ,'bw') as f:\n",
    "    pickle.dump(word_centroid_map, f)\n",
    "    \n",
    "#with open(os.path.join('..', 'models', filename), 'br') as f:\n",
    "#    word_centroid_map = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出一些clusters看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['positively']\n",
      "\n",
      "Cluster 1\n",
      "['prison', 'law', 'church', 'plans', 'court', 'jail', 'charge', 'hostage', 'pressure', 'orders', 'safety', 'authorities', 'arrest', 'protection', 'charges', 'papers', 'permission', 'instructions', 'lawyers', 'testify']\n",
      "\n",
      "Cluster 2\n",
      "['shu', 'qi']\n",
      "\n",
      "Cluster 3\n",
      "['bruno', 'wang', 'distinguished', 'pedro', 'wu', 'vega', 'philippe', 'milian', 'buckaroo', 'memorably', 'karyo', 'wei', 'eduardo', 'tang', 'tomas', 'berger']\n",
      "\n",
      "Cluster 4\n",
      "['remote', 'farm', 'treasure', 'paradise', 'stranded', 'deserted', 'shelter', 'boarding', 'tourist', 'ghostly', 'luxury', 'refuge', 'backwoods', 'desolate', 'housing', 'mining', 'vermont', 'lighthouse', 'manor', 'secluded', 'farmhouse', 'roam', 'hellgate', 'dilapidated', 'lobster', 'nursing']\n",
      "\n",
      "Cluster 5\n",
      "['culture', 'rules', 'lesson', 'cultural', 'myth', 'literature', 'versus', 'origin', 'mythology', 'traditions', 'milieu', 'myths', 'diversity', 'mysticism']\n",
      "\n",
      "Cluster 6\n",
      "['btw', 'whale', 'grudge', 'wraith', 'bigfoot']\n",
      "\n",
      "Cluster 7\n",
      "['ridiculous', 'flat', 'unbelievable', 'laughable', 'unnecessary', 'unrealistic', 'embarrassing', 'ludicrous', 'idiotic', 'useless', 'unwatchable', 'implausible', 'preposterous']\n",
      "\n",
      "Cluster 8\n",
      "['running', 'chase', 'shooting', 'driving', 'drag', 'riding', 'chases', 'racing', 'mo']\n",
      "\n",
      "Cluster 9\n",
      "['floor', 'nerves', 'couch', 'toes', 'lap', 'sofa', 'lawn']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    print('\\nCluster %d' % cluster)\n",
    "    print([w for w,c in word_centroid_map.items() if c == cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把评论数据转成cluster bag vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = set(word_centroid_map.keys())\n",
    "\n",
    "def make_cluster_bag(review):\n",
    "    words = clean_text(review, remove_stopwords=True)\n",
    "    return (pd.Series([word_centroid_map[w] for w in words if w in wordset]).value_counts().reindex(range(num_clusters+1), fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('labeled_train')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1296</th>\n",
       "      <th>1297</th>\n",
       "      <th>1298</th>\n",
       "      <th>1299</th>\n",
       "      <th>1300</th>\n",
       "      <th>1301</th>\n",
       "      <th>1302</th>\n",
       "      <th>1303</th>\n",
       "      <th>1304</th>\n",
       "      <th>1305</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   1296  \\\n",
       "0     0     1     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "2     0     0     0     0     0     0     0     1     1     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     1     0     0  ...      0   \n",
       "\n",
       "   1297  1298  1299  1300  1301  1302  1303  1304  1305  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     1     0     0     1     0     0  \n",
       "\n",
       "[5 rows x 1306 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features = df.review.apply(make_cluster_bag)\n",
    "train_data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1306)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再用随机森林算法建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest = forest.fit(train_data_features, df.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在训练集上看模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12500,     0],\n",
       "       [    0, 12500]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df.sentiment, forest.predict(train_data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除无用的占内存的量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "del  df\n",
    "del train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入测试数据做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>Naturally in a film who's main themes are of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>This movie is a disaster within a disaster fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>All in all, this is a movie for kids. We saw i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>Afraid of the Dark left me with the impression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>A very accurate depiction of small time mob li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             review\n",
       "0  12311_10  Naturally in a film who's main themes are of m...\n",
       "1    8348_2  This movie is a disaster within a disaster fil...\n",
       "2    5828_4  All in all, this is a movie for kids. We saw i...\n",
       "3    7186_2  Afraid of the Dark left me with the impression...\n",
       "4   12128_7  A very accurate depiction of small time mob li..."
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_dataset('test')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1296</th>\n",
       "      <th>1297</th>\n",
       "      <th>1298</th>\n",
       "      <th>1299</th>\n",
       "      <th>1300</th>\n",
       "      <th>1301</th>\n",
       "      <th>1302</th>\n",
       "      <th>1303</th>\n",
       "      <th>1304</th>\n",
       "      <th>1305</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   1296  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "2     0     1     0     0     0     0     0     0     0     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   1297  1298  1299  1300  1301  1302  1303  1304  1305  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 1306 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features = df.review.apply(make_cluster_bag)\n",
    "test_data_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment\n",
       "0  12311_10          1\n",
       "1    8348_2          0\n",
       "2    5828_4          1\n",
       "3    7186_2          1\n",
       "4   12128_7          0"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = forest.predict(test_data_features)\n",
    "output = pd.DataFrame({\"id\": df.id, 'sentiment': result})\n",
    "output.to_csv(os.path.join(BASE_PATH,'data', 'Word2Vec_BagOfClusters.csv'))\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del test_data_features\n",
    "del forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
